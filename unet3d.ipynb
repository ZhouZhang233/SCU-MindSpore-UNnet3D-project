{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、数据集准备(我已经上传了处理好的数据，这部分不用运行)\n",
    "    1、下载LUNA16数据集到本地，放在data文件夹中\n",
    "    2、进入路径./data/LUNA16, 将得到的subset0-9.rar和seg-lungs-LUNA16.rar共11个文件解压\n",
    "    3、设置路径，运行下面代码，划分训练集和验证集，并且将数据格式转化为niffi，得到如下的文件结构\n",
    "    ./train\n",
    "        /image\n",
    "        /seg\n",
    "    ./val\n",
    "        /image\n",
    "        /seg\n",
    "    注：\n",
    "        （1）LUNA16数据集共887个volume，我们选择subset9的最后三个volume作为验证集，subset0-8进而subset9剩下的部分作为训练集。\n",
    "        （2）可以通过下载ITK-SNAP软件可视化图片和分割标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%md\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function: Adjust Data Content Structure & Convert volume to nifti format\n",
    "# Author: Zhang Zhongzhou\n",
    "# Date: 2022/9/16\n",
    "# LUNA16\n",
    "#   /train\n",
    "#       /image\n",
    "#       /seg\n",
    "#   /val\n",
    "#       /image\n",
    "#       /seg\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import SimpleITK as sitk\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from src.model_utils.config import config\n",
    "\n",
    "def convert_nifti(input_im, save_image, roi_size):\n",
    "    \"\"\"\n",
    "    Convert dataset into mifti format.\n",
    "\n",
    "    Args:\n",
    "        input_im(str): input image name\n",
    "        save_image(str): output image name\n",
    "        roi_size(list): The size to crop the image\n",
    "    \"\"\"\n",
    "    img = sitk.ReadImage(input_im)\n",
    "    image_array = sitk.GetArrayFromImage(img)\n",
    "    D, H, W = image_array.shape\n",
    "    if H < roi_size[0] or W < roi_size[1] or D < roi_size[2]:\n",
    "        print(\"file {} size is smaller than roi size, ignore it.\".format(input_im))\n",
    "        # continue\n",
    "    sitk.WriteImage(img, save_image)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    root = \"../data/LUNA16\"\n",
    "    image_fold = [\"subset0\",\"subset1\",\"subset2\",\"subset3\",\"subset4\",\"subset5\",\"subset6\",\"subset7\",\"subset8\",\"subset9\"]\n",
    "    label_fold = \"seg-lungs-LUNA16\"\n",
    "\n",
    "    save_train_image_path = osp.join(root, \"train\", \"image\")\n",
    "    save_train_seg_path = osp.join(root, \"train\", \"seg\")\n",
    "    save_val_image_path = osp.join(root, \"val\", \"image\")\n",
    "    save_val_seg_path = osp.join(root, \"val\", \"seg\")\n",
    "\n",
    "    # create save path\n",
    "    if not os.path.exists(save_train_image_path):\n",
    "        os.makedirs(save_train_image_path)\n",
    "    if not os.path.exists(save_train_seg_path):\n",
    "        os.makedirs(save_train_seg_path)\n",
    "    if not os.path.exists(save_val_image_path):\n",
    "        os.makedirs(save_val_image_path)\n",
    "    if not os.path.exists(save_val_seg_path):\n",
    "        os.makedirs(save_val_seg_path)\n",
    "\n",
    "    # split train and val\n",
    "    # Here we select the last 10 volume in subset9 for validation\n",
    "    for subset in image_fold:\n",
    "        image_list = sorted(glob.glob(osp.join(root, subset, \"*.mhd\")))\n",
    "        for im in tqdm(image_list):\n",
    "                im_name = im.split(\"/\")[-1][:-4]\n",
    "                seg = osp.join(root, label_fold, im_name + \".mhd\")\n",
    "                save_im_name = osp.join(save_train_image_path, im_name+\".nii.gz\")\n",
    "                save_seg_name = osp.join(save_train_seg_path, im_name + \".nii.gz\")\n",
    "                convert_nifti(im, save_im_name, config.roi_size)\n",
    "                convert_nifti(seg, save_seg_name, config.roi_size)\n",
    "\n",
    "    all_images = sorted(os.listdir(save_train_image_path))[-10:]\n",
    "    all_segs = sorted(os.listdir(save_train_seg_path))[-10:]\n",
    "    for i, name in enumerate(all_images):\n",
    "        sou_im = osp.join(save_train_image_path, name)\n",
    "        sou_seg = osp.join(save_train_seg_path, name)\n",
    "        shutil.move(sou_im, save_val_image_path)\n",
    "        shutil.move(sou_seg, save_val_seg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、参数设置（训练参数和dataloader参数）\n",
    "在训练过程中的参数设置，如图片大小、训练batchsize大小等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.22.3)\r\n",
      "Requirement already satisfied: nibabel in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (4.0.2)\r\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (6.0)\r\n",
      "Requirement already satisfied: SimpleITK in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (2.2.0)\r\n",
      "Requirement already satisfied: onnxruntime-gpu in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.12.1)\r\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (4.64.1)\r\n",
      "Requirement already satisfied: ml_collections in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.1.1)\r\n",
      "Requirement already satisfied: logger in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.4)\r\n",
      "Requirement already satisfied: medpy in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.4.0)\r\n",
      "Requirement already satisfied: mindinsight in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (1.8.0)\r\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from nibabel->-r requirements.txt (line 2)) (63.4.1)\r\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from nibabel->-r requirements.txt (line 2)) (21.3)\r\n",
      "Requirement already satisfied: protobuf in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from onnxruntime-gpu->-r requirements.txt (line 5)) (3.20.1)\r\n",
      "Requirement already satisfied: flatbuffers in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from onnxruntime-gpu->-r requirements.txt (line 5)) (2.0.7)\r\n",
      "Requirement already satisfied: coloredlogs in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from onnxruntime-gpu->-r requirements.txt (line 5)) (15.0.1)\r\n",
      "Requirement already satisfied: sympy in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from onnxruntime-gpu->-r requirements.txt (line 5)) (1.11.1)\r\n",
      "Requirement already satisfied: absl-py in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from ml_collections->-r requirements.txt (line 7)) (1.2.0)\r\n",
      "Requirement already satisfied: six in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from ml_collections->-r requirements.txt (line 7)) (1.16.0)\r\n",
      "Requirement already satisfied: contextlib2 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from ml_collections->-r requirements.txt (line 7)) (21.6.0)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from medpy->-r requirements.txt (line 9)) (1.8.1)\r\n",
      "Requirement already satisfied: itsdangerous>=1.1.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (2.1.2)\r\n",
      "Requirement already satisfied: scikit-learn>=0.23.1 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (1.1.2)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (0.2.0)\r\n",
      "Requirement already satisfied: gunicorn>=20.0.4 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (20.1.0)\r\n",
      "Requirement already satisfied: marshmallow>=3.10.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (3.18.0)\r\n",
      "Requirement already satisfied: yapf>=0.30.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (0.32.0)\r\n",
      "Requirement already satisfied: Flask>=2.1.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (2.2.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (9.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=1.1.1 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (2.1.1)\r\n",
      "Requirement already satisfied: psutil>=5.7.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (5.9.0)\r\n",
      "Requirement already satisfied: pandas>=1.0.4 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (1.5.1)\r\n",
      "Requirement already satisfied: grpcio>=1.39.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (1.50.0)\r\n",
      "Requirement already satisfied: XlsxWriter>=1.3.2 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (3.0.3)\r\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (3.1.2)\r\n",
      "Requirement already satisfied: Werkzeug>2.1.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (2.2.2)\r\n",
      "Requirement already satisfied: treelib>=1.6.1 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from mindinsight->-r requirements.txt (line 10)) (1.6.1)\r\n",
      "Requirement already satisfied: click>=8.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from Flask>=2.1.0->mindinsight->-r requirements.txt (line 10)) (8.1.3)\r\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from Flask>=2.1.0->mindinsight->-r requirements.txt (line 10)) (4.12.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from packaging>=17.0->nibabel->-r requirements.txt (line 2)) (3.0.9)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from pandas>=1.0.4->mindinsight->-r requirements.txt (line 10)) (2022.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from pandas>=1.0.4->mindinsight->-r requirements.txt (line 10)) (2.8.2)\r\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from scikit-learn>=0.23.1->mindinsight->-r requirements.txt (line 10)) (1.1.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from scikit-learn>=0.23.1->mindinsight->-r requirements.txt (line 10)) (3.1.0)\r\n",
      "Requirement already satisfied: future in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from treelib>=1.6.1->mindinsight->-r requirements.txt (line 10)) (0.18.2)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from coloredlogs->onnxruntime-gpu->-r requirements.txt (line 5)) (10.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from sympy->onnxruntime-gpu->-r requirements.txt (line 5)) (1.2.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/miniconda/envs/mindspore/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->Flask>=2.1.0->mindinsight->-r requirements.txt (line 10)) (3.8.1)\r\n",
      "batch_size: 1\n",
      "checkpoint_file_path: Unet3d-10-110.ckpt\n",
      "checkpoint_path: ./checkpoint/\n",
      "checkpoint_url: ''\n",
      "ckpt_file: ./checkpoint/Unet3d-10-110.ckpt\n",
      "data_path: data/LUNA16/train/\n",
      "data_url: ''\n",
      "device_id: 0\n",
      "device_target: GPU\n",
      "enable_fp16_gpu: false\n",
      "enable_modelarts: false\n",
      "enable_profiling: false\n",
      "epoch_size: 10\n",
      "file_format: MINDIR\n",
      "file_name: unet3d\n",
      "in_channels: 1\n",
      "keep_checkpoint_max: 1\n",
      "load_path: /checkpoint_path/\n",
      "loss_scale: 256.0\n",
      "lower_limit: 3\n",
      "lr: 0.0005\n",
      "max_epoch: 3\n",
      "max_val: 1000\n",
      "min_val: -500\n",
      "num_classes: 4\n",
      "num_worker: 4\n",
      "output_path: output\n",
      "overlap: 0.25\n",
      "post_result_path: ./result_Files\n",
      "pre_result_path: ./preprocess_Result\n",
      "roi_size:\n",
      "- 224\n",
      "- 224\n",
      "- 96\n",
      "run_distribute: false\n",
      "train_ur: ''\n",
      "upper_limit: 5\n",
      "warmup_ratio: 0.3\n",
      "warmup_step: 120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n",
    "\n",
    "import ml_collections\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_config():\n",
    "    \"\"\"\n",
    "    Get Config according to the yaml file and cli arguments.\n",
    "    \"\"\"\n",
    "    cfg = ml_collections.ConfigDict()\n",
    "    # Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)\n",
    "    # args = argparse.ArgumentParser(description=\"default name\", add_help=False)\n",
    "    cfg.enable_fp16_gpu=False\n",
    "    cfg.enable_modelarts=False\n",
    "    # Url for modelarts\n",
    "    cfg.data_url=\"\"\n",
    "    cfg.train_ur=\"\"\n",
    "    cfg.checkpoint_url=\"\"\n",
    "    # Path for local\n",
    "    cfg.run_distribute=False\n",
    "    cfg.enable_profiling=False\n",
    "    cfg.data_path= \"data/LUNA16/train/\"\n",
    "    cfg.output_path=\"output\"\n",
    "    cfg.load_path=\"/checkpoint_path/\"\n",
    "    cfg.device_target = \"GPU\"  # or \"GPU\"\n",
    "    cfg.checkpoint_path=\"./checkpoint/\"\n",
    "    cfg.checkpoint_file_path=\"Unet3d-10-110.ckpt\"\n",
    "\n",
    "    # ==============================================================================\n",
    "    # data loader options\n",
    "    cfg.num_worker = 4\n",
    "    # Training options\n",
    "    cfg.max_epoch = 3\n",
    "    cfg.lr=0.0005\n",
    "    cfg.batch_size=1\n",
    "    cfg.epoch_size=10\n",
    "    cfg.warmup_step=120\n",
    "    cfg.warmup_ratio=0.3\n",
    "    cfg.num_classes=4\n",
    "    cfg.in_channels=1\n",
    "    cfg.keep_checkpoint_max=1\n",
    "    cfg.loss_scale=256.0\n",
    "    cfg.roi_size=[224, 224, 96]\n",
    "    cfg.overlap=0.25\n",
    "    cfg.min_val=-500\n",
    "    cfg.max_val=1000\n",
    "    cfg.upper_limit=5\n",
    "    cfg.lower_limit=3\n",
    "\n",
    "    # Export options\n",
    "    cfg.device_id=0\n",
    "    cfg.ckpt_file=\"./checkpoint/Unet3d-10-110.ckpt\"\n",
    "    cfg.file_name=\"unet3d\"\n",
    "    cfg.file_format=\"MINDIR\"\n",
    "\n",
    "    # 310 infer options\n",
    "    cfg.pre_result_path=\"./preprocess_Result\"\n",
    "    cfg.post_result_path=\"./result_Files\"\n",
    "\n",
    "    return cfg\n",
    "\n",
    "config = get_config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、创建不同的数据集增强方式\n",
    "完成上述的文件格式转换之后，并进一步划分了训练和测试数据集，但是直接将图片数据送入网络训练，结果往往不太理想，因此需要通过不同的transform操作进行数据集增强，数据增强的方式包括：ExpandChannel、ScaleIntensityRange、RandomCropSamples、OneHot等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# transforms\n",
    "import re\n",
    "import numpy as np\n",
    "import logger\n",
    "\n",
    "np_str_obj_array_pattern = re.compile(r'[SaUO]')\n",
    "\n",
    "def correct_nifti_head(img):\n",
    "    \"\"\"\n",
    "    Check nifti object header's format, update the header if needed.\n",
    "    In the updated image pixdim matches the affine.\n",
    "\n",
    "    Args:\n",
    "        img: nifti image object\n",
    "    \"\"\"\n",
    "    dim = img.header[\"dim\"][0]\n",
    "    if dim >= 5:\n",
    "        return img\n",
    "    pixdim = np.asarray(img.header.get_zooms())[:dim]\n",
    "    norm_affine = np.sqrt(np.sum(np.square(img.affine[:dim, :dim]), 0))\n",
    "    if np.allclose(pixdim, norm_affine):\n",
    "        return img\n",
    "    # if hasattr(img, \"get_sform\"):\n",
    "    #     return rectify_header_sform_qform(img)\n",
    "    return img\n",
    "\n",
    "class LoadData:\n",
    "    \"\"\"\n",
    "    Load Image data from provided files.\n",
    "    \"\"\"\n",
    "    def __init__(self, canonical=False, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        canonical: if True, load the image as closest to canonical axis format.\n",
    "        dtype: convert the loaded image to this data type.\n",
    "        \"\"\"\n",
    "        self.canonical = canonical\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def operation(self, filename):\n",
    "        img_array = list()\n",
    "        compatible_meta = dict()\n",
    "        filename = filename.item()\n",
    "        filename = [filename]\n",
    "        for name in filename:\n",
    "            img = nib.load(str(name)[2:-1])\n",
    "            img = correct_nifti_head(img)\n",
    "            header = dict(img.header)\n",
    "            header[\"filename_or_obj\"] = name\n",
    "            header[\"affine\"] = img.affine\n",
    "            header[\"original_affine\"] = img.affine.copy()\n",
    "            header[\"canonical\"] = self.canonical\n",
    "            ndim = img.header[\"dim\"][0]\n",
    "            spatial_rank = min(ndim, 3)\n",
    "            header[\"spatial_shape\"] = img.header[\"dim\"][1 : spatial_rank + 1]\n",
    "            if self.canonical:\n",
    "                img = nib.as_closest_canonical(img)\n",
    "                header[\"affine\"] = img.affine\n",
    "            img_array.append(np.array(img.get_fdata(dtype=self.dtype)))\n",
    "            img.uncache()\n",
    "            if not compatible_meta:\n",
    "                for meta_key in header:\n",
    "                    meta_datum = header[meta_key]\n",
    "                    if isinstance(meta_datum, np.ndarray) \\\n",
    "                        and np_str_obj_array_pattern.search(meta_datum.dtype.str) is not None:\n",
    "                        continue\n",
    "                    compatible_meta[meta_key] = meta_datum\n",
    "            else:\n",
    "                assert np.allclose(header[\"affine\"], compatible_meta[\"affine\"])\n",
    "\n",
    "        img_array = np.stack(img_array, axis=0) if len(img_array) > 1 else img_array[0]\n",
    "        return img_array\n",
    "\n",
    "    def __call__(self, img, label):\n",
    "        img_array = self.operation(img)\n",
    "        seg_array = self.operation(label)\n",
    "        return img_array, seg_array\n",
    "\n",
    "class ExpandChannel:\n",
    "    \"\"\"\n",
    "    Expand a 1-length channel dimension to the input image.\n",
    "    \"\"\"\n",
    "    def __call__(self, img, label):\n",
    "        img_array = img[None]\n",
    "        seg_array = label[None]\n",
    "        return img_array, seg_array\n",
    "\n",
    "class Orientation:\n",
    "    \"\"\"\n",
    "    Change the input image's orientation into the specified based on `ax`.\n",
    "    \"\"\"\n",
    "    def __init__(self, ax=\"RAS\", labels=tuple(zip(\"LPI\", \"RAS\"))):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ax: N elements sequence for ND input's orientation.\n",
    "            labels: optional, None or sequence of (2,) sequences\n",
    "                (2,) sequences are labels for (beginning, end) of output axis.\n",
    "        \"\"\"\n",
    "        self.ax = ax\n",
    "        self.labels = labels\n",
    "\n",
    "    def operation(self, data, affine=None):\n",
    "        \"\"\"\n",
    "        original orientation of `data` is defined by `affine`.\n",
    "\n",
    "        Args:\n",
    "            data: in shape (num_channels, H[, W, ...]).\n",
    "            affine (matrix): (N+1)x(N+1) original affine matrix for spatially ND `data`. Defaults to identity.\n",
    "\n",
    "        Returns:\n",
    "            data (reoriented in `self.ax`), original ax, current ax.\n",
    "        \"\"\"\n",
    "        if data.ndim <= 1:\n",
    "            raise ValueError(\"data must have at least one spatial dimension.\")\n",
    "        if affine is None:\n",
    "            affine = np.eye(data.ndim, dtype=np.float64)\n",
    "            affine_copy = affine\n",
    "        else:\n",
    "            affine_copy = to_affine_nd(data.ndim-1, affine)\n",
    "        src = nib.io_orientation(affine_copy)\n",
    "        dst = nib.orientations.axcodes2ornt(self.ax[:data.ndim-1], labels=self.labels)\n",
    "        spatial_ornt = nib.orientations.ornt_transform(src, dst)\n",
    "        ornt = spatial_ornt.copy()\n",
    "        ornt[:, 0] += 1\n",
    "        ornt = np.concatenate([np.array([[0, 1]]), ornt])\n",
    "        data = nib.orientations.apply_orientation(data, ornt)\n",
    "        return data\n",
    "\n",
    "    def __call__(self, img, label):\n",
    "        img_array = self.operation(img)\n",
    "        seg_array = self.operation(label)\n",
    "        return img_array, seg_array\n",
    "\n",
    "class ScaleIntensityRange:\n",
    "    \"\"\"\n",
    "    Apply specific intensity scaling to the whole numpy array.\n",
    "    Scaling from [src_min, src_max] to [tgt_min, tgt_max] with clip option.\n",
    "\n",
    "    Args:\n",
    "        src_min: intensity original range min.\n",
    "        src_max: intensity original range max.\n",
    "        tgt_min: intensity target range min.\n",
    "        tgt_max: intensity target range max.\n",
    "        is_clip: whether to clip after scaling.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_min, src_max, tgt_min, tgt_max, is_clip=False):\n",
    "        self.src_min = src_min\n",
    "        self.src_max = src_max\n",
    "        self.tgt_min = tgt_min\n",
    "        self.tgt_max = tgt_max\n",
    "        self.is_clip = is_clip\n",
    "\n",
    "    def operation(self, data):\n",
    "        if self.src_max - self.src_min == 0.0:\n",
    "            logger.warning(\"Divide by zero (src_min == src_max)\")\n",
    "            return data - self.src_min + self.tgt_min\n",
    "        data = (data - self.src_min) / (self.src_max - self.src_min)\n",
    "        data = data * (self.tgt_max - self.tgt_min) + self.tgt_min\n",
    "        if self.is_clip:\n",
    "            data = np.clip(data, self.tgt_min, self.tgt_max)\n",
    "        return data\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        image = self.operation(image)\n",
    "        return image, label\n",
    "\n",
    "class RandomCropSamples:\n",
    "    def __init__(self, roi_size, num_samples=1):\n",
    "        self.roi_size = roi_size\n",
    "        self.num_samples = num_samples\n",
    "        self.set_random_state(0)\n",
    "\n",
    "    def set_random_state(self, seed=None):\n",
    "        \"\"\"\n",
    "        Set the random seed to control the slice size.\n",
    "\n",
    "        Args:\n",
    "            seed: set the random state with an integer seed.\n",
    "        \"\"\"\n",
    "        MAX_SEED = np.iinfo(np.uint32).max + 1\n",
    "        if seed is not None:\n",
    "            _seed = seed % MAX_SEED\n",
    "            self.rand_fn = np.random.RandomState(_seed)\n",
    "        else:\n",
    "            self.rand_fn = np.random.RandomState()\n",
    "        return self\n",
    "\n",
    "    def get_random_patch(self, dims, patch_size, rand_fn=None):\n",
    "        \"\"\"\n",
    "        Returns a tuple of slices to define a random patch in an array of shape `dims` with size `patch_size`.\n",
    "        \"\"\"\n",
    "        rand_int = np.random.randint if rand_fn is None else rand_fn.randint\n",
    "        min_corner = tuple(rand_int(0, ms - ps + 1) if ms > ps else 0 for ms, ps in zip(dims, patch_size))\n",
    "        return tuple(slice(mc, mc + ps) for mc, ps in zip(min_corner, patch_size))\n",
    "\n",
    "    def get_random_slice(self, img_size):\n",
    "        slices = (slice(None),) + self.get_random_patch(img_size, self.roi_size, self.rand_fn)\n",
    "        return slices\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        res_image = []\n",
    "        res_label = []\n",
    "        for _ in range(self.num_samples):\n",
    "            slices = self.get_random_slice(image.shape[1:])\n",
    "            img = image[slices]\n",
    "            label_crop = label[slices]\n",
    "            res_image.append(img)\n",
    "            res_label.append(label_crop)\n",
    "        return np.array(res_image), np.array(res_label)\n",
    "\n",
    "class OneHot:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def one_hot(self, labels):\n",
    "        N, K = labels.shape\n",
    "        one_hot_encoding = np.zeros((N, self.num_classes, K), dtype=np.float32)\n",
    "        for i in range(N):\n",
    "            for j in range(K):\n",
    "                one_hot_encoding[i, labels[i][j], j] = 1\n",
    "        return one_hot_encoding\n",
    "\n",
    "    def operation(self, labels):\n",
    "        N, _, D, H, W = labels.shape\n",
    "        labels = labels.astype(np.int32)\n",
    "        labels = np.reshape(labels, (N, -1))\n",
    "        labels = self.one_hot(labels)\n",
    "        labels = np.reshape(labels, (N, self.num_classes, D, H, W))\n",
    "        return labels\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        label = self.operation(label)\n",
    "        return image, label\n",
    "\n",
    "class ConvertLabel:\n",
    "    \"\"\"\n",
    "    Crop at the center of image with specified ROI size.\n",
    "    Args:\n",
    "        roi_size: the spatial size of the crop region e.g. [224,224,128]\n",
    "        If its components have non-positive values, the corresponding size of input image will be used.\n",
    "    \"\"\"\n",
    "    def operation(self, data):\n",
    "        \"\"\"\n",
    "        Apply the transform to `img`, assuming `img` is channel-first and\n",
    "        slicing doesn't apply to the channel dim.\n",
    "        \"\"\"\n",
    "        data[data > config.upper_limit] = 0\n",
    "        data = data - (config.lower_limit - 1)\n",
    "        data = np.clip(data, 0, config.lower_limit)\n",
    "        return data\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        label = self.operation(label)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "四、创建Dataloader\n",
    "设置好数据预处理之后，接下来需要定义一个可迭代的Dataloader用于数据加载，然后送入网络"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create dataloader successfully!!\n",
      "train dataset length is: 877\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.dataset.transforms.transforms import Compose\n",
    "import nibabel as nib\n",
    "import os\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, data, seg):\n",
    "        self.data = data\n",
    "        self.seg = seg\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        seg = self.seg[index]\n",
    "        return [data], [seg]\n",
    "\n",
    "def create_dataset(data_path, seg_path, rank_size=1, rank_id=0, is_training=True):\n",
    "    # print(seg_path)\n",
    "    seg_files = sorted(glob.glob(os.path.join(seg_path, \"*.nii.gz\")))\n",
    "    train_files = [os.path.join(data_path, os.path.basename(seg)) for seg in seg_files]\n",
    "    # print(train_files)\n",
    "    train_ds = Dataset(data=train_files, seg=seg_files)\n",
    "    train_loader = ds.GeneratorDataset(train_ds, column_names=[\"image\", \"seg\"], num_parallel_workers=config.num_worker, \\\n",
    "                                       shuffle=is_training, num_shards=rank_size, shard_id=rank_id)\n",
    "\n",
    "    if is_training:\n",
    "        transform_image = Compose([LoadData(),\n",
    "                                   ExpandChannel(),\n",
    "                                   Orientation(),\n",
    "                                   ScaleIntensityRange(src_min=config.min_val, src_max=config.max_val, tgt_min=0.0, \\\n",
    "                                                       tgt_max=1.0, is_clip=True),\n",
    "                                   RandomCropSamples(roi_size=config.roi_size, num_samples=4),\n",
    "                                   ConvertLabel(),\n",
    "                                   OneHot(num_classes=config.num_classes)])\n",
    "    else:\n",
    "        transform_image = Compose([LoadData(),\n",
    "                                   ExpandChannel(),\n",
    "                                   Orientation(),\n",
    "                                   ScaleIntensityRange(src_min=config.min_val, src_max=config.max_val, tgt_min=0.0, \\\n",
    "                                                       tgt_max=1.0, is_clip=True),\n",
    "                                   ConvertLabel()])\n",
    "\n",
    "    train_loader = train_loader.map(operations=transform_image,\n",
    "                                    input_columns=[\"image\", \"seg\"],\n",
    "                                    num_parallel_workers=config.num_worker,\n",
    "                                    python_multiprocessing=True)\n",
    "    if not is_training:\n",
    "        train_loader = train_loader.batch(1)\n",
    "    return train_loader\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # cfg.data_path\n",
    "    # print(config.data_path)\n",
    "    train_dataset = create_dataset(data_path=config.data_path+\"image\",\\\n",
    "                                   seg_path=config.data_path+\"seg\", \\\n",
    "                                   is_training=True)\n",
    "    train_data_size = train_dataset.get_dataset_size()\n",
    "    print(\"create dataloader successfully!!\")\n",
    "    print(\"train dataset length is:\", train_data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "五、构建Unet3D网络结构\n",
    "构建Unet3D网络，包括Encoder和Decoder两部分，Encoder有4个下采样层；Decoder有4个上采样层，最后的输出和原图大小相同的分割结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet3d<\n",
      "  (down1): Down<\n",
      "    (down_conv): ResidualUnit<\n",
      "      (down_conv_1): Conv3d<input_channels=1, output_channels=16, kernel_size=(3, 3, 3), stride=(2, 2, 2), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal1): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=16, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=down1.down_conv.batchNormal1.bn2d.gamma, shape=(16,), dtype=Float32, requires_grad=True), beta=Parameter (name=down1.down_conv.batchNormal1.bn2d.beta, shape=(16,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=down1.down_conv.batchNormal1.bn2d.moving_mean, shape=(16,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=down1.down_conv.batchNormal1.bn2d.moving_variance, shape=(16,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      (relu1): PReLU<>\n",
      "      (down_conv_2): Conv3d<input_channels=16, output_channels=16, kernel_size=(3, 3, 3), stride=(1, 1, 1), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (relu2): PReLU<>\n",
      "      (residual): Conv3d<input_channels=1, output_channels=16, kernel_size=(3, 3, 3), stride=(2, 2, 2), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal2): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=16, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=down1.down_conv.batchNormal2.bn2d.gamma, shape=(16,), dtype=Float32, requires_grad=True), beta=Parameter (name=down1.down_conv.batchNormal2.bn2d.beta, shape=(16,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=down1.down_conv.batchNormal2.bn2d.moving_mean, shape=(16,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=down1.down_conv.batchNormal2.bn2d.moving_variance, shape=(16,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      >\n",
      "    >\n",
      "  (down2): Down<\n",
      "    (down_conv): ResidualUnit<\n",
      "      (down_conv_1): Conv3d<input_channels=16, output_channels=32, kernel_size=(3, 3, 3), stride=(2, 2, 2), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal1): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=down2.down_conv.batchNormal1.bn2d.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=down2.down_conv.batchNormal1.bn2d.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=down2.down_conv.batchNormal1.bn2d.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=down2.down_conv.batchNormal1.bn2d.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      (relu1): PReLU<>\n",
      "      (down_conv_2): Conv3d<input_channels=32, output_channels=32, kernel_size=(3, 3, 3), stride=(1, 1, 1), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (relu2): PReLU<>\n",
      "      (residual): Conv3d<input_channels=16, output_channels=32, kernel_size=(3, 3, 3), stride=(2, 2, 2), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal2): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=down2.down_conv.batchNormal2.bn2d.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=down2.down_conv.batchNormal2.bn2d.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=down2.down_conv.batchNormal2.bn2d.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=down2.down_conv.batchNormal2.bn2d.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      >\n",
      "    >\n",
      "  (down3): Down<\n",
      "    (down_conv): ResidualUnit<\n",
      "      (down_conv_1): Conv3d<input_channels=32, output_channels=64, kernel_size=(3, 3, 3), stride=(2, 2, 2), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal1): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=down3.down_conv.batchNormal1.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=down3.down_conv.batchNormal1.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=down3.down_conv.batchNormal1.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=down3.down_conv.batchNormal1.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      (relu1): PReLU<>\n",
      "      (down_conv_2): Conv3d<input_channels=64, output_channels=64, kernel_size=(3, 3, 3), stride=(1, 1, 1), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (relu2): PReLU<>\n",
      "      (residual): Conv3d<input_channels=32, output_channels=64, kernel_size=(3, 3, 3), stride=(2, 2, 2), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal2): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=down3.down_conv.batchNormal2.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=down3.down_conv.batchNormal2.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=down3.down_conv.batchNormal2.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=down3.down_conv.batchNormal2.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      >\n",
      "    >\n",
      "  (down4): Down<\n",
      "    (down_conv): ResidualUnit<\n",
      "      (down_conv_1): Conv3d<input_channels=64, output_channels=128, kernel_size=(3, 3, 3), stride=(2, 2, 2), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal1): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=128, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=down4.down_conv.batchNormal1.bn2d.gamma, shape=(128,), dtype=Float32, requires_grad=True), beta=Parameter (name=down4.down_conv.batchNormal1.bn2d.beta, shape=(128,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=down4.down_conv.batchNormal1.bn2d.moving_mean, shape=(128,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=down4.down_conv.batchNormal1.bn2d.moving_variance, shape=(128,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      (relu1): PReLU<>\n",
      "      (down_conv_2): Conv3d<input_channels=128, output_channels=128, kernel_size=(3, 3, 3), stride=(1, 1, 1), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (relu2): PReLU<>\n",
      "      (residual): Conv3d<input_channels=64, output_channels=128, kernel_size=(3, 3, 3), stride=(2, 2, 2), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal2): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=128, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=down4.down_conv.batchNormal2.bn2d.gamma, shape=(128,), dtype=Float32, requires_grad=True), beta=Parameter (name=down4.down_conv.batchNormal2.bn2d.beta, shape=(128,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=down4.down_conv.batchNormal2.bn2d.moving_mean, shape=(128,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=down4.down_conv.batchNormal2.bn2d.moving_variance, shape=(128,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      >\n",
      "    >\n",
      "  (down5): Down<\n",
      "    (down_conv): ResidualUnit<\n",
      "      (down_conv_1): Conv3d<input_channels=128, output_channels=256, kernel_size=(3, 3, 3), stride=(1, 1, 1), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal1): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=down5.down_conv.batchNormal1.bn2d.gamma, shape=(256,), dtype=Float32, requires_grad=True), beta=Parameter (name=down5.down_conv.batchNormal1.bn2d.beta, shape=(256,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=down5.down_conv.batchNormal1.bn2d.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=down5.down_conv.batchNormal1.bn2d.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      (relu1): PReLU<>\n",
      "      (down_conv_2): Conv3d<input_channels=256, output_channels=256, kernel_size=(3, 3, 3), stride=(1, 1, 1), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (relu2): PReLU<>\n",
      "      (residual): Conv3d<input_channels=128, output_channels=256, kernel_size=(1, 1, 1), stride=(1, 1, 1), pad_mode=valid, padding=0, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal2): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=down5.down_conv.batchNormal2.bn2d.gamma, shape=(256,), dtype=Float32, requires_grad=True), beta=Parameter (name=down5.down_conv.batchNormal2.bn2d.beta, shape=(256,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=down5.down_conv.batchNormal2.bn2d.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=down5.down_conv.batchNormal2.bn2d.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      >\n",
      "    >\n",
      "  (up1): Up<\n",
      "    (conv3d_transpose): Conv3dTranspose<input_channels=384, output_channels=64, kernel_size=(3, 3, 3), stride=(2, 2, 2), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros>\n",
      "    (conv): ResidualUnit<\n",
      "      (down_conv_1): Conv3d<input_channels=64, output_channels=64, kernel_size=(3, 3, 3), stride=(1, 1, 1), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal1): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=up1.conv.batchNormal1.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=up1.conv.batchNormal1.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=up1.conv.batchNormal1.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=up1.conv.batchNormal1.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      (relu1): PReLU<>\n",
      "      >\n",
      "    (batchNormal1): BatchNorm3d<\n",
      "      (bn2d): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=up1.batchNormal1.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=up1.batchNormal1.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=up1.batchNormal1.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=up1.batchNormal1.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
      "      >\n",
      "    (relu): PReLU<>\n",
      "    >\n",
      "  (up2): Up<\n",
      "    (conv3d_transpose): Conv3dTranspose<input_channels=128, output_channels=32, kernel_size=(3, 3, 3), stride=(2, 2, 2), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros>\n",
      "    (conv): ResidualUnit<\n",
      "      (down_conv_1): Conv3d<input_channels=32, output_channels=32, kernel_size=(3, 3, 3), stride=(1, 1, 1), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal1): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=up2.conv.batchNormal1.bn2d.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=up2.conv.batchNormal1.bn2d.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=up2.conv.batchNormal1.bn2d.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=up2.conv.batchNormal1.bn2d.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      (relu1): PReLU<>\n",
      "      >\n",
      "    (batchNormal1): BatchNorm3d<\n",
      "      (bn2d): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=up2.batchNormal1.bn2d.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=up2.batchNormal1.bn2d.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=up2.batchNormal1.bn2d.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=up2.batchNormal1.bn2d.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>\n",
      "      >\n",
      "    (relu): PReLU<>\n",
      "    >\n",
      "  (up3): Up<\n",
      "    (conv3d_transpose): Conv3dTranspose<input_channels=64, output_channels=16, kernel_size=(3, 3, 3), stride=(2, 2, 2), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros>\n",
      "    (conv): ResidualUnit<\n",
      "      (down_conv_1): Conv3d<input_channels=16, output_channels=16, kernel_size=(3, 3, 3), stride=(1, 1, 1), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      (batchNormal1): BatchNorm3d<\n",
      "        (bn2d): BatchNorm2d<num_features=16, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=up3.conv.batchNormal1.bn2d.gamma, shape=(16,), dtype=Float32, requires_grad=True), beta=Parameter (name=up3.conv.batchNormal1.bn2d.beta, shape=(16,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=up3.conv.batchNormal1.bn2d.moving_mean, shape=(16,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=up3.conv.batchNormal1.bn2d.moving_variance, shape=(16,), dtype=Float32, requires_grad=False)>\n",
      "        >\n",
      "      (relu1): PReLU<>\n",
      "      >\n",
      "    (batchNormal1): BatchNorm3d<\n",
      "      (bn2d): BatchNorm2d<num_features=16, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=up3.batchNormal1.bn2d.gamma, shape=(16,), dtype=Float32, requires_grad=True), beta=Parameter (name=up3.batchNormal1.bn2d.beta, shape=(16,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=up3.batchNormal1.bn2d.moving_mean, shape=(16,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=up3.batchNormal1.bn2d.moving_variance, shape=(16,), dtype=Float32, requires_grad=False)>\n",
      "      >\n",
      "    (relu): PReLU<>\n",
      "    >\n",
      "  (up4): Up<\n",
      "    (conv3d_transpose): Conv3dTranspose<input_channels=32, output_channels=4, kernel_size=(3, 3, 3), stride=(2, 2, 2), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros>\n",
      "    (conv): ResidualUnit<\n",
      "      (down_conv_1): Conv3d<input_channels=4, output_channels=4, kernel_size=(3, 3, 3), stride=(1, 1, 1), pad_mode=pad, padding=1, dilation=(1, 1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCDHW>\n",
      "      >\n",
      "    (batchNormal1): BatchNorm3d<\n",
      "      (bn2d): BatchNorm2d<num_features=4, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=up4.batchNormal1.bn2d.gamma, shape=(4,), dtype=Float32, requires_grad=True), beta=Parameter (name=up4.batchNormal1.bn2d.beta, shape=(4,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=up4.batchNormal1.bn2d.moving_mean, shape=(4,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=up4.batchNormal1.bn2d.moving_variance, shape=(4,), dtype=Float32, requires_grad=False)>\n",
      "      >\n",
      "    (relu): PReLU<>\n",
      "    >\n",
      "  >\n",
      "UNet3D model input: (1, 1, 224, 224, 96)\n",
      "UNet3D model output: (1, 4, 224, 224, 96)\n"
     ]
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "from mindspore import dtype as mstype\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore import Tensor, Model, context\n",
    "# from src.unet3d_parts import Down, Up\n",
    "import numpy as np\n",
    "\n",
    "class BatchNorm3d(nn.Cell):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = P.Shape()\n",
    "        self.bn2d = nn.BatchNorm2d(num_features, data_format=\"NCHW\")\n",
    "\n",
    "    def construct(self, x):\n",
    "        x_shape = self.shape(x)\n",
    "        x = self.reshape(x, (x_shape[0], x_shape[1], x_shape[2] * x_shape[3], x_shape[4]))\n",
    "        bn2d_out = self.bn2d(x)\n",
    "        bn3d_out = self.reshape(bn2d_out, x_shape)\n",
    "        return bn3d_out\n",
    "\n",
    "class ResidualUnit(nn.Cell):\n",
    "    def __init__(self, in_channel, out_channel, stride=2, kernel_size=(3, 3, 3), down=True, is_output=False):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.down = down\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.down_conv_1 = nn.Conv3d(in_channel, out_channel, kernel_size=(3, 3, 3), \\\n",
    "                                     pad_mode=\"pad\", stride=self.stride, padding=1)\n",
    "        self.is_output = is_output\n",
    "        if not is_output:\n",
    "            self.batchNormal1 = BatchNorm3d(num_features=self.out_channel)\n",
    "            self.relu1 = nn.PReLU()\n",
    "        if self.down:\n",
    "            self.down_conv_2 = nn.Conv3d(out_channel, out_channel, kernel_size=(3, 3, 3), \\\n",
    "                                         pad_mode=\"pad\", stride=1, padding=1)\n",
    "            self.relu2 = nn.PReLU()\n",
    "            if kernel_size[0] == 1:\n",
    "                self.residual = nn.Conv3d(in_channel, out_channel, kernel_size=(1, 1, 1), \\\n",
    "                                          pad_mode=\"valid\", stride=self.stride)\n",
    "            else:\n",
    "                self.residual = nn.Conv3d(in_channel, out_channel, kernel_size=(3, 3, 3), \\\n",
    "                                          pad_mode=\"pad\", stride=self.stride, padding=1)\n",
    "            self.batchNormal2 = BatchNorm3d(num_features=self.out_channel)\n",
    "\n",
    "\n",
    "    def construct(self, x):\n",
    "        out = self.down_conv_1(x)\n",
    "        if self.is_output:\n",
    "            return out\n",
    "        out = self.batchNormal1(out)\n",
    "        out = self.relu1(out)\n",
    "        if self.down:\n",
    "            out = self.down_conv_2(out)\n",
    "            out = self.batchNormal2(out)\n",
    "            out = self.relu2(out)\n",
    "            res = self.residual(x)\n",
    "        else:\n",
    "            res = x\n",
    "        return out + res\n",
    "\n",
    "class Down(nn.Cell):\n",
    "    def __init__(self, in_channel, out_channel, stride=2, kernel_size=(3, 3, 3), dtype=mstype.float16):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.down_conv = ResidualUnit(self.in_channel, self.out_channel, stride, kernel_size).to_float(dtype)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.down_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Up(nn.Cell):\n",
    "    def __init__(self, in_channel, down_in_channel, out_channel, stride=2, is_output=False, dtype=mstype.float16):\n",
    "        super().__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.down_in_channel = down_in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.stride = stride\n",
    "        self.conv3d_transpose = nn.Conv3dTranspose(in_channels=self.in_channel + self.down_in_channel, \\\n",
    "                                                   out_channels=self.out_channel, kernel_size=(3, 3, 3), \\\n",
    "                                                   pad_mode=\"pad\", stride=self.stride, \\\n",
    "                                                   output_padding=(1, 1, 1), padding=1)\n",
    "\n",
    "        self.concat = P.Concat(axis=1)\n",
    "        self.conv = ResidualUnit(self.out_channel, self.out_channel, stride=1, down=False, \\\n",
    "                                 is_output=is_output).to_float(dtype)\n",
    "        self.batchNormal1 = BatchNorm3d(num_features=self.out_channel)\n",
    "        self.relu = nn.PReLU()\n",
    "\n",
    "    def construct(self, input_data, down_input):\n",
    "        x = self.concat((input_data, down_input))\n",
    "        x = self.conv3d_transpose(x)\n",
    "        x = self.batchNormal1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class UNet3d_(nn.Cell):\n",
    "    \"\"\"\n",
    "    UNet3d_ support fp32 and fp16(amp) training on GPU.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(UNet3d_, self).__init__()\n",
    "        self.n_channels = config.in_channels\n",
    "        self.n_classes = config.num_classes\n",
    "\n",
    "        # down\n",
    "        self.down1 = Down(in_channel=self.n_channels, out_channel=16, dtype=mstype.float32)\n",
    "        self.down2 = Down(in_channel=16, out_channel=32, dtype=mstype.float32)\n",
    "        self.down3 = Down(in_channel=32, out_channel=64, dtype=mstype.float32)\n",
    "        self.down4 = Down(in_channel=64, out_channel=128, dtype=mstype.float32)\n",
    "        self.down5 = Down(in_channel=128, out_channel=256, stride=1, kernel_size=(1, 1, 1), \\\n",
    "                          dtype=mstype.float32)\n",
    "        # up\n",
    "        self.up1 = Up(in_channel=256, down_in_channel=128, out_channel=64, \\\n",
    "                      dtype=mstype.float32)\n",
    "        self.up2 = Up(in_channel=64, down_in_channel=64, out_channel=32, \\\n",
    "                      dtype=mstype.float32)\n",
    "        self.up3 = Up(in_channel=32, down_in_channel=32, out_channel=16, \\\n",
    "                      dtype=mstype.float32)\n",
    "        self.up4 = Up(in_channel=16, down_in_channel=16, out_channel=self.n_classes, \\\n",
    "                      dtype=mstype.float32, is_output=True)\n",
    "\n",
    "    def construct(self, input_data):\n",
    "        x1 = self.down1(input_data)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        x5 = self.down5(x4)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        return x\n",
    "\n",
    "class UNet3d(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(UNet3d, self).__init__()\n",
    "        self.n_channels = config.in_channels\n",
    "        self.n_classes = config.num_classes\n",
    "\n",
    "        # down\n",
    "        self.transpose = P.Transpose()\n",
    "        self.down1 = Down(in_channel=self.n_channels, out_channel=16, dtype=mstype.float16).to_float(mstype.float16)\n",
    "        self.down2 = Down(in_channel=16, out_channel=32, dtype=mstype.float16).to_float(mstype.float16)\n",
    "        self.down3 = Down(in_channel=32, out_channel=64, dtype=mstype.float16).to_float(mstype.float16)\n",
    "        self.down4 = Down(in_channel=64, out_channel=128, dtype=mstype.float16).to_float(mstype.float16)\n",
    "        self.down5 = Down(in_channel=128, out_channel=256, stride=1, kernel_size=(1, 1, 1), \\\n",
    "                          dtype=mstype.float16).to_float(mstype.float16)\n",
    "        # up\n",
    "        self.up1 = Up(in_channel=256, down_in_channel=128, out_channel=64, \\\n",
    "                      dtype=mstype.float16).to_float(mstype.float16)\n",
    "        self.up2 = Up(in_channel=64, down_in_channel=64, out_channel=32, \\\n",
    "                      dtype=mstype.float16).to_float(mstype.float16)\n",
    "        self.up3 = Up(in_channel=32, down_in_channel=32, out_channel=16, \\\n",
    "                      dtype=mstype.float16).to_float(mstype.float16)\n",
    "        self.up4 = Up(in_channel=16, down_in_channel=16, out_channel=self.n_classes, \\\n",
    "                      dtype=mstype.float16, is_output=True).to_float(mstype.float16)\n",
    "\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, input_data):\n",
    "        input_data = self.cast(input_data, mstype.float16)\n",
    "        x1 = self.down1(input_data)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        x5 = self.down5(x4)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.cast(x, mstype.float32)\n",
    "        return x\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=config.device_target, save_graphs=False)\n",
    "    x = ms.Tensor(np.zeros([1, 1, 224, 224, 96]), ms.float32)\n",
    "    model = UNet3d().set_train(True)\n",
    "    out = model(x)\n",
    "\n",
    "    print(model)\n",
    "    print(\"UNet3D model input:\", x.shape)\n",
    "    print(\"UNet3D model output:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "六、自定义Metrics\n",
    "在医学图像分割领域，通过Dice coefficient、Jaccard coefficient（JC）、Hausdorff distance 95（HD95）、Average surface distance（ASD）、Average symmetric surface distance metric（ASSD）、sensitivity（Sens）等量化指标来衡量分割效果的好坏。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from medpy.metric import binary\n",
    "\n",
    "class metrics:\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        self.smooth=1e-5\n",
    "\n",
    "    def dice_metric(self, y_pred, y_label, empty_score=1.0):\n",
    "        \"\"\"Calculates the dice coefficient for the images\"\"\"\n",
    "        return binary.dc(y_pred, y_label)\n",
    "\n",
    "    def jc_metric(self, y_pred, y_label):\n",
    "        \"\"\"Jaccard coefficient\"\"\"\n",
    "        return binary.jc(y_pred, y_label)\n",
    "\n",
    "    def hd95_metric(self, y_pred, y_label):\n",
    "        \"\"\"Calculates the hausdorff distance for the images\"\"\"\n",
    "        return binary.hd95(y_pred, y_label, voxelspacing=1)\n",
    "\n",
    "    def asd_metric(self, y_pred, y_label):\n",
    "        \"\"\"Average surface distance metric.\"\"\"\n",
    "        return binary.asd(y_pred, y_label, voxelspacing=None)\n",
    "\n",
    "    def assd_metric(self, y_pred, y_label):\n",
    "        \"\"\"Average symmetric surface distance metric.\"\"\"\n",
    "        return binary.assd(y_pred, y_label, voxelspacing=None)\n",
    "\n",
    "    def precision_metric(self, y_pred, y_label):\n",
    "        \"\"\"precision metric.\"\"\"\n",
    "        return binary.precision(y_pred, y_label, voxelspacing=None)\n",
    "\n",
    "    def sensitivity_metric(self, y_pred, y_label, smooth = 1e-5):\n",
    "        \"\"\"recall(also sensitivity) metric.\"\"\"\n",
    "        return binary.recall(y_pred, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "七、设置学习率策略\n",
    "学习率的设置对网络的训练至关重要，在这里我们使用两阶段的学习率，前三个epoch进行warm up，使用线性上升学习率策略，后面七个epoch使用consine下降学习率策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# dyanmic learning rate\n",
    "import math\n",
    "\n",
    "def linear_warmup_learning_rate(current_step, warmup_steps, base_lr, init_lr):\n",
    "    lr_inc = (float(base_lr) - float(init_lr)) / float(warmup_steps)\n",
    "    learning_rate = float(init_lr) + lr_inc * current_step\n",
    "    return learning_rate\n",
    "\n",
    "def a_cosine_learning_rate(current_step, base_lr, warmup_steps, decay_steps):\n",
    "    base = float(current_step - warmup_steps) / float(decay_steps)\n",
    "    learning_rate = (1 + math.cos(base * math.pi)) / 2 * base_lr\n",
    "    return learning_rate\n",
    "\n",
    "def dynamic_lr(config, base_step):\n",
    "    \"\"\"dynamic learning rate generator\"\"\"\n",
    "    base_lr = config.lr\n",
    "    total_steps = int(base_step * config.epoch_size)\n",
    "    warmup_steps = config.warmup_step\n",
    "    lr = []\n",
    "    for i in range(total_steps):\n",
    "        if i < warmup_steps:\n",
    "            lr.append(linear_warmup_learning_rate(i, warmup_steps, base_lr, base_lr * config.warmup_ratio))\n",
    "        else:\n",
    "            lr.append(a_cosine_learning_rate(i, base_lr, warmup_steps, total_steps))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Huawei Technologies Co., Ltd\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"Moxing adapter for ModelArts\"\"\"\n",
    "\n",
    "import os\n",
    "import functools\n",
    "from mindspore import context\n",
    "# from src.model_utils.config import config\n",
    "\n",
    "_global_sync_count = 0\n",
    "\n",
    "def get_device_id():\n",
    "    device_id = os.getenv('DEVICE_ID', '0')\n",
    "    return int(device_id)\n",
    "\n",
    "\n",
    "def get_device_num():\n",
    "    device_num = os.getenv('RANK_SIZE', '1')\n",
    "    return int(device_num)\n",
    "\n",
    "\n",
    "def get_rank_id():\n",
    "    global_rank_id = os.getenv('RANK_ID', '0')\n",
    "    return int(global_rank_id)\n",
    "\n",
    "\n",
    "def get_job_id():\n",
    "    job_id = os.getenv('JOB_ID')\n",
    "    job_id = job_id if job_id != \"\" else \"default\"\n",
    "    return job_id\n",
    "\n",
    "def sync_data(from_path, to_path):\n",
    "    \"\"\"\n",
    "    Download data from remote obs to local directory if the first url is remote url and the second one is local path\n",
    "    Upload data from local directory to remote obs in contrast.\n",
    "    \"\"\"\n",
    "    import moxing as mox\n",
    "    import time\n",
    "    global _global_sync_count\n",
    "    sync_lock = \"/tmp/copy_sync.lock\" + str(_global_sync_count)\n",
    "    _global_sync_count += 1\n",
    "\n",
    "    # Each server contains 8 devices as most.\n",
    "    if get_device_id() % min(get_device_num(), 8) == 0 and not os.path.exists(sync_lock):\n",
    "        print(\"from path: \", from_path)\n",
    "        print(\"to path: \", to_path)\n",
    "        mox.file.copy_parallel(from_path, to_path)\n",
    "        print(\"===finish data synchronization===\")\n",
    "        try:\n",
    "            os.mknod(sync_lock)\n",
    "        except IOError:\n",
    "            pass\n",
    "        print(\"===save flag===\")\n",
    "\n",
    "    while True:\n",
    "        if os.path.exists(sync_lock):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"Finish sync data from {} to {}.\".format(from_path, to_path))\n",
    "\n",
    "\n",
    "def moxing_wrapper(pre_process=None, post_process=None):\n",
    "    \"\"\"\n",
    "    Moxing wrapper to download dataset and upload outputs.\n",
    "    \"\"\"\n",
    "    def wrapper(run_func):\n",
    "        @functools.wraps(run_func)\n",
    "        def wrapped_func(*args, **kwargs):\n",
    "            # Download data from data_url\n",
    "            if config.enable_modelarts:\n",
    "                if config.data_url:\n",
    "                    sync_data(config.data_url, config.data_path)\n",
    "                    print(\"Dataset downloaded: \", os.listdir(config.data_path))\n",
    "                if config.checkpoint_url:\n",
    "                    sync_data(config.checkpoint_url, config.load_path)\n",
    "                    print(\"Preload downloaded: \", os.listdir(config.load_path))\n",
    "                if config.train_url:\n",
    "                    sync_data(config.train_url, config.output_path)\n",
    "                    print(\"Workspace downloaded: \", os.listdir(config.output_path))\n",
    "\n",
    "                context.set_context(save_graphs_path=os.path.join(config.output_path, str(get_rank_id())))\n",
    "                config.device_num = get_device_num()\n",
    "                config.device_id = get_device_id()\n",
    "                if not os.path.exists(config.output_path):\n",
    "                    os.makedirs(config.output_path)\n",
    "\n",
    "                if pre_process:\n",
    "                    pre_process()\n",
    "\n",
    "            run_func(*args, **kwargs)\n",
    "\n",
    "            # Upload data to train_url\n",
    "            if config.enable_modelarts:\n",
    "                if post_process:\n",
    "                    post_process()\n",
    "\n",
    "                if config.train_url:\n",
    "                    print(\"Start to copy output directory\")\n",
    "                    sync_data(config.output_path, config.train_url)\n",
    "        return wrapped_func\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "八、主函数训练\n",
    "主函数训练过程主要包括以下几步：\n",
    "\n",
    "选择运行设备GPU或者Ascend；\n",
    "调用create_dataset函数，创建dataloader；\n",
    "调用Unet3D函数，构建网络；\n",
    "定义损失函数，这里我们使用常见的dice loss和交叉熵损失（cross entropy loss）；\n",
    "调用学习率函数，设置优化器；\n",
    "设置网络为训练模式；\n",
    "使用for循环，不断将数据送入网络进行训练；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(8379:140121727778944,MainProcess):2022-11-09-03:06:53.754.37 [mindspore/dataset/engine/datasets_user_defined.py:766] GeneratorDataset's num_parallel_workers: 4 is too large which may cause a lot of memory occupation (>85%) or out of memory(OOM) during multiprocessing. Therefore, it is recommended to reduce num_parallel_workers to 3 or smaller.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length is: 877\n",
      "============== Starting Training ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(8379:140121727778944,MainProcess):2022-11-09-03:06:57.192.598 [mindspore/dataset/engine/datasets_user_defined.py:766] GeneratorDataset's num_parallel_workers: 4 is too large which may cause a lot of memory occupation (>85%) or out of memory(OOM) during multiprocessing. Therefore, it is recommended to reduce num_parallel_workers to 3 or smaller.\n",
      "[WARNING] ME(8379:140121727778944,MainProcess):2022-11-09-03:06:58.828.214 [mindspore/dataset/engine/datasets_user_defined.py:766] GeneratorDataset's num_parallel_workers: 4 is too large which may cause a lot of memory occupation (>85%) or out of memory(OOM) during multiprocessing. Therefore, it is recommended to reduce num_parallel_workers to 2 or smaller.\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:04.566.884 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:05.567.390 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:06.567.745 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:07.567.971 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:08.568.213 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:09.568.443 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:10.568.726 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:11.568.989 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:12.569.417 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:13.569.725 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:14.569.942 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:15.570.172 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:16.570.423 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:17.570.707 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:18.571.011 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:19.571.257 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:20.571.507 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:21.571.769 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:22.572.021 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:23.572.289 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:24.572.591 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] MD(8379,7f70a1cf4080,python):2022-11-09-03:07:25.572.820 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140107319342848 is not responding. Interrupt again\n",
      "[WARNING] ME(13217:140121727778944,_MPWorker-80):2022-11-09-03:07:48.642.41 [mindspore/dataset/engine/queue.py:120] Using shared memory queue, but rowsize is larger than allocated memory max_rowsize 16777216 current rowsize 77070336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 [0/877] [0/2631] lr:0.0001529 Loss: 2.4361 Loss_dice: 1.0371 Loss_ce: 1.3990\n",
      "Epoch: 0 [1/877] [1/2631] lr:0.0001588 Loss: 2.4083 Loss_dice: 1.0172 Loss_ce: 1.3911\n",
      "Epoch: 0 [2/877] [2/2631] lr:0.0001646 Loss: 2.3624 Loss_dice: 0.9854 Loss_ce: 1.3770\n",
      "Epoch: 0 [3/877] [3/2631] lr:0.0001704 Loss: 2.3289 Loss_dice: 0.9619 Loss_ce: 1.3670\n",
      "Epoch: 0 [4/877] [4/2631] lr:0.0001763 Loss: 2.3446 Loss_dice: 0.9709 Loss_ce: 1.3736\n",
      "Epoch: 0 [5/877] [5/2631] lr:0.0001821 Loss: 2.3213 Loss_dice: 0.9547 Loss_ce: 1.3667\n",
      "Epoch: 0 [6/877] [6/2631] lr:0.0001879 Loss: 2.2483 Loss_dice: 0.9056 Loss_ce: 1.3427\n",
      "Epoch: 0 [7/877] [7/2631] lr:0.0001938 Loss: 2.2662 Loss_dice: 0.9167 Loss_ce: 1.3495\n",
      "Epoch: 0 [8/877] [8/2631] lr:0.0001996 Loss: 2.2343 Loss_dice: 0.8950 Loss_ce: 1.3392\n",
      "Epoch: 0 [9/877] [9/2631] lr:0.0002054 Loss: 2.1536 Loss_dice: 0.8417 Loss_ce: 1.3119\n",
      "Epoch: 0 [10/877] [10/2631] lr:0.0002112 Loss: 2.1069 Loss_dice: 0.8109 Loss_ce: 1.2960\n",
      "Epoch: 0 [11/877] [11/2631] lr:0.0002171 Loss: 2.0873 Loss_dice: 0.7980 Loss_ce: 1.2893\n",
      "Epoch: 0 [12/877] [12/2631] lr:0.0002229 Loss: 2.0016 Loss_dice: 0.7425 Loss_ce: 1.2591\n",
      "Epoch: 0 [13/877] [13/2631] lr:0.0002287 Loss: 2.1126 Loss_dice: 0.8144 Loss_ce: 1.2982\n",
      "Epoch: 0 [14/877] [14/2631] lr:0.0002346 Loss: 2.0194 Loss_dice: 0.7549 Loss_ce: 1.2645\n",
      "Epoch: 0 [15/877] [15/2631] lr:0.0002404 Loss: 2.0285 Loss_dice: 0.7616 Loss_ce: 1.2669\n",
      "Epoch: 0 [16/877] [16/2631] lr:0.0002463 Loss: 2.1518 Loss_dice: 0.8402 Loss_ce: 1.3116\n",
      "Epoch: 0 [17/877] [17/2631] lr:0.0002521 Loss: 1.9536 Loss_dice: 0.7153 Loss_ce: 1.2383\n",
      "Epoch: 0 [18/877] [18/2631] lr:0.0002579 Loss: 2.0242 Loss_dice: 0.7607 Loss_ce: 1.2635\n",
      "Epoch: 0 [19/877] [19/2631] lr:0.0002638 Loss: 2.0620 Loss_dice: 0.7847 Loss_ce: 1.2773\n",
      "Epoch: 0 [20/877] [20/2631] lr:0.0002696 Loss: 1.8488 Loss_dice: 0.6538 Loss_ce: 1.1950\n",
      "Epoch: 0 [21/877] [21/2631] lr:0.0002754 Loss: 1.7665 Loss_dice: 0.6047 Loss_ce: 1.1618\n",
      "Epoch: 0 [22/877] [22/2631] lr:0.0002812 Loss: 1.8211 Loss_dice: 0.6400 Loss_ce: 1.1812\n",
      "Epoch: 0 [23/877] [23/2631] lr:0.0002871 Loss: 1.6760 Loss_dice: 0.5537 Loss_ce: 1.1223\n",
      "Epoch: 0 [24/877] [24/2631] lr:0.0002929 Loss: 1.7978 Loss_dice: 0.6290 Loss_ce: 1.1688\n",
      "Epoch: 0 [25/877] [25/2631] lr:0.0002988 Loss: 1.7644 Loss_dice: 0.6109 Loss_ce: 1.1535\n",
      "Epoch: 0 [26/877] [26/2631] lr:0.0003046 Loss: 1.7307 Loss_dice: 0.5938 Loss_ce: 1.1369\n",
      "Epoch: 0 [27/877] [27/2631] lr:0.0003104 Loss: 1.7328 Loss_dice: 0.5980 Loss_ce: 1.1348\n",
      "Epoch: 0 [28/877] [28/2631] lr:0.0003162 Loss: 1.6459 Loss_dice: 0.5500 Loss_ce: 1.0958\n",
      "Epoch: 0 [29/877] [29/2631] lr:0.0003221 Loss: 1.7360 Loss_dice: 0.6051 Loss_ce: 1.1310\n",
      "Epoch: 0 [30/877] [30/2631] lr:0.0003279 Loss: 1.7257 Loss_dice: 0.6034 Loss_ce: 1.1223\n",
      "Epoch: 0 [31/877] [31/2631] lr:0.0003338 Loss: 1.7446 Loss_dice: 0.6139 Loss_ce: 1.1307\n",
      "Epoch: 0 [32/877] [32/2631] lr:0.0003396 Loss: 1.6071 Loss_dice: 0.5403 Loss_ce: 1.0669\n",
      "Epoch: 0 [33/877] [33/2631] lr:0.0003454 Loss: 1.8732 Loss_dice: 0.6883 Loss_ce: 1.1849\n",
      "Epoch: 0 [34/877] [34/2631] lr:0.0003512 Loss: 1.6649 Loss_dice: 0.5775 Loss_ce: 1.0873\n",
      "Epoch: 0 [35/877] [35/2631] lr:0.0003571 Loss: 1.5626 Loss_dice: 0.5237 Loss_ce: 1.0389\n",
      "Epoch: 0 [36/877] [36/2631] lr:0.0003629 Loss: 1.4869 Loss_dice: 0.4843 Loss_ce: 1.0026\n",
      "Epoch: 0 [37/877] [37/2631] lr:0.0003688 Loss: 1.8127 Loss_dice: 0.6563 Loss_ce: 1.1564\n",
      "Epoch: 0 [38/877] [38/2631] lr:0.0003746 Loss: 1.5573 Loss_dice: 0.5235 Loss_ce: 1.0338\n",
      "Epoch: 0 [39/877] [39/2631] lr:0.0003804 Loss: 1.4214 Loss_dice: 0.4545 Loss_ce: 0.9668\n",
      "Epoch: 0 [40/877] [40/2631] lr:0.0003863 Loss: 1.3483 Loss_dice: 0.4188 Loss_ce: 0.9295\n",
      "Epoch: 0 [41/877] [41/2631] lr:0.0003921 Loss: 1.3553 Loss_dice: 0.4248 Loss_ce: 0.9305\n",
      "Epoch: 0 [42/877] [42/2631] lr:0.0003979 Loss: 1.5543 Loss_dice: 0.5270 Loss_ce: 1.0273\n",
      "Epoch: 0 [43/877] [43/2631] lr:0.0004037 Loss: 1.5665 Loss_dice: 0.5336 Loss_ce: 1.0329\n",
      "Epoch: 0 [44/877] [44/2631] lr:0.0004096 Loss: 1.4341 Loss_dice: 0.4692 Loss_ce: 0.9649\n",
      "Epoch: 0 [45/877] [45/2631] lr:0.0004154 Loss: 1.3647 Loss_dice: 0.4340 Loss_ce: 0.9307\n",
      "Epoch: 0 [46/877] [46/2631] lr:0.0004213 Loss: 1.5634 Loss_dice: 0.5351 Loss_ce: 1.0283\n",
      "Epoch: 0 [47/877] [47/2631] lr:0.0004271 Loss: 1.4017 Loss_dice: 0.4546 Loss_ce: 0.9471\n",
      "Epoch: 0 [48/877] [48/2631] lr:0.0004329 Loss: 1.4323 Loss_dice: 0.4665 Loss_ce: 0.9659\n",
      "Epoch: 0 [49/877] [49/2631] lr:0.0004387 Loss: 1.4352 Loss_dice: 0.4687 Loss_ce: 0.9664\n",
      "Epoch: 0 [50/877] [50/2631] lr:0.0004446 Loss: 1.4799 Loss_dice: 0.4906 Loss_ce: 0.9893\n",
      "Epoch: 0 [51/877] [51/2631] lr:0.0004504 Loss: 1.4716 Loss_dice: 0.4854 Loss_ce: 0.9862\n",
      "Epoch: 0 [52/877] [52/2631] lr:0.0004563 Loss: 1.6080 Loss_dice: 0.5478 Loss_ce: 1.0602\n",
      "Epoch: 0 [53/877] [53/2631] lr:0.0004621 Loss: 1.3856 Loss_dice: 0.4373 Loss_ce: 0.9483\n",
      "Epoch: 0 [54/877] [54/2631] lr:0.0004679 Loss: 1.2781 Loss_dice: 0.3790 Loss_ce: 0.8991\n",
      "Epoch: 0 [55/877] [55/2631] lr:0.0004737 Loss: 1.4757 Loss_dice: 0.4725 Loss_ce: 1.0032\n",
      "Epoch: 0 [56/877] [56/2631] lr:0.0004796 Loss: 1.3764 Loss_dice: 0.4244 Loss_ce: 0.9521\n",
      "Epoch: 0 [57/877] [57/2631] lr:0.0004854 Loss: 1.2797 Loss_dice: 0.3706 Loss_ce: 0.9091\n",
      "Epoch: 0 [58/877] [58/2631] lr:0.0004913 Loss: 1.3193 Loss_dice: 0.3874 Loss_ce: 0.9318\n",
      "Epoch: 0 [59/877] [59/2631] lr:0.0004971 Loss: 1.4393 Loss_dice: 0.4483 Loss_ce: 0.9910\n",
      "Epoch: 0 [60/877] [60/2631] lr:0.0005000 Loss: 1.2602 Loss_dice: 0.3545 Loss_ce: 0.9057\n",
      "Epoch: 0 [61/877] [61/2631] lr:0.0005000 Loss: 1.2221 Loss_dice: 0.3295 Loss_ce: 0.8926\n",
      "Epoch: 0 [62/877] [62/2631] lr:0.0005000 Loss: 1.5685 Loss_dice: 0.4973 Loss_ce: 1.0711\n",
      "Epoch: 0 [63/877] [63/2631] lr:0.0005000 Loss: 1.1186 Loss_dice: 0.2705 Loss_ce: 0.8481\n",
      "Epoch: 0 [64/877] [64/2631] lr:0.0005000 Loss: 1.2175 Loss_dice: 0.3169 Loss_ce: 0.9006\n",
      "Epoch: 0 [65/877] [65/2631] lr:0.0005000 Loss: 1.2929 Loss_dice: 0.3530 Loss_ce: 0.9399\n",
      "Epoch: 0 [66/877] [66/2631] lr:0.0005000 Loss: 1.2873 Loss_dice: 0.3460 Loss_ce: 0.9413\n",
      "Epoch: 0 [67/877] [67/2631] lr:0.0005000 Loss: 1.3308 Loss_dice: 0.3620 Loss_ce: 0.9687\n",
      "Epoch: 0 [68/877] [68/2631] lr:0.0005000 Loss: 1.3673 Loss_dice: 0.3764 Loss_ce: 0.9909\n",
      "Epoch: 0 [69/877] [69/2631] lr:0.0005000 Loss: 1.1589 Loss_dice: 0.2661 Loss_ce: 0.8928\n",
      "Epoch: 0 [70/877] [70/2631] lr:0.0005000 Loss: 1.1370 Loss_dice: 0.2508 Loss_ce: 0.8862\n",
      "Epoch: 0 [71/877] [71/2631] lr:0.0005000 Loss: 1.1396 Loss_dice: 0.2518 Loss_ce: 0.8878\n",
      "Epoch: 0 [72/877] [72/2631] lr:0.0005000 Loss: 1.1929 Loss_dice: 0.2723 Loss_ce: 0.9206\n",
      "Epoch: 0 [73/877] [73/2631] lr:0.0005000 Loss: 1.3118 Loss_dice: 0.3338 Loss_ce: 0.9780\n",
      "Epoch: 0 [74/877] [74/2631] lr:0.0005000 Loss: 1.0748 Loss_dice: 0.2092 Loss_ce: 0.8656\n",
      "Epoch: 0 [75/877] [75/2631] lr:0.0005000 Loss: 1.0888 Loss_dice: 0.2173 Loss_ce: 0.8715\n",
      "Epoch: 0 [76/877] [76/2631] lr:0.0005000 Loss: 1.0718 Loss_dice: 0.2066 Loss_ce: 0.8653\n",
      "Epoch: 0 [77/877] [77/2631] lr:0.0005000 Loss: 1.0947 Loss_dice: 0.2149 Loss_ce: 0.8798\n",
      "Epoch: 0 [78/877] [78/2631] lr:0.0005000 Loss: 1.1042 Loss_dice: 0.2192 Loss_ce: 0.8849\n",
      "Epoch: 0 [79/877] [79/2631] lr:0.0005000 Loss: 1.0612 Loss_dice: 0.1934 Loss_ce: 0.8678\n",
      "Epoch: 0 [80/877] [80/2631] lr:0.0005000 Loss: 1.2177 Loss_dice: 0.2755 Loss_ce: 0.9422\n",
      "Epoch: 0 [81/877] [81/2631] lr:0.0005000 Loss: 1.1866 Loss_dice: 0.2643 Loss_ce: 0.9223\n",
      "Epoch: 0 [82/877] [82/2631] lr:0.0005000 Loss: 1.0067 Loss_dice: 0.1629 Loss_ce: 0.8438\n",
      "Epoch: 0 [83/877] [83/2631] lr:0.0005000 Loss: 1.0705 Loss_dice: 0.2058 Loss_ce: 0.8646\n",
      "Epoch: 0 [84/877] [84/2631] lr:0.0005000 Loss: 1.3456 Loss_dice: 0.3516 Loss_ce: 0.9940\n",
      "Epoch: 0 [85/877] [85/2631] lr:0.0005000 Loss: 1.0663 Loss_dice: 0.1910 Loss_ce: 0.8753\n",
      "Epoch: 0 [86/877] [86/2631] lr:0.0005000 Loss: 1.1886 Loss_dice: 0.2644 Loss_ce: 0.9242\n",
      "Epoch: 0 [87/877] [87/2631] lr:0.0005000 Loss: 1.0003 Loss_dice: 0.1601 Loss_ce: 0.8402\n",
      "Epoch: 0 [88/877] [88/2631] lr:0.0004999 Loss: 1.0039 Loss_dice: 0.1640 Loss_ce: 0.8399\n",
      "Epoch: 0 [89/877] [89/2631] lr:0.0004999 Loss: 1.1329 Loss_dice: 0.2383 Loss_ce: 0.8946\n",
      "Epoch: 0 [90/877] [90/2631] lr:0.0004999 Loss: 1.1304 Loss_dice: 0.2341 Loss_ce: 0.8963\n",
      "Epoch: 0 [91/877] [91/2631] lr:0.0004999 Loss: 1.1258 Loss_dice: 0.2344 Loss_ce: 0.8914\n",
      "Epoch: 0 [92/877] [92/2631] lr:0.0004999 Loss: 1.0311 Loss_dice: 0.1928 Loss_ce: 0.8383\n",
      "Epoch: 0 [93/877] [93/2631] lr:0.0004999 Loss: 1.0402 Loss_dice: 0.1899 Loss_ce: 0.8503\n",
      "Epoch: 0 [94/877] [94/2631] lr:0.0004999 Loss: 0.9750 Loss_dice: 0.1578 Loss_ce: 0.8172\n",
      "Epoch: 0 [95/877] [95/2631] lr:0.0004999 Loss: 0.9514 Loss_dice: 0.1438 Loss_ce: 0.8075\n",
      "Epoch: 0 [96/877] [96/2631] lr:0.0004999 Loss: 0.9757 Loss_dice: 0.1626 Loss_ce: 0.8131\n",
      "Epoch: 0 [97/877] [97/2631] lr:0.0004999 Loss: 1.0948 Loss_dice: 0.2257 Loss_ce: 0.8691\n",
      "Epoch: 0 [98/877] [98/2631] lr:0.0004999 Loss: 1.0937 Loss_dice: 0.2217 Loss_ce: 0.8720\n",
      "Epoch: 0 [99/877] [99/2631] lr:0.0004999 Loss: 0.9702 Loss_dice: 0.1645 Loss_ce: 0.8057\n",
      "Epoch: 0 [100/877] [100/2631] lr:0.0004999 Loss: 1.0775 Loss_dice: 0.2101 Loss_ce: 0.8675\n",
      "Epoch: 0 [101/877] [101/2631] lr:0.0004999 Loss: 0.9050 Loss_dice: 0.1293 Loss_ce: 0.7757\n",
      "Epoch: 0 [102/877] [102/2631] lr:0.0004999 Loss: 1.0524 Loss_dice: 0.2143 Loss_ce: 0.8382\n",
      "Epoch: 0 [103/877] [103/2631] lr:0.0004999 Loss: 1.1070 Loss_dice: 0.2407 Loss_ce: 0.8662\n",
      "Epoch: 0 [104/877] [104/2631] lr:0.0004999 Loss: 1.2307 Loss_dice: 0.3118 Loss_ce: 0.9190\n",
      "Epoch: 0 [105/877] [105/2631] lr:0.0004999 Loss: 1.1261 Loss_dice: 0.2616 Loss_ce: 0.8645\n",
      "Epoch: 0 [106/877] [106/2631] lr:0.0004999 Loss: 1.0682 Loss_dice: 0.2242 Loss_ce: 0.8440\n",
      "Epoch: 0 [107/877] [107/2631] lr:0.0004999 Loss: 1.0148 Loss_dice: 0.1966 Loss_ce: 0.8183\n",
      "Epoch: 0 [108/877] [108/2631] lr:0.0004998 Loss: 0.9610 Loss_dice: 0.1719 Loss_ce: 0.7891\n",
      "Epoch: 0 [109/877] [109/2631] lr:0.0004998 Loss: 0.8535 Loss_dice: 0.1058 Loss_ce: 0.7477\n",
      "Epoch: 0 [110/877] [110/2631] lr:0.0004998 Loss: 0.9384 Loss_dice: 0.1610 Loss_ce: 0.7774\n",
      "Epoch: 0 [111/877] [111/2631] lr:0.0004998 Loss: 0.9672 Loss_dice: 0.1869 Loss_ce: 0.7803\n",
      "Epoch: 0 [112/877] [112/2631] lr:0.0004998 Loss: 1.1799 Loss_dice: 0.2902 Loss_ce: 0.8897\n",
      "Epoch: 0 [113/877] [113/2631] lr:0.0004998 Loss: 1.2064 Loss_dice: 0.3056 Loss_ce: 0.9008\n",
      "Epoch: 0 [114/877] [114/2631] lr:0.0004998 Loss: 1.0060 Loss_dice: 0.2112 Loss_ce: 0.7949\n",
      "Epoch: 0 [115/877] [115/2631] lr:0.0004998 Loss: 1.2687 Loss_dice: 0.3471 Loss_ce: 0.9216\n",
      "Epoch: 0 [116/877] [116/2631] lr:0.0004998 Loss: 1.0733 Loss_dice: 0.2562 Loss_ce: 0.8171\n",
      "Epoch: 0 [117/877] [117/2631] lr:0.0004998 Loss: 1.2156 Loss_dice: 0.3333 Loss_ce: 0.8823\n",
      "Epoch: 0 [118/877] [118/2631] lr:0.0004998 Loss: 1.0952 Loss_dice: 0.2780 Loss_ce: 0.8172\n",
      "Epoch: 0 [119/877] [119/2631] lr:0.0004998 Loss: 1.0807 Loss_dice: 0.2634 Loss_ce: 0.8173\n",
      "Epoch: 0 [120/877] [120/2631] lr:0.0004998 Loss: 1.0915 Loss_dice: 0.2592 Loss_ce: 0.8322\n",
      "Epoch: 0 [121/877] [121/2631] lr:0.0004998 Loss: 0.9880 Loss_dice: 0.2036 Loss_ce: 0.7845\n",
      "Epoch: 0 [122/877] [122/2631] lr:0.0004997 Loss: 0.9810 Loss_dice: 0.1908 Loss_ce: 0.7902\n",
      "Epoch: 0 [123/877] [123/2631] lr:0.0004997 Loss: 1.3276 Loss_dice: 0.3457 Loss_ce: 0.9819\n",
      "Epoch: 0 [124/877] [124/2631] lr:0.0004997 Loss: 0.9216 Loss_dice: 0.1608 Loss_ce: 0.7608\n",
      "Epoch: 0 [125/877] [125/2631] lr:0.0004997 Loss: 1.0929 Loss_dice: 0.2467 Loss_ce: 0.8462\n",
      "Epoch: 0 [126/877] [126/2631] lr:0.0004997 Loss: 0.9393 Loss_dice: 0.1822 Loss_ce: 0.7571\n",
      "Epoch: 0 [127/877] [127/2631] lr:0.0004997 Loss: 0.8922 Loss_dice: 0.1621 Loss_ce: 0.7301\n",
      "Epoch: 0 [128/877] [128/2631] lr:0.0004997 Loss: 0.9171 Loss_dice: 0.1771 Loss_ce: 0.7400\n",
      "Epoch: 0 [129/877] [129/2631] lr:0.0004997 Loss: 0.8728 Loss_dice: 0.1614 Loss_ce: 0.7115\n",
      "Epoch: 0 [130/877] [130/2631] lr:0.0004997 Loss: 0.9011 Loss_dice: 0.1813 Loss_ce: 0.7198\n",
      "Epoch: 0 [131/877] [131/2631] lr:0.0004997 Loss: 0.9020 Loss_dice: 0.1806 Loss_ce: 0.7214\n",
      "Epoch: 0 [132/877] [132/2631] lr:0.0004997 Loss: 0.9082 Loss_dice: 0.1491 Loss_ce: 0.7590\n",
      "Epoch: 0 [133/877] [133/2631] lr:0.0004997 Loss: 0.8475 Loss_dice: 0.1515 Loss_ce: 0.6960\n",
      "Epoch: 0 [134/877] [134/2631] lr:0.0004996 Loss: 0.8091 Loss_dice: 0.1107 Loss_ce: 0.6984\n",
      "Epoch: 0 [135/877] [135/2631] lr:0.0004996 Loss: 0.8642 Loss_dice: 0.1701 Loss_ce: 0.6941\n",
      "Epoch: 0 [136/877] [136/2631] lr:0.0004996 Loss: 0.9728 Loss_dice: 0.2169 Loss_ce: 0.7559\n",
      "Epoch: 0 [137/877] [137/2631] lr:0.0004996 Loss: 0.9362 Loss_dice: 0.2063 Loss_ce: 0.7299\n",
      "Epoch: 0 [138/877] [138/2631] lr:0.0004996 Loss: 0.9060 Loss_dice: 0.2025 Loss_ce: 0.7035\n",
      "Epoch: 0 [139/877] [139/2631] lr:0.0004996 Loss: 0.8636 Loss_dice: 0.1851 Loss_ce: 0.6785\n",
      "Epoch: 0 [140/877] [140/2631] lr:0.0004996 Loss: 0.9851 Loss_dice: 0.2430 Loss_ce: 0.7421\n",
      "Epoch: 0 [141/877] [141/2631] lr:0.0004996 Loss: 0.9812 Loss_dice: 0.2338 Loss_ce: 0.7474\n",
      "Epoch: 0 [142/877] [142/2631] lr:0.0004996 Loss: 1.1786 Loss_dice: 0.3192 Loss_ce: 0.8594\n",
      "Epoch: 0 [143/877] [143/2631] lr:0.0004996 Loss: 0.7834 Loss_dice: 0.1368 Loss_ce: 0.6466\n",
      "Epoch: 0 [144/877] [144/2631] lr:0.0004995 Loss: 0.7972 Loss_dice: 0.1390 Loss_ce: 0.6583\n",
      "Epoch: 0 [145/877] [145/2631] lr:0.0004995 Loss: 0.8148 Loss_dice: 0.1564 Loss_ce: 0.6584\n",
      "Epoch: 0 [146/877] [146/2631] lr:0.0004995 Loss: 0.9699 Loss_dice: 0.2224 Loss_ce: 0.7475\n",
      "Epoch: 0 [147/877] [147/2631] lr:0.0004995 Loss: 1.1981 Loss_dice: 0.3240 Loss_ce: 0.8741\n",
      "Epoch: 0 [148/877] [148/2631] lr:0.0004995 Loss: 1.1698 Loss_dice: 0.3274 Loss_ce: 0.8424\n",
      "Epoch: 0 [149/877] [149/2631] lr:0.0004995 Loss: 0.9887 Loss_dice: 0.2511 Loss_ce: 0.7376\n",
      "Epoch: 0 [150/877] [150/2631] lr:0.0004995 Loss: 0.9651 Loss_dice: 0.2367 Loss_ce: 0.7284\n",
      "Epoch: 0 [151/877] [151/2631] lr:0.0004995 Loss: 1.1833 Loss_dice: 0.3451 Loss_ce: 0.8382\n",
      "Epoch: 0 [152/877] [152/2631] lr:0.0004995 Loss: 0.9936 Loss_dice: 0.2619 Loss_ce: 0.7318\n",
      "Epoch: 0 [153/877] [153/2631] lr:0.0004994 Loss: 0.8549 Loss_dice: 0.1975 Loss_ce: 0.6574\n",
      "Epoch: 0 [154/877] [154/2631] lr:0.0004994 Loss: 0.8689 Loss_dice: 0.1987 Loss_ce: 0.6702\n",
      "Epoch: 0 [155/877] [155/2631] lr:0.0004994 Loss: 1.0646 Loss_dice: 0.2700 Loss_ce: 0.7946\n",
      "Epoch: 0 [156/877] [156/2631] lr:0.0004994 Loss: 1.0063 Loss_dice: 0.2664 Loss_ce: 0.7399\n",
      "Epoch: 0 [157/877] [157/2631] lr:0.0004994 Loss: 1.0867 Loss_dice: 0.2941 Loss_ce: 0.7926\n",
      "Epoch: 0 [158/877] [158/2631] lr:0.0004994 Loss: 0.8893 Loss_dice: 0.1874 Loss_ce: 0.7019\n",
      "Epoch: 0 [159/877] [159/2631] lr:0.0004994 Loss: 1.0466 Loss_dice: 0.2651 Loss_ce: 0.7815\n",
      "Epoch: 0 [160/877] [160/2631] lr:0.0004994 Loss: 1.0589 Loss_dice: 0.2656 Loss_ce: 0.7933\n",
      "Epoch: 0 [161/877] [161/2631] lr:0.0004993 Loss: 0.8564 Loss_dice: 0.2024 Loss_ce: 0.6540\n",
      "Epoch: 0 [162/877] [162/2631] lr:0.0004993 Loss: 0.9013 Loss_dice: 0.2348 Loss_ce: 0.6665\n",
      "Epoch: 0 [163/877] [163/2631] lr:0.0004993 Loss: 1.0967 Loss_dice: 0.3250 Loss_ce: 0.7716\n",
      "Epoch: 0 [164/877] [164/2631] lr:0.0004993 Loss: 1.0938 Loss_dice: 0.3227 Loss_ce: 0.7711\n",
      "Epoch: 0 [165/877] [165/2631] lr:0.0004993 Loss: 1.0372 Loss_dice: 0.2983 Loss_ce: 0.7389\n",
      "Epoch: 0 [166/877] [166/2631] lr:0.0004993 Loss: 0.8629 Loss_dice: 0.2236 Loss_ce: 0.6393\n",
      "Epoch: 0 [167/877] [167/2631] lr:0.0004993 Loss: 1.2012 Loss_dice: 0.3699 Loss_ce: 0.8313\n",
      "Epoch: 0 [168/877] [168/2631] lr:0.0004992 Loss: 1.0822 Loss_dice: 0.3197 Loss_ce: 0.7626\n",
      "Epoch: 0 [169/877] [169/2631] lr:0.0004992 Loss: 1.0879 Loss_dice: 0.3239 Loss_ce: 0.7641\n",
      "Epoch: 0 [170/877] [170/2631] lr:0.0004992 Loss: 1.0568 Loss_dice: 0.3079 Loss_ce: 0.7489\n",
      "Epoch: 0 [171/877] [171/2631] lr:0.0004992 Loss: 0.8533 Loss_dice: 0.1953 Loss_ce: 0.6580\n",
      "Epoch: 0 [172/877] [172/2631] lr:0.0004992 Loss: 0.8786 Loss_dice: 0.2160 Loss_ce: 0.6626\n",
      "Epoch: 0 [173/877] [173/2631] lr:0.0004992 Loss: 1.0282 Loss_dice: 0.2847 Loss_ce: 0.7435\n",
      "Epoch: 0 [174/877] [174/2631] lr:0.0004992 Loss: 1.0111 Loss_dice: 0.2729 Loss_ce: 0.7382\n",
      "Epoch: 0 [175/877] [175/2631] lr:0.0004991 Loss: 0.8478 Loss_dice: 0.1951 Loss_ce: 0.6527\n",
      "Epoch: 0 [176/877] [176/2631] lr:0.0004991 Loss: 0.8432 Loss_dice: 0.1985 Loss_ce: 0.6447\n",
      "Epoch: 0 [177/877] [177/2631] lr:0.0004991 Loss: 0.8405 Loss_dice: 0.1907 Loss_ce: 0.6499\n",
      "Epoch: 0 [178/877] [178/2631] lr:0.0004991 Loss: 0.8121 Loss_dice: 0.1823 Loss_ce: 0.6298\n",
      "Epoch: 0 [179/877] [179/2631] lr:0.0004991 Loss: 0.7943 Loss_dice: 0.1718 Loss_ce: 0.6225\n",
      "Epoch: 0 [180/877] [180/2631] lr:0.0004991 Loss: 1.0270 Loss_dice: 0.2845 Loss_ce: 0.7425\n",
      "Epoch: 0 [181/877] [181/2631] lr:0.0004991 Loss: 1.0333 Loss_dice: 0.2876 Loss_ce: 0.7457\n",
      "Epoch: 0 [182/877] [182/2631] lr:0.0004990 Loss: 0.9548 Loss_dice: 0.2726 Loss_ce: 0.6822\n",
      "Epoch: 0 [183/877] [183/2631] lr:0.0004990 Loss: 0.8259 Loss_dice: 0.2021 Loss_ce: 0.6239\n",
      "Epoch: 0 [184/877] [184/2631] lr:0.0004990 Loss: 1.0836 Loss_dice: 0.3306 Loss_ce: 0.7530\n",
      "Epoch: 0 [185/877] [185/2631] lr:0.0004990 Loss: 0.9148 Loss_dice: 0.2573 Loss_ce: 0.6576\n",
      "Epoch: 0 [186/877] [186/2631] lr:0.0004990 Loss: 0.9664 Loss_dice: 0.2860 Loss_ce: 0.6803\n",
      "Epoch: 0 [187/877] [187/2631] lr:0.0004990 Loss: 1.1497 Loss_dice: 0.3607 Loss_ce: 0.7890\n",
      "Epoch: 0 [188/877] [188/2631] lr:0.0004989 Loss: 0.9761 Loss_dice: 0.2846 Loss_ce: 0.6915\n",
      "Epoch: 0 [189/877] [189/2631] lr:0.0004989 Loss: 0.8562 Loss_dice: 0.2292 Loss_ce: 0.6270\n",
      "Epoch: 0 [190/877] [190/2631] lr:0.0004989 Loss: 0.8872 Loss_dice: 0.1973 Loss_ce: 0.6899\n",
      "Epoch: 0 [191/877] [191/2631] lr:0.0004989 Loss: 1.0775 Loss_dice: 0.3298 Loss_ce: 0.7477\n",
      "Epoch: 0 [192/877] [192/2631] lr:0.0004989 Loss: 0.9338 Loss_dice: 0.2399 Loss_ce: 0.6939\n",
      "Epoch: 0 [193/877] [193/2631] lr:0.0004989 Loss: 0.9863 Loss_dice: 0.2843 Loss_ce: 0.7020\n",
      "Epoch: 0 [194/877] [194/2631] lr:0.0004988 Loss: 0.9989 Loss_dice: 0.2818 Loss_ce: 0.7172\n",
      "Epoch: 0 [195/877] [195/2631] lr:0.0004988 Loss: 0.8270 Loss_dice: 0.1844 Loss_ce: 0.6426\n",
      "Epoch: 0 [196/877] [196/2631] lr:0.0004988 Loss: 0.8170 Loss_dice: 0.1919 Loss_ce: 0.6251\n",
      "Epoch: 0 [197/877] [197/2631] lr:0.0004988 Loss: 0.8387 Loss_dice: 0.2092 Loss_ce: 0.6295\n",
      "Epoch: 0 [198/877] [198/2631] lr:0.0004988 Loss: 0.9084 Loss_dice: 0.2356 Loss_ce: 0.6728\n",
      "Epoch: 0 [199/877] [199/2631] lr:0.0004988 Loss: 0.9742 Loss_dice: 0.2584 Loss_ce: 0.7158\n",
      "Epoch: 0 [200/877] [200/2631] lr:0.0004987 Loss: 1.5269 Loss_dice: 0.4608 Loss_ce: 1.0661\n",
      "Epoch: 0 [201/877] [201/2631] lr:0.0004987 Loss: 0.8453 Loss_dice: 0.2076 Loss_ce: 0.6377\n",
      "Epoch: 0 [202/877] [202/2631] lr:0.0004987 Loss: 0.7840 Loss_dice: 0.1773 Loss_ce: 0.6067\n",
      "Epoch: 0 [203/877] [203/2631] lr:0.0004987 Loss: 0.8028 Loss_dice: 0.1578 Loss_ce: 0.6450\n",
      "Epoch: 0 [204/877] [204/2631] lr:0.0004987 Loss: 0.9220 Loss_dice: 0.2284 Loss_ce: 0.6935\n",
      "Epoch: 0 [205/877] [205/2631] lr:0.0004986 Loss: 0.7953 Loss_dice: 0.1728 Loss_ce: 0.6225\n",
      "Epoch: 0 [206/877] [206/2631] lr:0.0004986 Loss: 0.8962 Loss_dice: 0.2370 Loss_ce: 0.6591\n",
      "Epoch: 0 [207/877] [207/2631] lr:0.0004986 Loss: 1.0161 Loss_dice: 0.2967 Loss_ce: 0.7194\n",
      "Epoch: 0 [208/877] [208/2631] lr:0.0004986 Loss: 1.0176 Loss_dice: 0.2960 Loss_ce: 0.7215\n",
      "Epoch: 0 [209/877] [209/2631] lr:0.0004986 Loss: 1.1496 Loss_dice: 0.3485 Loss_ce: 0.8011\n",
      "Epoch: 0 [210/877] [210/2631] lr:0.0004985 Loss: 1.0955 Loss_dice: 0.3228 Loss_ce: 0.7727\n",
      "Epoch: 0 [211/877] [211/2631] lr:0.0004985 Loss: 1.0781 Loss_dice: 0.3089 Loss_ce: 0.7692\n",
      "Epoch: 0 [212/877] [212/2631] lr:0.0004985 Loss: 0.8285 Loss_dice: 0.2017 Loss_ce: 0.6268\n",
      "Epoch: 0 [213/877] [213/2631] lr:0.0004985 Loss: 0.8234 Loss_dice: 0.1938 Loss_ce: 0.6296\n",
      "Epoch: 0 [214/877] [214/2631] lr:0.0004985 Loss: 0.7451 Loss_dice: 0.1514 Loss_ce: 0.5937\n",
      "Epoch: 0 [215/877] [215/2631] lr:0.0004985 Loss: 1.0961 Loss_dice: 0.2973 Loss_ce: 0.7988\n",
      "Epoch: 0 [216/877] [216/2631] lr:0.0004984 Loss: 0.8095 Loss_dice: 0.1840 Loss_ce: 0.6255\n",
      "Epoch: 0 [217/877] [217/2631] lr:0.0004984 Loss: 0.8207 Loss_dice: 0.2008 Loss_ce: 0.6200\n",
      "Epoch: 0 [218/877] [218/2631] lr:0.0004984 Loss: 0.9180 Loss_dice: 0.2416 Loss_ce: 0.6764\n",
      "Epoch: 0 [219/877] [219/2631] lr:0.0004984 Loss: 0.9123 Loss_dice: 0.2478 Loss_ce: 0.6645\n",
      "Epoch: 0 [220/877] [220/2631] lr:0.0004983 Loss: 0.8213 Loss_dice: 0.2102 Loss_ce: 0.6112\n",
      "Epoch: 0 [221/877] [221/2631] lr:0.0004983 Loss: 0.8285 Loss_dice: 0.2149 Loss_ce: 0.6136\n",
      "Epoch: 0 [222/877] [222/2631] lr:0.0004983 Loss: 0.7934 Loss_dice: 0.1800 Loss_ce: 0.6134\n",
      "Epoch: 0 [223/877] [223/2631] lr:0.0004983 Loss: 0.7546 Loss_dice: 0.1879 Loss_ce: 0.5667\n",
      "Epoch: 0 [224/877] [224/2631] lr:0.0004983 Loss: 0.7533 Loss_dice: 0.1858 Loss_ce: 0.5675\n",
      "Epoch: 0 [225/877] [225/2631] lr:0.0004982 Loss: 0.8512 Loss_dice: 0.2276 Loss_ce: 0.6236\n",
      "Epoch: 0 [226/877] [226/2631] lr:0.0004982 Loss: 0.7869 Loss_dice: 0.2068 Loss_ce: 0.5801\n",
      "Epoch: 0 [227/877] [227/2631] lr:0.0004982 Loss: 0.7187 Loss_dice: 0.1842 Loss_ce: 0.5345\n",
      "Epoch: 0 [228/877] [228/2631] lr:0.0004982 Loss: 0.8771 Loss_dice: 0.2492 Loss_ce: 0.6279\n",
      "Epoch: 0 [229/877] [229/2631] lr:0.0004982 Loss: 0.8042 Loss_dice: 0.2151 Loss_ce: 0.5892\n",
      "Epoch: 0 [230/877] [230/2631] lr:0.0004981 Loss: 0.7635 Loss_dice: 0.1969 Loss_ce: 0.5666\n",
      "Epoch: 0 [231/877] [231/2631] lr:0.0004981 Loss: 0.7553 Loss_dice: 0.2026 Loss_ce: 0.5528\n",
      "Epoch: 0 [232/877] [232/2631] lr:0.0004981 Loss: 0.8905 Loss_dice: 0.2563 Loss_ce: 0.6342\n",
      "Epoch: 0 [233/877] [233/2631] lr:0.0004981 Loss: 0.7654 Loss_dice: 0.2055 Loss_ce: 0.5599\n",
      "Epoch: 0 [234/877] [234/2631] lr:0.0004980 Loss: 0.8090 Loss_dice: 0.2255 Loss_ce: 0.5835\n",
      "Epoch: 0 [235/877] [235/2631] lr:0.0004980 Loss: 0.8895 Loss_dice: 0.2651 Loss_ce: 0.6244\n",
      "Epoch: 0 [236/877] [236/2631] lr:0.0004980 Loss: 0.7249 Loss_dice: 0.1950 Loss_ce: 0.5299\n",
      "Epoch: 0 [237/877] [237/2631] lr:0.0004980 Loss: 0.7639 Loss_dice: 0.1873 Loss_ce: 0.5766\n",
      "Epoch: 0 [238/877] [238/2631] lr:0.0004980 Loss: 0.7108 Loss_dice: 0.1959 Loss_ce: 0.5149\n",
      "Epoch: 0 [239/877] [239/2631] lr:0.0004979 Loss: 0.9797 Loss_dice: 0.3034 Loss_ce: 0.6763\n",
      "Epoch: 0 [240/877] [240/2631] lr:0.0004979 Loss: 0.8011 Loss_dice: 0.2141 Loss_ce: 0.5870\n",
      "Epoch: 0 [241/877] [241/2631] lr:0.0004979 Loss: 0.8399 Loss_dice: 0.2553 Loss_ce: 0.5846\n",
      "Epoch: 0 [242/877] [242/2631] lr:0.0004979 Loss: 0.9077 Loss_dice: 0.2829 Loss_ce: 0.6248\n",
      "Epoch: 0 [243/877] [243/2631] lr:0.0004978 Loss: 0.7937 Loss_dice: 0.2466 Loss_ce: 0.5471\n",
      "Epoch: 0 [244/877] [244/2631] lr:0.0004978 Loss: 0.8631 Loss_dice: 0.2770 Loss_ce: 0.5861\n",
      "Epoch: 0 [245/877] [245/2631] lr:0.0004978 Loss: 0.8047 Loss_dice: 0.2483 Loss_ce: 0.5564\n",
      "Epoch: 0 [246/877] [246/2631] lr:0.0004978 Loss: 0.8397 Loss_dice: 0.2696 Loss_ce: 0.5701\n",
      "Epoch: 0 [247/877] [247/2631] lr:0.0004977 Loss: 0.9065 Loss_dice: 0.2887 Loss_ce: 0.6178\n",
      "Epoch: 0 [248/877] [248/2631] lr:0.0004977 Loss: 0.9589 Loss_dice: 0.3141 Loss_ce: 0.6448\n",
      "Epoch: 0 [249/877] [249/2631] lr:0.0004977 Loss: 0.7371 Loss_dice: 0.2045 Loss_ce: 0.5326\n",
      "Epoch: 0 [250/877] [250/2631] lr:0.0004977 Loss: 0.7578 Loss_dice: 0.2184 Loss_ce: 0.5394\n",
      "Epoch: 0 [251/877] [251/2631] lr:0.0004977 Loss: 0.7897 Loss_dice: 0.2471 Loss_ce: 0.5426\n",
      "Epoch: 0 [252/877] [252/2631] lr:0.0004976 Loss: 0.8403 Loss_dice: 0.2684 Loss_ce: 0.5719\n",
      "Epoch: 0 [253/877] [253/2631] lr:0.0004976 Loss: 0.8184 Loss_dice: 0.2552 Loss_ce: 0.5631\n",
      "Epoch: 0 [254/877] [254/2631] lr:0.0004976 Loss: 0.7248 Loss_dice: 0.2093 Loss_ce: 0.5156\n",
      "Epoch: 0 [255/877] [255/2631] lr:0.0004976 Loss: 0.7906 Loss_dice: 0.2402 Loss_ce: 0.5504\n",
      "Epoch: 0 [256/877] [256/2631] lr:0.0004975 Loss: 0.7914 Loss_dice: 0.2407 Loss_ce: 0.5506\n",
      "Epoch: 0 [257/877] [257/2631] lr:0.0004975 Loss: 0.8306 Loss_dice: 0.2596 Loss_ce: 0.5709\n",
      "Epoch: 0 [258/877] [258/2631] lr:0.0004975 Loss: 0.8025 Loss_dice: 0.2581 Loss_ce: 0.5444\n",
      "Epoch: 0 [259/877] [259/2631] lr:0.0004975 Loss: 0.9755 Loss_dice: 0.3173 Loss_ce: 0.6582\n",
      "Epoch: 0 [260/877] [260/2631] lr:0.0004974 Loss: 0.8656 Loss_dice: 0.2804 Loss_ce: 0.5852\n",
      "Epoch: 0 [261/877] [261/2631] lr:0.0004974 Loss: 0.7589 Loss_dice: 0.2407 Loss_ce: 0.5183\n",
      "Epoch: 0 [262/877] [262/2631] lr:0.0004974 Loss: 0.7799 Loss_dice: 0.2414 Loss_ce: 0.5385\n",
      "Epoch: 0 [263/877] [263/2631] lr:0.0004973 Loss: 0.7993 Loss_dice: 0.2620 Loss_ce: 0.5373\n",
      "Epoch: 0 [264/877] [264/2631] lr:0.0004973 Loss: 0.8345 Loss_dice: 0.2811 Loss_ce: 0.5534\n",
      "Epoch: 0 [265/877] [265/2631] lr:0.0004973 Loss: 1.1745 Loss_dice: 0.3924 Loss_ce: 0.7820\n",
      "Epoch: 0 [266/877] [266/2631] lr:0.0004973 Loss: 0.7558 Loss_dice: 0.2465 Loss_ce: 0.5093\n",
      "Epoch: 0 [267/877] [267/2631] lr:0.0004972 Loss: 0.9413 Loss_dice: 0.3258 Loss_ce: 0.6155\n",
      "Epoch: 0 [268/877] [268/2631] lr:0.0004972 Loss: 0.9627 Loss_dice: 0.3289 Loss_ce: 0.6338\n",
      "Epoch: 0 [269/877] [269/2631] lr:0.0004972 Loss: 0.9131 Loss_dice: 0.2898 Loss_ce: 0.6233\n",
      "Epoch: 0 [270/877] [270/2631] lr:0.0004972 Loss: 0.8201 Loss_dice: 0.2653 Loss_ce: 0.5548\n",
      "Epoch: 0 [271/877] [271/2631] lr:0.0004971 Loss: 0.9981 Loss_dice: 0.3343 Loss_ce: 0.6638\n",
      "Epoch: 0 [272/877] [272/2631] lr:0.0004971 Loss: 0.8428 Loss_dice: 0.2774 Loss_ce: 0.5654\n",
      "Epoch: 0 [273/877] [273/2631] lr:0.0004971 Loss: 0.9462 Loss_dice: 0.2990 Loss_ce: 0.6472\n",
      "Epoch: 0 [274/877] [274/2631] lr:0.0004971 Loss: 0.9041 Loss_dice: 0.2820 Loss_ce: 0.6221\n",
      "Epoch: 0 [275/877] [275/2631] lr:0.0004970 Loss: 0.9019 Loss_dice: 0.2699 Loss_ce: 0.6320\n",
      "Epoch: 0 [276/877] [276/2631] lr:0.0004970 Loss: 1.0458 Loss_dice: 0.3231 Loss_ce: 0.7227\n",
      "Epoch: 0 [277/877] [277/2631] lr:0.0004970 Loss: 0.9383 Loss_dice: 0.2883 Loss_ce: 0.6500\n",
      "Epoch: 0 [278/877] [278/2631] lr:0.0004969 Loss: 0.8591 Loss_dice: 0.2649 Loss_ce: 0.5942\n",
      "Epoch: 0 [279/877] [279/2631] lr:0.0004969 Loss: 0.8415 Loss_dice: 0.2557 Loss_ce: 0.5858\n",
      "Epoch: 0 [280/877] [280/2631] lr:0.0004969 Loss: 1.0297 Loss_dice: 0.3175 Loss_ce: 0.7122\n",
      "Epoch: 0 [281/877] [281/2631] lr:0.0004969 Loss: 0.7781 Loss_dice: 0.2273 Loss_ce: 0.5507\n",
      "Epoch: 0 [282/877] [282/2631] lr:0.0004968 Loss: 0.8343 Loss_dice: 0.2520 Loss_ce: 0.5824\n",
      "Epoch: 0 [283/877] [283/2631] lr:0.0004968 Loss: 0.7788 Loss_dice: 0.2258 Loss_ce: 0.5530\n",
      "Epoch: 0 [284/877] [284/2631] lr:0.0004968 Loss: 0.7555 Loss_dice: 0.2091 Loss_ce: 0.5464\n",
      "Epoch: 0 [285/877] [285/2631] lr:0.0004967 Loss: 0.8013 Loss_dice: 0.2413 Loss_ce: 0.5600\n",
      "Epoch: 0 [286/877] [286/2631] lr:0.0004967 Loss: 0.8382 Loss_dice: 0.2480 Loss_ce: 0.5902\n",
      "Epoch: 0 [287/877] [287/2631] lr:0.0004967 Loss: 0.7582 Loss_dice: 0.2197 Loss_ce: 0.5385\n",
      "Epoch: 0 [288/877] [288/2631] lr:0.0004967 Loss: 0.8194 Loss_dice: 0.2531 Loss_ce: 0.5663\n",
      "Epoch: 0 [289/877] [289/2631] lr:0.0004966 Loss: 0.7339 Loss_dice: 0.2053 Loss_ce: 0.5286\n",
      "Epoch: 0 [290/877] [290/2631] lr:0.0004966 Loss: 0.7556 Loss_dice: 0.2121 Loss_ce: 0.5435\n",
      "Epoch: 0 [291/877] [291/2631] lr:0.0004966 Loss: 0.8824 Loss_dice: 0.2694 Loss_ce: 0.6129\n",
      "Epoch: 0 [292/877] [292/2631] lr:0.0004965 Loss: 0.7816 Loss_dice: 0.2318 Loss_ce: 0.5498\n",
      "Epoch: 0 [293/877] [293/2631] lr:0.0004965 Loss: 0.7336 Loss_dice: 0.2204 Loss_ce: 0.5133\n",
      "Epoch: 0 [294/877] [294/2631] lr:0.0004965 Loss: 0.7847 Loss_dice: 0.2461 Loss_ce: 0.5386\n",
      "Epoch: 0 [295/877] [295/2631] lr:0.0004965 Loss: 0.8579 Loss_dice: 0.2784 Loss_ce: 0.5794\n",
      "Epoch: 0 [296/877] [296/2631] lr:0.0004964 Loss: 0.8846 Loss_dice: 0.2875 Loss_ce: 0.5971\n",
      "Epoch: 0 [297/877] [297/2631] lr:0.0004964 Loss: 0.9689 Loss_dice: 0.3201 Loss_ce: 0.6488\n",
      "Epoch: 0 [298/877] [298/2631] lr:0.0004964 Loss: 0.7247 Loss_dice: 0.2177 Loss_ce: 0.5070\n",
      "Epoch: 0 [299/877] [299/2631] lr:0.0004963 Loss: 0.8308 Loss_dice: 0.2733 Loss_ce: 0.5575\n",
      "Epoch: 0 [300/877] [300/2631] lr:0.0004963 Loss: 0.7433 Loss_dice: 0.2324 Loss_ce: 0.5110\n",
      "Epoch: 0 [301/877] [301/2631] lr:0.0004963 Loss: 0.8118 Loss_dice: 0.2653 Loss_ce: 0.5465\n",
      "Epoch: 0 [302/877] [302/2631] lr:0.0004962 Loss: 0.7219 Loss_dice: 0.2029 Loss_ce: 0.5190\n",
      "Epoch: 0 [303/877] [303/2631] lr:0.0004962 Loss: 0.7733 Loss_dice: 0.2433 Loss_ce: 0.5300\n",
      "Epoch: 0 [304/877] [304/2631] lr:0.0004962 Loss: 0.7711 Loss_dice: 0.2502 Loss_ce: 0.5209\n",
      "Epoch: 0 [305/877] [305/2631] lr:0.0004961 Loss: 0.8082 Loss_dice: 0.2564 Loss_ce: 0.5518\n",
      "Epoch: 0 [306/877] [306/2631] lr:0.0004961 Loss: 0.7983 Loss_dice: 0.2613 Loss_ce: 0.5370\n",
      "Epoch: 0 [307/877] [307/2631] lr:0.0004961 Loss: 0.7344 Loss_dice: 0.2307 Loss_ce: 0.5038\n",
      "Epoch: 0 [308/877] [308/2631] lr:0.0004960 Loss: 0.7737 Loss_dice: 0.2490 Loss_ce: 0.5247\n",
      "Epoch: 0 [309/877] [309/2631] lr:0.0004960 Loss: 0.9864 Loss_dice: 0.3262 Loss_ce: 0.6602\n",
      "Epoch: 0 [310/877] [310/2631] lr:0.0004960 Loss: 0.8007 Loss_dice: 0.2626 Loss_ce: 0.5381\n",
      "Epoch: 0 [311/877] [311/2631] lr:0.0004960 Loss: 0.7662 Loss_dice: 0.2600 Loss_ce: 0.5062\n",
      "Epoch: 0 [312/877] [312/2631] lr:0.0004959 Loss: 0.7277 Loss_dice: 0.2403 Loss_ce: 0.4875\n",
      "Epoch: 0 [313/877] [313/2631] lr:0.0004959 Loss: 0.9397 Loss_dice: 0.3245 Loss_ce: 0.6152\n",
      "Epoch: 0 [314/877] [314/2631] lr:0.0004959 Loss: 0.9358 Loss_dice: 0.3264 Loss_ce: 0.6093\n",
      "Epoch: 0 [315/877] [315/2631] lr:0.0004958 Loss: 0.7268 Loss_dice: 0.2326 Loss_ce: 0.4942\n",
      "Epoch: 0 [316/877] [316/2631] lr:0.0004958 Loss: 1.0179 Loss_dice: 0.3416 Loss_ce: 0.6764\n",
      "Epoch: 0 [317/877] [317/2631] lr:0.0004958 Loss: 0.7942 Loss_dice: 0.2679 Loss_ce: 0.5263\n",
      "Epoch: 0 [318/877] [318/2631] lr:0.0004957 Loss: 0.7788 Loss_dice: 0.2603 Loss_ce: 0.5185\n",
      "Epoch: 0 [319/877] [319/2631] lr:0.0004957 Loss: 0.8856 Loss_dice: 0.3015 Loss_ce: 0.5841\n",
      "Epoch: 0 [320/877] [320/2631] lr:0.0004957 Loss: 0.9879 Loss_dice: 0.3346 Loss_ce: 0.6532\n",
      "Epoch: 0 [321/877] [321/2631] lr:0.0004956 Loss: 1.1122 Loss_dice: 0.3744 Loss_ce: 0.7378\n",
      "Epoch: 0 [322/877] [322/2631] lr:0.0004956 Loss: 0.8955 Loss_dice: 0.3120 Loss_ce: 0.5834\n",
      "Epoch: 0 [323/877] [323/2631] lr:0.0004956 Loss: 1.0156 Loss_dice: 0.3319 Loss_ce: 0.6837\n",
      "Epoch: 0 [324/877] [324/2631] lr:0.0004955 Loss: 0.9270 Loss_dice: 0.3146 Loss_ce: 0.6124\n",
      "Epoch: 0 [325/877] [325/2631] lr:0.0004955 Loss: 0.8471 Loss_dice: 0.2924 Loss_ce: 0.5546\n",
      "Epoch: 0 [326/877] [326/2631] lr:0.0004955 Loss: 0.8952 Loss_dice: 0.2974 Loss_ce: 0.5978\n",
      "Epoch: 0 [327/877] [327/2631] lr:0.0004954 Loss: 0.8311 Loss_dice: 0.2741 Loss_ce: 0.5570\n",
      "Epoch: 0 [328/877] [328/2631] lr:0.0004954 Loss: 0.8736 Loss_dice: 0.2861 Loss_ce: 0.5876\n",
      "Epoch: 0 [329/877] [329/2631] lr:0.0004954 Loss: 0.7554 Loss_dice: 0.2300 Loss_ce: 0.5254\n",
      "Epoch: 0 [330/877] [330/2631] lr:0.0004953 Loss: 0.7894 Loss_dice: 0.2443 Loss_ce: 0.5450\n",
      "Epoch: 0 [331/877] [331/2631] lr:0.0004953 Loss: 0.8115 Loss_dice: 0.2508 Loss_ce: 0.5608\n",
      "Epoch: 0 [332/877] [332/2631] lr:0.0004953 Loss: 1.3476 Loss_dice: 0.4244 Loss_ce: 0.9232\n",
      "Epoch: 0 [333/877] [333/2631] lr:0.0004952 Loss: 0.7177 Loss_dice: 0.2148 Loss_ce: 0.5029\n",
      "Epoch: 0 [334/877] [334/2631] lr:0.0004952 Loss: 0.8087 Loss_dice: 0.2588 Loss_ce: 0.5498\n",
      "Epoch: 0 [335/877] [335/2631] lr:0.0004951 Loss: 0.8632 Loss_dice: 0.2732 Loss_ce: 0.5901\n",
      "Epoch: 0 [336/877] [336/2631] lr:0.0004951 Loss: 1.0533 Loss_dice: 0.3399 Loss_ce: 0.7133\n",
      "Epoch: 0 [337/877] [337/2631] lr:0.0004951 Loss: 0.9142 Loss_dice: 0.2954 Loss_ce: 0.6188\n",
      "Epoch: 0 [338/877] [338/2631] lr:0.0004950 Loss: 0.9607 Loss_dice: 0.3119 Loss_ce: 0.6488\n",
      "Epoch: 0 [339/877] [339/2631] lr:0.0004950 Loss: 1.0028 Loss_dice: 0.3194 Loss_ce: 0.6834\n",
      "Epoch: 0 [340/877] [340/2631] lr:0.0004950 Loss: 0.7395 Loss_dice: 0.2296 Loss_ce: 0.5098\n",
      "Epoch: 0 [341/877] [341/2631] lr:0.0004949 Loss: 0.9903 Loss_dice: 0.3175 Loss_ce: 0.6728\n",
      "Epoch: 0 [342/877] [342/2631] lr:0.0004949 Loss: 0.7977 Loss_dice: 0.2552 Loss_ce: 0.5424\n",
      "Epoch: 0 [343/877] [343/2631] lr:0.0004949 Loss: 0.8319 Loss_dice: 0.2664 Loss_ce: 0.5655\n",
      "Epoch: 0 [344/877] [344/2631] lr:0.0004948 Loss: 0.8872 Loss_dice: 0.2831 Loss_ce: 0.6040\n",
      "Epoch: 0 [345/877] [345/2631] lr:0.0004948 Loss: 0.7454 Loss_dice: 0.2354 Loss_ce: 0.5100\n",
      "Epoch: 0 [346/877] [346/2631] lr:0.0004948 Loss: 0.7746 Loss_dice: 0.2413 Loss_ce: 0.5333\n",
      "Epoch: 0 [347/877] [347/2631] lr:0.0004947 Loss: 0.7644 Loss_dice: 0.2407 Loss_ce: 0.5237\n",
      "Epoch: 0 [348/877] [348/2631] lr:0.0004947 Loss: 0.8653 Loss_dice: 0.2787 Loss_ce: 0.5866\n",
      "Epoch: 0 [349/877] [349/2631] lr:0.0004946 Loss: 0.7776 Loss_dice: 0.2553 Loss_ce: 0.5223\n",
      "Epoch: 0 [350/877] [350/2631] lr:0.0004946 Loss: 0.8936 Loss_dice: 0.2868 Loss_ce: 0.6069\n",
      "Epoch: 0 [351/877] [351/2631] lr:0.0004946 Loss: 0.7673 Loss_dice: 0.2447 Loss_ce: 0.5226\n",
      "Epoch: 0 [352/877] [352/2631] lr:0.0004945 Loss: 0.8838 Loss_dice: 0.2885 Loss_ce: 0.5953\n",
      "Epoch: 0 [353/877] [353/2631] lr:0.0004945 Loss: 0.8785 Loss_dice: 0.2783 Loss_ce: 0.6003\n",
      "Epoch: 0 [354/877] [354/2631] lr:0.0004945 Loss: 0.7925 Loss_dice: 0.2580 Loss_ce: 0.5345\n",
      "Epoch: 0 [355/877] [355/2631] lr:0.0004944 Loss: 1.3384 Loss_dice: 0.4267 Loss_ce: 0.9118\n",
      "Epoch: 0 [356/877] [356/2631] lr:0.0004944 Loss: 0.8257 Loss_dice: 0.2753 Loss_ce: 0.5504\n",
      "Epoch: 0 [357/877] [357/2631] lr:0.0004943 Loss: 0.7587 Loss_dice: 0.2451 Loss_ce: 0.5136\n",
      "Epoch: 0 [358/877] [358/2631] lr:0.0004943 Loss: 0.8480 Loss_dice: 0.2812 Loss_ce: 0.5669\n",
      "Epoch: 0 [359/877] [359/2631] lr:0.0004943 Loss: 0.8492 Loss_dice: 0.2791 Loss_ce: 0.5701\n",
      "Epoch: 0 [360/877] [360/2631] lr:0.0004942 Loss: 0.7972 Loss_dice: 0.2611 Loss_ce: 0.5361\n",
      "Epoch: 0 [361/877] [361/2631] lr:0.0004942 Loss: 0.9242 Loss_dice: 0.3016 Loss_ce: 0.6226\n",
      "Epoch: 0 [362/877] [362/2631] lr:0.0004942 Loss: 0.7838 Loss_dice: 0.2548 Loss_ce: 0.5290\n",
      "Epoch: 0 [363/877] [363/2631] lr:0.0004941 Loss: 0.8269 Loss_dice: 0.2341 Loss_ce: 0.5928\n",
      "Epoch: 0 [364/877] [364/2631] lr:0.0004941 Loss: 0.7283 Loss_dice: 0.2186 Loss_ce: 0.5096\n",
      "Epoch: 0 [365/877] [365/2631] lr:0.0004940 Loss: 0.7406 Loss_dice: 0.2266 Loss_ce: 0.5139\n",
      "Epoch: 0 [366/877] [366/2631] lr:0.0004940 Loss: 0.8696 Loss_dice: 0.2658 Loss_ce: 0.6037\n",
      "Epoch: 0 [367/877] [367/2631] lr:0.0004940 Loss: 0.7342 Loss_dice: 0.2171 Loss_ce: 0.5170\n",
      "Epoch: 0 [368/877] [368/2631] lr:0.0004939 Loss: 0.7841 Loss_dice: 0.2417 Loss_ce: 0.5424\n",
      "Epoch: 0 [369/877] [369/2631] lr:0.0004939 Loss: 0.7355 Loss_dice: 0.2210 Loss_ce: 0.5146\n",
      "Epoch: 0 [370/877] [370/2631] lr:0.0004938 Loss: 0.8304 Loss_dice: 0.2602 Loss_ce: 0.5702\n",
      "Epoch: 0 [371/877] [371/2631] lr:0.0004938 Loss: 0.7441 Loss_dice: 0.2389 Loss_ce: 0.5052\n",
      "Epoch: 0 [372/877] [372/2631] lr:0.0004938 Loss: 0.7340 Loss_dice: 0.2391 Loss_ce: 0.4949\n",
      "Epoch: 0 [373/877] [373/2631] lr:0.0004937 Loss: 0.8015 Loss_dice: 0.2580 Loss_ce: 0.5434\n",
      "Epoch: 0 [374/877] [374/2631] lr:0.0004937 Loss: 0.7924 Loss_dice: 0.2639 Loss_ce: 0.5285\n",
      "Epoch: 0 [375/877] [375/2631] lr:0.0004936 Loss: 0.7192 Loss_dice: 0.2247 Loss_ce: 0.4945\n",
      "Epoch: 0 [376/877] [376/2631] lr:0.0004936 Loss: 0.6693 Loss_dice: 0.2181 Loss_ce: 0.4512\n",
      "Epoch: 0 [377/877] [377/2631] lr:0.0004936 Loss: 1.2391 Loss_dice: 0.4052 Loss_ce: 0.8339\n",
      "Epoch: 0 [378/877] [378/2631] lr:0.0004935 Loss: 0.8005 Loss_dice: 0.2526 Loss_ce: 0.5478\n",
      "Epoch: 0 [379/877] [379/2631] lr:0.0004935 Loss: 0.7793 Loss_dice: 0.2593 Loss_ce: 0.5200\n",
      "Epoch: 0 [380/877] [380/2631] lr:0.0004934 Loss: 0.7254 Loss_dice: 0.2387 Loss_ce: 0.4867\n",
      "Epoch: 0 [381/877] [381/2631] lr:0.0004934 Loss: 0.7221 Loss_dice: 0.2318 Loss_ce: 0.4902\n",
      "Epoch: 0 [382/877] [382/2631] lr:0.0004934 Loss: 0.6934 Loss_dice: 0.2219 Loss_ce: 0.4715\n",
      "Epoch: 0 [383/877] [383/2631] lr:0.0004933 Loss: 0.7582 Loss_dice: 0.2563 Loss_ce: 0.5019\n",
      "Epoch: 0 [384/877] [384/2631] lr:0.0004933 Loss: 0.7691 Loss_dice: 0.2571 Loss_ce: 0.5120\n",
      "Epoch: 0 [385/877] [385/2631] lr:0.0004932 Loss: 0.8429 Loss_dice: 0.2791 Loss_ce: 0.5638\n",
      "Epoch: 0 [386/877] [386/2631] lr:0.0004932 Loss: 1.1156 Loss_dice: 0.3720 Loss_ce: 0.7436\n",
      "Epoch: 0 [387/877] [387/2631] lr:0.0004931 Loss: 1.1024 Loss_dice: 0.3492 Loss_ce: 0.7532\n",
      "Epoch: 0 [388/877] [388/2631] lr:0.0004931 Loss: 0.9045 Loss_dice: 0.3025 Loss_ce: 0.6020\n",
      "Epoch: 0 [389/877] [389/2631] lr:0.0004931 Loss: 0.7310 Loss_dice: 0.2316 Loss_ce: 0.4994\n",
      "Epoch: 0 [390/877] [390/2631] lr:0.0004930 Loss: 0.7070 Loss_dice: 0.2141 Loss_ce: 0.4929\n",
      "Epoch: 0 [391/877] [391/2631] lr:0.0004930 Loss: 0.7297 Loss_dice: 0.2231 Loss_ce: 0.5066\n",
      "Epoch: 0 [392/877] [392/2631] lr:0.0004929 Loss: 0.7342 Loss_dice: 0.2120 Loss_ce: 0.5222\n",
      "Epoch: 0 [393/877] [393/2631] lr:0.0004929 Loss: 0.9771 Loss_dice: 0.2678 Loss_ce: 0.7092\n",
      "Epoch: 0 [394/877] [394/2631] lr:0.0004929 Loss: 0.9249 Loss_dice: 0.2760 Loss_ce: 0.6489\n",
      "Epoch: 0 [395/877] [395/2631] lr:0.0004928 Loss: 1.0065 Loss_dice: 0.2877 Loss_ce: 0.7189\n",
      "Epoch: 0 [396/877] [396/2631] lr:0.0004928 Loss: 0.8819 Loss_dice: 0.2603 Loss_ce: 0.6216\n",
      "Epoch: 0 [397/877] [397/2631] lr:0.0004927 Loss: 0.9263 Loss_dice: 0.2834 Loss_ce: 0.6429\n",
      "Epoch: 0 [398/877] [398/2631] lr:0.0004927 Loss: 0.7218 Loss_dice: 0.2303 Loss_ce: 0.4915\n",
      "Epoch: 0 [399/877] [399/2631] lr:0.0004926 Loss: 0.9053 Loss_dice: 0.2938 Loss_ce: 0.6115\n",
      "Epoch: 0 [400/877] [400/2631] lr:0.0004926 Loss: 0.8711 Loss_dice: 0.2743 Loss_ce: 0.5968\n",
      "Epoch: 0 [401/877] [401/2631] lr:0.0004926 Loss: 0.8484 Loss_dice: 0.2802 Loss_ce: 0.5683\n",
      "Epoch: 0 [402/877] [402/2631] lr:0.0004925 Loss: 0.8327 Loss_dice: 0.2724 Loss_ce: 0.5604\n",
      "Epoch: 0 [403/877] [403/2631] lr:0.0004925 Loss: 0.9337 Loss_dice: 0.3029 Loss_ce: 0.6308\n",
      "Epoch: 0 [404/877] [404/2631] lr:0.0004924 Loss: 0.8308 Loss_dice: 0.2693 Loss_ce: 0.5615\n",
      "Epoch: 0 [405/877] [405/2631] lr:0.0004924 Loss: 1.0744 Loss_dice: 0.3520 Loss_ce: 0.7225\n",
      "Epoch: 0 [406/877] [406/2631] lr:0.0004923 Loss: 0.7988 Loss_dice: 0.2594 Loss_ce: 0.5394\n",
      "Epoch: 0 [407/877] [407/2631] lr:0.0004923 Loss: 0.8124 Loss_dice: 0.2730 Loss_ce: 0.5394\n",
      "Epoch: 0 [408/877] [408/2631] lr:0.0004922 Loss: 0.7951 Loss_dice: 0.2679 Loss_ce: 0.5272\n",
      "Epoch: 0 [409/877] [409/2631] lr:0.0004922 Loss: 0.7395 Loss_dice: 0.2428 Loss_ce: 0.4966\n",
      "Epoch: 0 [410/877] [410/2631] lr:0.0004922 Loss: 0.7260 Loss_dice: 0.2391 Loss_ce: 0.4870\n",
      "Epoch: 0 [411/877] [411/2631] lr:0.0004921 Loss: 0.7923 Loss_dice: 0.2659 Loss_ce: 0.5264\n",
      "Epoch: 0 [412/877] [412/2631] lr:0.0004921 Loss: 0.8202 Loss_dice: 0.2738 Loss_ce: 0.5464\n",
      "Epoch: 0 [413/877] [413/2631] lr:0.0004920 Loss: 0.8862 Loss_dice: 0.2892 Loss_ce: 0.5970\n",
      "Epoch: 0 [414/877] [414/2631] lr:0.0004920 Loss: 0.8142 Loss_dice: 0.2701 Loss_ce: 0.5441\n",
      "Epoch: 0 [415/877] [415/2631] lr:0.0004919 Loss: 0.6989 Loss_dice: 0.2197 Loss_ce: 0.4792\n",
      "Epoch: 0 [416/877] [416/2631] lr:0.0004919 Loss: 0.7314 Loss_dice: 0.2430 Loss_ce: 0.4884\n",
      "Epoch: 0 [417/877] [417/2631] lr:0.0004918 Loss: 0.7981 Loss_dice: 0.2687 Loss_ce: 0.5295\n",
      "Epoch: 0 [418/877] [418/2631] lr:0.0004918 Loss: 0.6935 Loss_dice: 0.2241 Loss_ce: 0.4695\n",
      "Epoch: 0 [419/877] [419/2631] lr:0.0004918 Loss: 0.7040 Loss_dice: 0.2296 Loss_ce: 0.4744\n",
      "Epoch: 0 [420/877] [420/2631] lr:0.0004917 Loss: 0.7521 Loss_dice: 0.2539 Loss_ce: 0.4983\n",
      "Epoch: 0 [421/877] [421/2631] lr:0.0004917 Loss: 0.7859 Loss_dice: 0.2676 Loss_ce: 0.5183\n",
      "Epoch: 0 [422/877] [422/2631] lr:0.0004916 Loss: 0.9033 Loss_dice: 0.2998 Loss_ce: 0.6035\n",
      "Epoch: 0 [423/877] [423/2631] lr:0.0004916 Loss: 0.8285 Loss_dice: 0.2723 Loss_ce: 0.5563\n",
      "Epoch: 0 [424/877] [424/2631] lr:0.0004915 Loss: 0.7653 Loss_dice: 0.2566 Loss_ce: 0.5087\n",
      "Epoch: 0 [425/877] [425/2631] lr:0.0004915 Loss: 1.0041 Loss_dice: 0.3322 Loss_ce: 0.6720\n",
      "Epoch: 0 [426/877] [426/2631] lr:0.0004914 Loss: 0.7071 Loss_dice: 0.2438 Loss_ce: 0.4632\n",
      "Epoch: 0 [427/877] [427/2631] lr:0.0004914 Loss: 0.8772 Loss_dice: 0.3018 Loss_ce: 0.5753\n",
      "Epoch: 0 [428/877] [428/2631] lr:0.0004913 Loss: 0.7897 Loss_dice: 0.2787 Loss_ce: 0.5109\n",
      "Epoch: 0 [429/877] [429/2631] lr:0.0004913 Loss: 0.7367 Loss_dice: 0.2621 Loss_ce: 0.4746\n",
      "Epoch: 0 [430/877] [430/2631] lr:0.0004912 Loss: 0.6971 Loss_dice: 0.2388 Loss_ce: 0.4583\n",
      "Epoch: 0 [431/877] [431/2631] lr:0.0004912 Loss: 0.8870 Loss_dice: 0.3144 Loss_ce: 0.5726\n",
      "Epoch: 0 [432/877] [432/2631] lr:0.0004912 Loss: 0.8052 Loss_dice: 0.2818 Loss_ce: 0.5233\n",
      "Epoch: 0 [433/877] [433/2631] lr:0.0004911 Loss: 0.8983 Loss_dice: 0.2932 Loss_ce: 0.6051\n",
      "Epoch: 0 [434/877] [434/2631] lr:0.0004911 Loss: 0.7874 Loss_dice: 0.2791 Loss_ce: 0.5083\n",
      "Epoch: 0 [435/877] [435/2631] lr:0.0004910 Loss: 0.7288 Loss_dice: 0.2465 Loss_ce: 0.4824\n",
      "Epoch: 0 [436/877] [436/2631] lr:0.0004910 Loss: 0.8963 Loss_dice: 0.2967 Loss_ce: 0.5996\n",
      "Epoch: 0 [437/877] [437/2631] lr:0.0004909 Loss: 0.8870 Loss_dice: 0.3004 Loss_ce: 0.5867\n",
      "Epoch: 0 [438/877] [438/2631] lr:0.0004909 Loss: 0.7241 Loss_dice: 0.2413 Loss_ce: 0.4828\n",
      "Epoch: 0 [439/877] [439/2631] lr:0.0004908 Loss: 0.8111 Loss_dice: 0.2795 Loss_ce: 0.5317\n",
      "Epoch: 0 [440/877] [440/2631] lr:0.0004908 Loss: 0.7395 Loss_dice: 0.2367 Loss_ce: 0.5028\n",
      "Epoch: 0 [441/877] [441/2631] lr:0.0004907 Loss: 0.9536 Loss_dice: 0.3163 Loss_ce: 0.6372\n",
      "Epoch: 0 [442/877] [442/2631] lr:0.0004907 Loss: 0.7238 Loss_dice: 0.2360 Loss_ce: 0.4878\n",
      "Epoch: 0 [443/877] [443/2631] lr:0.0004906 Loss: 0.7559 Loss_dice: 0.2689 Loss_ce: 0.4870\n",
      "Epoch: 0 [444/877] [444/2631] lr:0.0004906 Loss: 0.7901 Loss_dice: 0.2704 Loss_ce: 0.5197\n",
      "Epoch: 0 [445/877] [445/2631] lr:0.0004905 Loss: 0.7567 Loss_dice: 0.2470 Loss_ce: 0.5097\n",
      "Epoch: 0 [446/877] [446/2631] lr:0.0004905 Loss: 1.1432 Loss_dice: 0.3713 Loss_ce: 0.7719\n",
      "Epoch: 0 [447/877] [447/2631] lr:0.0004904 Loss: 0.8263 Loss_dice: 0.2814 Loss_ce: 0.5449\n",
      "Epoch: 0 [448/877] [448/2631] lr:0.0004904 Loss: 0.7151 Loss_dice: 0.2445 Loss_ce: 0.4706\n",
      "Epoch: 0 [449/877] [449/2631] lr:0.0004903 Loss: 0.9197 Loss_dice: 0.3099 Loss_ce: 0.6098\n",
      "Epoch: 0 [450/877] [450/2631] lr:0.0004903 Loss: 0.7770 Loss_dice: 0.2719 Loss_ce: 0.5050\n",
      "Epoch: 0 [451/877] [451/2631] lr:0.0004902 Loss: 0.7834 Loss_dice: 0.2792 Loss_ce: 0.5042\n",
      "Epoch: 0 [452/877] [452/2631] lr:0.0004902 Loss: 0.7550 Loss_dice: 0.2509 Loss_ce: 0.5041\n",
      "Epoch: 0 [453/877] [453/2631] lr:0.0004901 Loss: 0.7018 Loss_dice: 0.2193 Loss_ce: 0.4825\n",
      "Epoch: 0 [454/877] [454/2631] lr:0.0004901 Loss: 0.9567 Loss_dice: 0.3101 Loss_ce: 0.6466\n",
      "Epoch: 0 [455/877] [455/2631] lr:0.0004900 Loss: 1.0727 Loss_dice: 0.3475 Loss_ce: 0.7252\n",
      "Epoch: 0 [456/877] [456/2631] lr:0.0004900 Loss: 0.7477 Loss_dice: 0.2483 Loss_ce: 0.4994\n",
      "Epoch: 0 [457/877] [457/2631] lr:0.0004899 Loss: 0.8292 Loss_dice: 0.2738 Loss_ce: 0.5554\n",
      "Epoch: 0 [458/877] [458/2631] lr:0.0004899 Loss: 0.9510 Loss_dice: 0.3099 Loss_ce: 0.6411\n",
      "Epoch: 0 [459/877] [459/2631] lr:0.0004898 Loss: 0.9092 Loss_dice: 0.3044 Loss_ce: 0.6047\n",
      "Epoch: 0 [460/877] [460/2631] lr:0.0004898 Loss: 0.8608 Loss_dice: 0.2897 Loss_ce: 0.5711\n",
      "Epoch: 0 [461/877] [461/2631] lr:0.0004897 Loss: 0.8304 Loss_dice: 0.2787 Loss_ce: 0.5517\n",
      "Epoch: 0 [462/877] [462/2631] lr:0.0004897 Loss: 0.8007 Loss_dice: 0.2737 Loss_ce: 0.5270\n",
      "Epoch: 0 [463/877] [463/2631] lr:0.0004896 Loss: 0.8024 Loss_dice: 0.2791 Loss_ce: 0.5234\n",
      "Epoch: 0 [464/877] [464/2631] lr:0.0004896 Loss: 0.9823 Loss_dice: 0.3273 Loss_ce: 0.6550\n",
      "Epoch: 0 [465/877] [465/2631] lr:0.0004895 Loss: 0.7223 Loss_dice: 0.2473 Loss_ce: 0.4750\n",
      "Epoch: 0 [466/877] [466/2631] lr:0.0004895 Loss: 0.7892 Loss_dice: 0.2724 Loss_ce: 0.5168\n",
      "Epoch: 0 [467/877] [467/2631] lr:0.0004894 Loss: 1.1135 Loss_dice: 0.3583 Loss_ce: 0.7552\n",
      "Epoch: 0 [468/877] [468/2631] lr:0.0004894 Loss: 0.8050 Loss_dice: 0.2641 Loss_ce: 0.5410\n",
      "Epoch: 0 [469/877] [469/2631] lr:0.0004893 Loss: 0.6917 Loss_dice: 0.2224 Loss_ce: 0.4693\n",
      "Epoch: 0 [470/877] [470/2631] lr:0.0004893 Loss: 0.8449 Loss_dice: 0.2784 Loss_ce: 0.5665\n",
      "Epoch: 0 [471/877] [471/2631] lr:0.0004892 Loss: 0.7243 Loss_dice: 0.2421 Loss_ce: 0.4822\n",
      "Epoch: 0 [472/877] [472/2631] lr:0.0004892 Loss: 0.7759 Loss_dice: 0.2648 Loss_ce: 0.5111\n",
      "Epoch: 0 [473/877] [473/2631] lr:0.0004891 Loss: 0.7645 Loss_dice: 0.2646 Loss_ce: 0.5000\n",
      "Epoch: 0 [474/877] [474/2631] lr:0.0004891 Loss: 0.8335 Loss_dice: 0.2834 Loss_ce: 0.5501\n",
      "Epoch: 0 [475/877] [475/2631] lr:0.0004890 Loss: 0.7521 Loss_dice: 0.2559 Loss_ce: 0.4962\n",
      "Epoch: 0 [476/877] [476/2631] lr:0.0004890 Loss: 0.7563 Loss_dice: 0.2596 Loss_ce: 0.4967\n",
      "Epoch: 0 [477/877] [477/2631] lr:0.0004889 Loss: 0.7042 Loss_dice: 0.2444 Loss_ce: 0.4597\n",
      "Epoch: 0 [478/877] [478/2631] lr:0.0004888 Loss: 0.8231 Loss_dice: 0.2854 Loss_ce: 0.5377\n",
      "Epoch: 0 [479/877] [479/2631] lr:0.0004888 Loss: 0.9384 Loss_dice: 0.3194 Loss_ce: 0.6190\n",
      "Epoch: 0 [480/877] [480/2631] lr:0.0004887 Loss: 0.8006 Loss_dice: 0.2755 Loss_ce: 0.5250\n",
      "Epoch: 0 [481/877] [481/2631] lr:0.0004887 Loss: 0.8910 Loss_dice: 0.2995 Loss_ce: 0.5916\n",
      "Epoch: 0 [482/877] [482/2631] lr:0.0004886 Loss: 0.7299 Loss_dice: 0.2554 Loss_ce: 0.4745\n",
      "Epoch: 0 [483/877] [483/2631] lr:0.0004886 Loss: 0.7276 Loss_dice: 0.2487 Loss_ce: 0.4789\n",
      "Epoch: 0 [484/877] [484/2631] lr:0.0004885 Loss: 0.6644 Loss_dice: 0.2257 Loss_ce: 0.4388\n",
      "Epoch: 0 [485/877] [485/2631] lr:0.0004885 Loss: 0.7544 Loss_dice: 0.2673 Loss_ce: 0.4871\n",
      "Epoch: 0 [486/877] [486/2631] lr:0.0004884 Loss: 0.8824 Loss_dice: 0.2933 Loss_ce: 0.5891\n",
      "Epoch: 0 [487/877] [487/2631] lr:0.0004884 Loss: 0.7485 Loss_dice: 0.2596 Loss_ce: 0.4890\n",
      "Epoch: 0 [488/877] [488/2631] lr:0.0004883 Loss: 0.6973 Loss_dice: 0.2436 Loss_ce: 0.4537\n",
      "Epoch: 0 [489/877] [489/2631] lr:0.0004883 Loss: 0.7245 Loss_dice: 0.2536 Loss_ce: 0.4709\n",
      "Epoch: 0 [490/877] [490/2631] lr:0.0004882 Loss: 0.7939 Loss_dice: 0.2692 Loss_ce: 0.5247\n",
      "Epoch: 0 [491/877] [491/2631] lr:0.0004881 Loss: 0.8702 Loss_dice: 0.2937 Loss_ce: 0.5765\n",
      "Epoch: 0 [492/877] [492/2631] lr:0.0004881 Loss: 0.6800 Loss_dice: 0.2334 Loss_ce: 0.4466\n",
      "Epoch: 0 [493/877] [493/2631] lr:0.0004880 Loss: 0.7184 Loss_dice: 0.2518 Loss_ce: 0.4666\n",
      "Epoch: 0 [494/877] [494/2631] lr:0.0004880 Loss: 0.7494 Loss_dice: 0.2646 Loss_ce: 0.4848\n",
      "Epoch: 0 [495/877] [495/2631] lr:0.0004879 Loss: 1.1389 Loss_dice: 0.3640 Loss_ce: 0.7749\n",
      "Epoch: 0 [496/877] [496/2631] lr:0.0004879 Loss: 0.8211 Loss_dice: 0.2945 Loss_ce: 0.5267\n",
      "Epoch: 0 [497/877] [497/2631] lr:0.0004878 Loss: 0.7879 Loss_dice: 0.2756 Loss_ce: 0.5123\n",
      "Epoch: 0 [498/877] [498/2631] lr:0.0004878 Loss: 0.7309 Loss_dice: 0.2629 Loss_ce: 0.4680\n",
      "Epoch: 0 [499/877] [499/2631] lr:0.0004877 Loss: 0.7272 Loss_dice: 0.2566 Loss_ce: 0.4706\n",
      "Epoch: 0 [500/877] [500/2631] lr:0.0004877 Loss: 0.8188 Loss_dice: 0.2822 Loss_ce: 0.5366\n",
      "Epoch: 0 [501/877] [501/2631] lr:0.0004876 Loss: 0.8375 Loss_dice: 0.2857 Loss_ce: 0.5518\n",
      "Epoch: 0 [502/877] [502/2631] lr:0.0004875 Loss: 0.8628 Loss_dice: 0.2992 Loss_ce: 0.5636\n",
      "Epoch: 0 [503/877] [503/2631] lr:0.0004875 Loss: 0.7483 Loss_dice: 0.2643 Loss_ce: 0.4841\n",
      "Epoch: 0 [504/877] [504/2631] lr:0.0004874 Loss: 0.7748 Loss_dice: 0.2691 Loss_ce: 0.5058\n",
      "Epoch: 0 [505/877] [505/2631] lr:0.0004874 Loss: 0.8001 Loss_dice: 0.2681 Loss_ce: 0.5320\n",
      "Epoch: 0 [506/877] [506/2631] lr:0.0004873 Loss: 0.8008 Loss_dice: 0.2659 Loss_ce: 0.5348\n",
      "Epoch: 0 [507/877] [507/2631] lr:0.0004873 Loss: 0.7274 Loss_dice: 0.2432 Loss_ce: 0.4842\n",
      "Epoch: 0 [508/877] [508/2631] lr:0.0004872 Loss: 0.6181 Loss_dice: 0.2156 Loss_ce: 0.4025\n",
      "Epoch: 0 [509/877] [509/2631] lr:0.0004871 Loss: 0.8018 Loss_dice: 0.2615 Loss_ce: 0.5404\n",
      "Epoch: 0 [510/877] [510/2631] lr:0.0004871 Loss: 0.7528 Loss_dice: 0.2485 Loss_ce: 0.5043\n",
      "Epoch: 0 [511/877] [511/2631] lr:0.0004870 Loss: 0.8156 Loss_dice: 0.2612 Loss_ce: 0.5543\n",
      "Epoch: 0 [512/877] [512/2631] lr:0.0004870 Loss: 0.7692 Loss_dice: 0.2563 Loss_ce: 0.5129\n",
      "Epoch: 0 [513/877] [513/2631] lr:0.0004869 Loss: 0.6930 Loss_dice: 0.2448 Loss_ce: 0.4482\n",
      "Epoch: 0 [514/877] [514/2631] lr:0.0004869 Loss: 0.7426 Loss_dice: 0.2570 Loss_ce: 0.4856\n",
      "Epoch: 0 [515/877] [515/2631] lr:0.0004868 Loss: 0.8180 Loss_dice: 0.2804 Loss_ce: 0.5376\n",
      "Epoch: 0 [516/877] [516/2631] lr:0.0004867 Loss: 0.7052 Loss_dice: 0.2503 Loss_ce: 0.4549\n",
      "Epoch: 0 [517/877] [517/2631] lr:0.0004867 Loss: 0.8409 Loss_dice: 0.2915 Loss_ce: 0.5495\n",
      "Epoch: 0 [518/877] [518/2631] lr:0.0004866 Loss: 0.9282 Loss_dice: 0.3152 Loss_ce: 0.6129\n",
      "Epoch: 0 [519/877] [519/2631] lr:0.0004866 Loss: 0.7699 Loss_dice: 0.2785 Loss_ce: 0.4914\n",
      "Epoch: 0 [520/877] [520/2631] lr:0.0004865 Loss: 0.9809 Loss_dice: 0.3337 Loss_ce: 0.6472\n",
      "Epoch: 0 [521/877] [521/2631] lr:0.0004865 Loss: 0.8242 Loss_dice: 0.2853 Loss_ce: 0.5389\n",
      "Epoch: 0 [522/877] [522/2631] lr:0.0004864 Loss: 0.7408 Loss_dice: 0.2665 Loss_ce: 0.4743\n",
      "Epoch: 0 [523/877] [523/2631] lr:0.0004863 Loss: 0.6236 Loss_dice: 0.2348 Loss_ce: 0.3888\n",
      "Epoch: 0 [524/877] [524/2631] lr:0.0004863 Loss: 1.4724 Loss_dice: 0.4779 Loss_ce: 0.9945\n",
      "Epoch: 0 [525/877] [525/2631] lr:0.0004862 Loss: 0.6894 Loss_dice: 0.2452 Loss_ce: 0.4441\n",
      "Epoch: 0 [526/877] [526/2631] lr:0.0004862 Loss: 0.8741 Loss_dice: 0.2953 Loss_ce: 0.5789\n",
      "Epoch: 0 [527/877] [527/2631] lr:0.0004861 Loss: 0.8780 Loss_dice: 0.2747 Loss_ce: 0.6032\n",
      "Epoch: 0 [528/877] [528/2631] lr:0.0004860 Loss: 0.8845 Loss_dice: 0.2881 Loss_ce: 0.5964\n",
      "Epoch: 0 [529/877] [529/2631] lr:0.0004860 Loss: 0.6728 Loss_dice: 0.2318 Loss_ce: 0.4410\n",
      "Epoch: 0 [530/877] [530/2631] lr:0.0004859 Loss: 0.7082 Loss_dice: 0.2404 Loss_ce: 0.4678\n",
      "Epoch: 0 [531/877] [531/2631] lr:0.0004859 Loss: 0.8512 Loss_dice: 0.2770 Loss_ce: 0.5742\n",
      "Epoch: 0 [532/877] [532/2631] lr:0.0004858 Loss: 0.7021 Loss_dice: 0.2337 Loss_ce: 0.4684\n",
      "Epoch: 0 [533/877] [533/2631] lr:0.0004858 Loss: 0.8536 Loss_dice: 0.2716 Loss_ce: 0.5820\n",
      "Epoch: 0 [534/877] [534/2631] lr:0.0004857 Loss: 0.8214 Loss_dice: 0.2526 Loss_ce: 0.5689\n",
      "Epoch: 0 [535/877] [535/2631] lr:0.0004856 Loss: 0.8701 Loss_dice: 0.2726 Loss_ce: 0.5976\n",
      "Epoch: 0 [536/877] [536/2631] lr:0.0004856 Loss: 0.7490 Loss_dice: 0.2435 Loss_ce: 0.5055\n",
      "Epoch: 0 [537/877] [537/2631] lr:0.0004855 Loss: 0.8387 Loss_dice: 0.2741 Loss_ce: 0.5646\n",
      "Epoch: 0 [538/877] [538/2631] lr:0.0004855 Loss: 0.7784 Loss_dice: 0.2607 Loss_ce: 0.5177\n",
      "Epoch: 0 [539/877] [539/2631] lr:0.0004854 Loss: 0.8409 Loss_dice: 0.2822 Loss_ce: 0.5587\n",
      "Epoch: 0 [540/877] [540/2631] lr:0.0004853 Loss: 0.8038 Loss_dice: 0.2696 Loss_ce: 0.5342\n",
      "Epoch: 0 [541/877] [541/2631] lr:0.0004853 Loss: 1.1473 Loss_dice: 0.3751 Loss_ce: 0.7722\n",
      "Epoch: 0 [542/877] [542/2631] lr:0.0004852 Loss: 0.9277 Loss_dice: 0.3138 Loss_ce: 0.6139\n",
      "Epoch: 0 [543/877] [543/2631] lr:0.0004852 Loss: 0.6698 Loss_dice: 0.2376 Loss_ce: 0.4322\n",
      "Epoch: 0 [544/877] [544/2631] lr:0.0004851 Loss: 0.7622 Loss_dice: 0.2574 Loss_ce: 0.5048\n",
      "Epoch: 0 [545/877] [545/2631] lr:0.0004850 Loss: 0.8615 Loss_dice: 0.2858 Loss_ce: 0.5757\n",
      "Epoch: 0 [546/877] [546/2631] lr:0.0004850 Loss: 0.6847 Loss_dice: 0.2101 Loss_ce: 0.4746\n",
      "Epoch: 0 [547/877] [547/2631] lr:0.0004849 Loss: 0.6759 Loss_dice: 0.2176 Loss_ce: 0.4583\n",
      "Epoch: 0 [548/877] [548/2631] lr:0.0004848 Loss: 0.7718 Loss_dice: 0.2476 Loss_ce: 0.5242\n",
      "Epoch: 0 [549/877] [549/2631] lr:0.0004848 Loss: 0.8083 Loss_dice: 0.2679 Loss_ce: 0.5405\n",
      "Epoch: 0 [550/877] [550/2631] lr:0.0004847 Loss: 0.8626 Loss_dice: 0.2816 Loss_ce: 0.5810\n",
      "Epoch: 0 [551/877] [551/2631] lr:0.0004847 Loss: 0.7654 Loss_dice: 0.2521 Loss_ce: 0.5133\n",
      "Epoch: 0 [552/877] [552/2631] lr:0.0004846 Loss: 0.9444 Loss_dice: 0.3070 Loss_ce: 0.6374\n",
      "Epoch: 0 [553/877] [553/2631] lr:0.0004845 Loss: 0.8823 Loss_dice: 0.2930 Loss_ce: 0.5893\n",
      "Epoch: 0 [554/877] [554/2631] lr:0.0004845 Loss: 0.7657 Loss_dice: 0.2718 Loss_ce: 0.4939\n",
      "Epoch: 0 [555/877] [555/2631] lr:0.0004844 Loss: 0.9296 Loss_dice: 0.3192 Loss_ce: 0.6104\n",
      "Epoch: 0 [556/877] [556/2631] lr:0.0004843 Loss: 0.8835 Loss_dice: 0.3158 Loss_ce: 0.5677\n",
      "Epoch: 0 [557/877] [557/2631] lr:0.0004843 Loss: 0.9169 Loss_dice: 0.3263 Loss_ce: 0.5906\n",
      "Epoch: 0 [558/877] [558/2631] lr:0.0004842 Loss: 0.7809 Loss_dice: 0.2867 Loss_ce: 0.4943\n",
      "Epoch: 0 [559/877] [559/2631] lr:0.0004842 Loss: 0.8335 Loss_dice: 0.3081 Loss_ce: 0.5254\n",
      "Epoch: 0 [560/877] [560/2631] lr:0.0004841 Loss: 0.7387 Loss_dice: 0.2696 Loss_ce: 0.4691\n",
      "Epoch: 0 [561/877] [561/2631] lr:0.0004840 Loss: 0.7033 Loss_dice: 0.2640 Loss_ce: 0.4392\n",
      "Epoch: 0 [562/877] [562/2631] lr:0.0004840 Loss: 0.8005 Loss_dice: 0.3017 Loss_ce: 0.4988\n",
      "Epoch: 0 [563/877] [563/2631] lr:0.0004839 Loss: 0.8699 Loss_dice: 0.3152 Loss_ce: 0.5546\n",
      "Epoch: 0 [564/877] [564/2631] lr:0.0004838 Loss: 1.0254 Loss_dice: 0.3560 Loss_ce: 0.6694\n",
      "Epoch: 0 [565/877] [565/2631] lr:0.0004838 Loss: 0.6988 Loss_dice: 0.2582 Loss_ce: 0.4406\n",
      "Epoch: 0 [566/877] [566/2631] lr:0.0004837 Loss: 0.7533 Loss_dice: 0.2762 Loss_ce: 0.4772\n",
      "Epoch: 0 [567/877] [567/2631] lr:0.0004837 Loss: 0.7606 Loss_dice: 0.2445 Loss_ce: 0.5161\n",
      "Epoch: 0 [568/877] [568/2631] lr:0.0004836 Loss: 0.7726 Loss_dice: 0.2822 Loss_ce: 0.4904\n",
      "Epoch: 0 [569/877] [569/2631] lr:0.0004835 Loss: 0.6648 Loss_dice: 0.2296 Loss_ce: 0.4352\n",
      "Epoch: 0 [570/877] [570/2631] lr:0.0004835 Loss: 0.8343 Loss_dice: 0.2903 Loss_ce: 0.5440\n",
      "Epoch: 0 [571/877] [571/2631] lr:0.0004834 Loss: 0.8511 Loss_dice: 0.2934 Loss_ce: 0.5577\n",
      "Epoch: 0 [572/877] [572/2631] lr:0.0004833 Loss: 0.7082 Loss_dice: 0.2430 Loss_ce: 0.4652\n",
      "Epoch: 0 [573/877] [573/2631] lr:0.0004833 Loss: 0.8322 Loss_dice: 0.2976 Loss_ce: 0.5346\n",
      "Epoch: 0 [574/877] [574/2631] lr:0.0004832 Loss: 0.7465 Loss_dice: 0.2647 Loss_ce: 0.4818\n",
      "Epoch: 0 [575/877] [575/2631] lr:0.0004831 Loss: 0.7175 Loss_dice: 0.2538 Loss_ce: 0.4637\n",
      "Epoch: 0 [576/877] [576/2631] lr:0.0004831 Loss: 0.8376 Loss_dice: 0.2916 Loss_ce: 0.5460\n",
      "Epoch: 0 [577/877] [577/2631] lr:0.0004830 Loss: 0.8281 Loss_dice: 0.2876 Loss_ce: 0.5405\n",
      "Epoch: 0 [578/877] [578/2631] lr:0.0004829 Loss: 0.8629 Loss_dice: 0.2979 Loss_ce: 0.5650\n",
      "Epoch: 0 [579/877] [579/2631] lr:0.0004829 Loss: 0.6814 Loss_dice: 0.2165 Loss_ce: 0.4650\n",
      "Epoch: 0 [580/877] [580/2631] lr:0.0004828 Loss: 0.9903 Loss_dice: 0.3195 Loss_ce: 0.6707\n",
      "Epoch: 0 [581/877] [581/2631] lr:0.0004828 Loss: 0.8991 Loss_dice: 0.3043 Loss_ce: 0.5948\n",
      "Epoch: 0 [582/877] [582/2631] lr:0.0004827 Loss: 0.7641 Loss_dice: 0.2672 Loss_ce: 0.4969\n",
      "Epoch: 0 [583/877] [583/2631] lr:0.0004826 Loss: 0.7298 Loss_dice: 0.2552 Loss_ce: 0.4746\n",
      "Epoch: 0 [584/877] [584/2631] lr:0.0004826 Loss: 0.7521 Loss_dice: 0.2709 Loss_ce: 0.4811\n",
      "Epoch: 0 [585/877] [585/2631] lr:0.0004825 Loss: 0.7261 Loss_dice: 0.2573 Loss_ce: 0.4688\n",
      "Epoch: 0 [586/877] [586/2631] lr:0.0004824 Loss: 0.7409 Loss_dice: 0.2592 Loss_ce: 0.4817\n",
      "Epoch: 0 [587/877] [587/2631] lr:0.0004824 Loss: 0.6739 Loss_dice: 0.2383 Loss_ce: 0.4355\n",
      "Epoch: 0 [588/877] [588/2631] lr:0.0004823 Loss: 0.8793 Loss_dice: 0.3118 Loss_ce: 0.5674\n",
      "Epoch: 0 [589/877] [589/2631] lr:0.0004822 Loss: 0.8218 Loss_dice: 0.2997 Loss_ce: 0.5220\n",
      "Epoch: 0 [590/877] [590/2631] lr:0.0004822 Loss: 0.6889 Loss_dice: 0.2538 Loss_ce: 0.4351\n",
      "Epoch: 0 [591/877] [591/2631] lr:0.0004821 Loss: 0.9650 Loss_dice: 0.3345 Loss_ce: 0.6305\n",
      "Epoch: 0 [592/877] [592/2631] lr:0.0004820 Loss: 0.6991 Loss_dice: 0.2470 Loss_ce: 0.4520\n",
      "Epoch: 0 [593/877] [593/2631] lr:0.0004820 Loss: 0.9088 Loss_dice: 0.3178 Loss_ce: 0.5910\n",
      "Epoch: 0 [594/877] [594/2631] lr:0.0004819 Loss: 1.1211 Loss_dice: 0.3733 Loss_ce: 0.7479\n",
      "Epoch: 0 [595/877] [595/2631] lr:0.0004818 Loss: 0.7995 Loss_dice: 0.2864 Loss_ce: 0.5131\n",
      "Epoch: 0 [596/877] [596/2631] lr:0.0004818 Loss: 0.7926 Loss_dice: 0.2846 Loss_ce: 0.5080\n",
      "Epoch: 0 [597/877] [597/2631] lr:0.0004817 Loss: 0.9088 Loss_dice: 0.3076 Loss_ce: 0.6012\n",
      "Epoch: 0 [598/877] [598/2631] lr:0.0004816 Loss: 0.8414 Loss_dice: 0.2933 Loss_ce: 0.5481\n",
      "Epoch: 0 [599/877] [599/2631] lr:0.0004816 Loss: 0.7609 Loss_dice: 0.2745 Loss_ce: 0.4863\n",
      "Epoch: 0 [600/877] [600/2631] lr:0.0004815 Loss: 0.6872 Loss_dice: 0.2448 Loss_ce: 0.4423\n",
      "Epoch: 0 [601/877] [601/2631] lr:0.0004814 Loss: 0.8016 Loss_dice: 0.2772 Loss_ce: 0.5244\n",
      "Epoch: 0 [602/877] [602/2631] lr:0.0004814 Loss: 0.9405 Loss_dice: 0.3059 Loss_ce: 0.6346\n",
      "Epoch: 0 [603/877] [603/2631] lr:0.0004813 Loss: 0.8709 Loss_dice: 0.2879 Loss_ce: 0.5830\n",
      "Epoch: 0 [604/877] [604/2631] lr:0.0004812 Loss: 0.7785 Loss_dice: 0.2670 Loss_ce: 0.5115\n",
      "Epoch: 0 [605/877] [605/2631] lr:0.0004811 Loss: 0.8307 Loss_dice: 0.2846 Loss_ce: 0.5461\n",
      "Epoch: 0 [606/877] [606/2631] lr:0.0004811 Loss: 0.7392 Loss_dice: 0.2559 Loss_ce: 0.4833\n",
      "Epoch: 0 [607/877] [607/2631] lr:0.0004810 Loss: 0.8767 Loss_dice: 0.2955 Loss_ce: 0.5813\n",
      "Epoch: 0 [608/877] [608/2631] lr:0.0004809 Loss: 0.6929 Loss_dice: 0.2401 Loss_ce: 0.4528\n",
      "Epoch: 0 [609/877] [609/2631] lr:0.0004809 Loss: 0.7800 Loss_dice: 0.2620 Loss_ce: 0.5180\n",
      "Epoch: 0 [610/877] [610/2631] lr:0.0004808 Loss: 0.7222 Loss_dice: 0.2580 Loss_ce: 0.4642\n",
      "Epoch: 0 [611/877] [611/2631] lr:0.0004807 Loss: 0.7126 Loss_dice: 0.2551 Loss_ce: 0.4575\n",
      "Epoch: 0 [612/877] [612/2631] lr:0.0004807 Loss: 1.0028 Loss_dice: 0.3258 Loss_ce: 0.6770\n",
      "Epoch: 0 [613/877] [613/2631] lr:0.0004806 Loss: 0.8023 Loss_dice: 0.2778 Loss_ce: 0.5245\n",
      "Epoch: 0 [614/877] [614/2631] lr:0.0004805 Loss: 0.7671 Loss_dice: 0.2550 Loss_ce: 0.5121\n",
      "Epoch: 0 [615/877] [615/2631] lr:0.0004805 Loss: 0.7382 Loss_dice: 0.2648 Loss_ce: 0.4734\n",
      "Epoch: 0 [616/877] [616/2631] lr:0.0004804 Loss: 0.7289 Loss_dice: 0.2632 Loss_ce: 0.4657\n",
      "Epoch: 0 [617/877] [617/2631] lr:0.0004803 Loss: 0.7637 Loss_dice: 0.2698 Loss_ce: 0.4939\n",
      "Epoch: 0 [618/877] [618/2631] lr:0.0004803 Loss: 0.7770 Loss_dice: 0.2728 Loss_ce: 0.5042\n",
      "Epoch: 0 [619/877] [619/2631] lr:0.0004802 Loss: 0.6648 Loss_dice: 0.2368 Loss_ce: 0.4281\n",
      "Epoch: 0 [620/877] [620/2631] lr:0.0004801 Loss: 0.7871 Loss_dice: 0.2674 Loss_ce: 0.5197\n",
      "Epoch: 0 [621/877] [621/2631] lr:0.0004800 Loss: 0.6777 Loss_dice: 0.2329 Loss_ce: 0.4448\n",
      "Epoch: 0 [622/877] [622/2631] lr:0.0004800 Loss: 1.1772 Loss_dice: 0.3882 Loss_ce: 0.7891\n",
      "Epoch: 0 [623/877] [623/2631] lr:0.0004799 Loss: 1.0715 Loss_dice: 0.3503 Loss_ce: 0.7212\n",
      "Epoch: 0 [624/877] [624/2631] lr:0.0004798 Loss: 0.7529 Loss_dice: 0.2609 Loss_ce: 0.4920\n",
      "Epoch: 0 [625/877] [625/2631] lr:0.0004798 Loss: 0.7065 Loss_dice: 0.2439 Loss_ce: 0.4625\n",
      "Epoch: 0 [626/877] [626/2631] lr:0.0004797 Loss: 0.6624 Loss_dice: 0.2301 Loss_ce: 0.4323\n",
      "Epoch: 0 [627/877] [627/2631] lr:0.0004796 Loss: 0.8005 Loss_dice: 0.2848 Loss_ce: 0.5157\n",
      "Epoch: 0 [628/877] [628/2631] lr:0.0004795 Loss: 0.8439 Loss_dice: 0.2943 Loss_ce: 0.5496\n",
      "Epoch: 0 [629/877] [629/2631] lr:0.0004795 Loss: 0.7096 Loss_dice: 0.2360 Loss_ce: 0.4736\n",
      "Epoch: 0 [630/877] [630/2631] lr:0.0004794 Loss: 0.8598 Loss_dice: 0.2962 Loss_ce: 0.5636\n",
      "Epoch: 0 [631/877] [631/2631] lr:0.0004793 Loss: 0.8407 Loss_dice: 0.2880 Loss_ce: 0.5527\n",
      "Epoch: 0 [632/877] [632/2631] lr:0.0004793 Loss: 0.7354 Loss_dice: 0.2605 Loss_ce: 0.4749\n",
      "Epoch: 0 [633/877] [633/2631] lr:0.0004792 Loss: 0.7194 Loss_dice: 0.2423 Loss_ce: 0.4771\n",
      "Epoch: 0 [634/877] [634/2631] lr:0.0004791 Loss: 0.7868 Loss_dice: 0.2599 Loss_ce: 0.5269\n",
      "Epoch: 0 [635/877] [635/2631] lr:0.0004790 Loss: 0.9638 Loss_dice: 0.2996 Loss_ce: 0.6642\n",
      "Epoch: 0 [636/877] [636/2631] lr:0.0004790 Loss: 0.7290 Loss_dice: 0.2438 Loss_ce: 0.4852\n",
      "Epoch: 0 [637/877] [637/2631] lr:0.0004789 Loss: 0.7542 Loss_dice: 0.2542 Loss_ce: 0.5000\n",
      "Epoch: 0 [638/877] [638/2631] lr:0.0004788 Loss: 0.7137 Loss_dice: 0.2352 Loss_ce: 0.4785\n",
      "Epoch: 0 [639/877] [639/2631] lr:0.0004788 Loss: 0.7565 Loss_dice: 0.2473 Loss_ce: 0.5092\n",
      "Epoch: 0 [640/877] [640/2631] lr:0.0004787 Loss: 0.7989 Loss_dice: 0.2648 Loss_ce: 0.5342\n",
      "Epoch: 0 [641/877] [641/2631] lr:0.0004786 Loss: 0.6687 Loss_dice: 0.2267 Loss_ce: 0.4420\n",
      "Epoch: 0 [642/877] [642/2631] lr:0.0004785 Loss: 0.7341 Loss_dice: 0.2504 Loss_ce: 0.4837\n",
      "Epoch: 0 [643/877] [643/2631] lr:0.0004785 Loss: 0.7460 Loss_dice: 0.2597 Loss_ce: 0.4863\n",
      "Epoch: 0 [644/877] [644/2631] lr:0.0004784 Loss: 0.8209 Loss_dice: 0.2795 Loss_ce: 0.5414\n",
      "Epoch: 0 [645/877] [645/2631] lr:0.0004783 Loss: 0.7036 Loss_dice: 0.2453 Loss_ce: 0.4582\n",
      "Epoch: 0 [646/877] [646/2631] lr:0.0004783 Loss: 0.6199 Loss_dice: 0.2200 Loss_ce: 0.3999\n",
      "Epoch: 0 [647/877] [647/2631] lr:0.0004782 Loss: 1.0461 Loss_dice: 0.3538 Loss_ce: 0.6923\n",
      "Epoch: 0 [648/877] [648/2631] lr:0.0004781 Loss: 0.7312 Loss_dice: 0.2566 Loss_ce: 0.4746\n",
      "Epoch: 0 [649/877] [649/2631] lr:0.0004780 Loss: 0.6843 Loss_dice: 0.2554 Loss_ce: 0.4289\n",
      "Epoch: 0 [650/877] [650/2631] lr:0.0004780 Loss: 0.8970 Loss_dice: 0.3125 Loss_ce: 0.5845\n",
      "Epoch: 0 [651/877] [651/2631] lr:0.0004779 Loss: 0.7999 Loss_dice: 0.2914 Loss_ce: 0.5085\n",
      "Epoch: 0 [652/877] [652/2631] lr:0.0004778 Loss: 1.2918 Loss_dice: 0.4196 Loss_ce: 0.8722\n",
      "Epoch: 0 [653/877] [653/2631] lr:0.0004777 Loss: 0.8222 Loss_dice: 0.2950 Loss_ce: 0.5272\n",
      "Epoch: 0 [654/877] [654/2631] lr:0.0004777 Loss: 0.6816 Loss_dice: 0.2347 Loss_ce: 0.4469\n",
      "Epoch: 0 [655/877] [655/2631] lr:0.0004776 Loss: 0.7316 Loss_dice: 0.2684 Loss_ce: 0.4632\n",
      "Epoch: 0 [656/877] [656/2631] lr:0.0004775 Loss: 0.7545 Loss_dice: 0.2719 Loss_ce: 0.4826\n",
      "Epoch: 0 [657/877] [657/2631] lr:0.0004774 Loss: 0.7156 Loss_dice: 0.2617 Loss_ce: 0.4539\n",
      "Epoch: 0 [658/877] [658/2631] lr:0.0004774 Loss: 0.8776 Loss_dice: 0.2985 Loss_ce: 0.5790\n",
      "Epoch: 0 [659/877] [659/2631] lr:0.0004773 Loss: 0.8459 Loss_dice: 0.2902 Loss_ce: 0.5557\n",
      "Epoch: 0 [660/877] [660/2631] lr:0.0004772 Loss: 0.7399 Loss_dice: 0.2520 Loss_ce: 0.4879\n",
      "Epoch: 0 [661/877] [661/2631] lr:0.0004771 Loss: 0.6128 Loss_dice: 0.2158 Loss_ce: 0.3970\n",
      "Epoch: 0 [662/877] [662/2631] lr:0.0004771 Loss: 0.6640 Loss_dice: 0.2268 Loss_ce: 0.4372\n",
      "Epoch: 0 [663/877] [663/2631] lr:0.0004770 Loss: 0.7726 Loss_dice: 0.2591 Loss_ce: 0.5135\n",
      "Epoch: 0 [664/877] [664/2631] lr:0.0004769 Loss: 0.9066 Loss_dice: 0.3004 Loss_ce: 0.6062\n",
      "Epoch: 0 [665/877] [665/2631] lr:0.0004768 Loss: 0.8188 Loss_dice: 0.2723 Loss_ce: 0.5466\n",
      "Epoch: 0 [666/877] [666/2631] lr:0.0004768 Loss: 0.6419 Loss_dice: 0.2042 Loss_ce: 0.4377\n",
      "Epoch: 0 [667/877] [667/2631] lr:0.0004767 Loss: 0.7855 Loss_dice: 0.2643 Loss_ce: 0.5211\n",
      "Epoch: 0 [668/877] [668/2631] lr:0.0004766 Loss: 0.8852 Loss_dice: 0.2860 Loss_ce: 0.5992\n",
      "Epoch: 0 [669/877] [669/2631] lr:0.0004765 Loss: 0.8662 Loss_dice: 0.2881 Loss_ce: 0.5781\n",
      "Epoch: 0 [670/877] [670/2631] lr:0.0004765 Loss: 0.7374 Loss_dice: 0.2558 Loss_ce: 0.4816\n",
      "Epoch: 0 [671/877] [671/2631] lr:0.0004764 Loss: 1.2044 Loss_dice: 0.3848 Loss_ce: 0.8195\n",
      "Epoch: 0 [672/877] [672/2631] lr:0.0004763 Loss: 0.7171 Loss_dice: 0.2487 Loss_ce: 0.4684\n",
      "Epoch: 0 [673/877] [673/2631] lr:0.0004762 Loss: 0.6474 Loss_dice: 0.2316 Loss_ce: 0.4158\n",
      "Epoch: 0 [674/877] [674/2631] lr:0.0004762 Loss: 0.8438 Loss_dice: 0.2990 Loss_ce: 0.5448\n",
      "Epoch: 0 [675/877] [675/2631] lr:0.0004761 Loss: 0.6808 Loss_dice: 0.2264 Loss_ce: 0.4544\n",
      "Epoch: 0 [676/877] [676/2631] lr:0.0004760 Loss: 0.7217 Loss_dice: 0.2652 Loss_ce: 0.4565\n",
      "Epoch: 0 [677/877] [677/2631] lr:0.0004759 Loss: 0.7949 Loss_dice: 0.2894 Loss_ce: 0.5055\n",
      "Epoch: 0 [678/877] [678/2631] lr:0.0004759 Loss: 0.6241 Loss_dice: 0.2189 Loss_ce: 0.4052\n",
      "Epoch: 0 [679/877] [679/2631] lr:0.0004758 Loss: 0.6923 Loss_dice: 0.2506 Loss_ce: 0.4418\n",
      "Epoch: 0 [680/877] [680/2631] lr:0.0004757 Loss: 0.7673 Loss_dice: 0.2770 Loss_ce: 0.4902\n",
      "Epoch: 0 [681/877] [681/2631] lr:0.0004756 Loss: 1.1387 Loss_dice: 0.3739 Loss_ce: 0.7648\n",
      "Epoch: 0 [682/877] [682/2631] lr:0.0004755 Loss: 0.8135 Loss_dice: 0.2860 Loss_ce: 0.5275\n",
      "Epoch: 0 [683/877] [683/2631] lr:0.0004755 Loss: 0.7459 Loss_dice: 0.2709 Loss_ce: 0.4750\n",
      "Epoch: 0 [684/877] [684/2631] lr:0.0004754 Loss: 0.7377 Loss_dice: 0.2649 Loss_ce: 0.4728\n",
      "Epoch: 0 [685/877] [685/2631] lr:0.0004753 Loss: 0.7129 Loss_dice: 0.2604 Loss_ce: 0.4525\n",
      "Epoch: 0 [686/877] [686/2631] lr:0.0004752 Loss: 0.9860 Loss_dice: 0.3221 Loss_ce: 0.6639\n",
      "Epoch: 0 [687/877] [687/2631] lr:0.0004752 Loss: 0.8565 Loss_dice: 0.3018 Loss_ce: 0.5547\n",
      "Epoch: 0 [688/877] [688/2631] lr:0.0004751 Loss: 0.7339 Loss_dice: 0.2657 Loss_ce: 0.4682\n",
      "Epoch: 0 [689/877] [689/2631] lr:0.0004750 Loss: 0.7609 Loss_dice: 0.2724 Loss_ce: 0.4885\n",
      "Epoch: 0 [690/877] [690/2631] lr:0.0004749 Loss: 0.7527 Loss_dice: 0.2720 Loss_ce: 0.4807\n",
      "Epoch: 0 [691/877] [691/2631] lr:0.0004748 Loss: 0.6673 Loss_dice: 0.2424 Loss_ce: 0.4250\n",
      "Epoch: 0 [692/877] [692/2631] lr:0.0004748 Loss: 0.8381 Loss_dice: 0.2953 Loss_ce: 0.5428\n",
      "Epoch: 0 [693/877] [693/2631] lr:0.0004747 Loss: 0.8931 Loss_dice: 0.3132 Loss_ce: 0.5799\n",
      "Epoch: 0 [694/877] [694/2631] lr:0.0004746 Loss: 0.8245 Loss_dice: 0.2932 Loss_ce: 0.5313\n",
      "Epoch: 0 [695/877] [695/2631] lr:0.0004745 Loss: 0.8400 Loss_dice: 0.2993 Loss_ce: 0.5406\n",
      "Epoch: 0 [696/877] [696/2631] lr:0.0004745 Loss: 0.7928 Loss_dice: 0.2918 Loss_ce: 0.5011\n",
      "Epoch: 0 [697/877] [697/2631] lr:0.0004744 Loss: 0.7370 Loss_dice: 0.2764 Loss_ce: 0.4605\n",
      "Epoch: 0 [698/877] [698/2631] lr:0.0004743 Loss: 0.7973 Loss_dice: 0.2909 Loss_ce: 0.5064\n",
      "Epoch: 0 [699/877] [699/2631] lr:0.0004742 Loss: 0.6968 Loss_dice: 0.2623 Loss_ce: 0.4345\n",
      "Epoch: 0 [700/877] [700/2631] lr:0.0004741 Loss: 0.7900 Loss_dice: 0.2784 Loss_ce: 0.5116\n",
      "Epoch: 0 [701/877] [701/2631] lr:0.0004741 Loss: 0.8192 Loss_dice: 0.2935 Loss_ce: 0.5257\n",
      "Epoch: 0 [702/877] [702/2631] lr:0.0004740 Loss: 0.7175 Loss_dice: 0.2575 Loss_ce: 0.4601\n",
      "Epoch: 0 [703/877] [703/2631] lr:0.0004739 Loss: 0.7458 Loss_dice: 0.2727 Loss_ce: 0.4731\n",
      "Epoch: 0 [704/877] [704/2631] lr:0.0004738 Loss: 0.9565 Loss_dice: 0.3262 Loss_ce: 0.6302\n",
      "Epoch: 0 [705/877] [705/2631] lr:0.0004737 Loss: 0.8236 Loss_dice: 0.2899 Loss_ce: 0.5337\n",
      "Epoch: 0 [706/877] [706/2631] lr:0.0004737 Loss: 0.7318 Loss_dice: 0.2631 Loss_ce: 0.4687\n",
      "Epoch: 0 [707/877] [707/2631] lr:0.0004736 Loss: 0.8013 Loss_dice: 0.2892 Loss_ce: 0.5121\n",
      "Epoch: 0 [708/877] [708/2631] lr:0.0004735 Loss: 0.7558 Loss_dice: 0.2735 Loss_ce: 0.4823\n",
      "Epoch: 0 [709/877] [709/2631] lr:0.0004734 Loss: 0.9473 Loss_dice: 0.3315 Loss_ce: 0.6158\n",
      "Epoch: 0 [710/877] [710/2631] lr:0.0004733 Loss: 0.9194 Loss_dice: 0.3178 Loss_ce: 0.6016\n",
      "Epoch: 0 [711/877] [711/2631] lr:0.0004733 Loss: 0.8096 Loss_dice: 0.2975 Loss_ce: 0.5121\n",
      "Epoch: 0 [712/877] [712/2631] lr:0.0004732 Loss: 0.7119 Loss_dice: 0.2642 Loss_ce: 0.4476\n",
      "Epoch: 0 [713/877] [713/2631] lr:0.0004731 Loss: 0.8546 Loss_dice: 0.3111 Loss_ce: 0.5435\n",
      "Epoch: 0 [714/877] [714/2631] lr:0.0004730 Loss: 0.9052 Loss_dice: 0.3198 Loss_ce: 0.5854\n",
      "Epoch: 0 [715/877] [715/2631] lr:0.0004729 Loss: 0.8688 Loss_dice: 0.3070 Loss_ce: 0.5617\n",
      "Epoch: 0 [716/877] [716/2631] lr:0.0004729 Loss: 0.6507 Loss_dice: 0.2118 Loss_ce: 0.4388\n",
      "Epoch: 0 [717/877] [717/2631] lr:0.0004728 Loss: 0.7264 Loss_dice: 0.2589 Loss_ce: 0.4675\n",
      "Epoch: 0 [718/877] [718/2631] lr:0.0004727 Loss: 0.7565 Loss_dice: 0.2711 Loss_ce: 0.4854\n",
      "Epoch: 0 [719/877] [719/2631] lr:0.0004726 Loss: 0.8069 Loss_dice: 0.2818 Loss_ce: 0.5251\n",
      "Epoch: 0 [720/877] [720/2631] lr:0.0004725 Loss: 0.7018 Loss_dice: 0.2361 Loss_ce: 0.4657\n",
      "Epoch: 0 [721/877] [721/2631] lr:0.0004724 Loss: 0.7950 Loss_dice: 0.2748 Loss_ce: 0.5202\n",
      "Epoch: 0 [722/877] [722/2631] lr:0.0004724 Loss: 0.8125 Loss_dice: 0.2822 Loss_ce: 0.5303\n",
      "Epoch: 0 [723/877] [723/2631] lr:0.0004723 Loss: 0.8204 Loss_dice: 0.2832 Loss_ce: 0.5371\n",
      "Epoch: 0 [724/877] [724/2631] lr:0.0004722 Loss: 0.7117 Loss_dice: 0.2369 Loss_ce: 0.4748\n",
      "Epoch: 0 [725/877] [725/2631] lr:0.0004721 Loss: 0.9301 Loss_dice: 0.3113 Loss_ce: 0.6188\n",
      "Epoch: 0 [726/877] [726/2631] lr:0.0004720 Loss: 0.7838 Loss_dice: 0.2779 Loss_ce: 0.5059\n",
      "Epoch: 0 [727/877] [727/2631] lr:0.0004720 Loss: 1.0472 Loss_dice: 0.3367 Loss_ce: 0.7105\n",
      "Epoch: 0 [728/877] [728/2631] lr:0.0004719 Loss: 0.6477 Loss_dice: 0.2283 Loss_ce: 0.4195\n",
      "Epoch: 0 [729/877] [729/2631] lr:0.0004718 Loss: 0.7161 Loss_dice: 0.2512 Loss_ce: 0.4649\n",
      "Epoch: 0 [730/877] [730/2631] lr:0.0004717 Loss: 0.9472 Loss_dice: 0.3098 Loss_ce: 0.6373\n",
      "Epoch: 0 [731/877] [731/2631] lr:0.0004716 Loss: 0.8291 Loss_dice: 0.2961 Loss_ce: 0.5330\n",
      "Epoch: 0 [732/877] [732/2631] lr:0.0004715 Loss: 1.0062 Loss_dice: 0.3341 Loss_ce: 0.6721\n",
      "Epoch: 0 [733/877] [733/2631] lr:0.0004715 Loss: 0.8675 Loss_dice: 0.3105 Loss_ce: 0.5569\n",
      "Epoch: 0 [734/877] [734/2631] lr:0.0004714 Loss: 0.7090 Loss_dice: 0.2215 Loss_ce: 0.4875\n",
      "Epoch: 0 [735/877] [735/2631] lr:0.0004713 Loss: 0.9879 Loss_dice: 0.3153 Loss_ce: 0.6726\n",
      "Epoch: 0 [736/877] [736/2631] lr:0.0004712 Loss: 0.7000 Loss_dice: 0.2397 Loss_ce: 0.4603\n",
      "Epoch: 0 [737/877] [737/2631] lr:0.0004711 Loss: 0.7270 Loss_dice: 0.2569 Loss_ce: 0.4701\n",
      "Epoch: 0 [738/877] [738/2631] lr:0.0004710 Loss: 0.6865 Loss_dice: 0.2297 Loss_ce: 0.4568\n",
      "Epoch: 0 [739/877] [739/2631] lr:0.0004710 Loss: 1.0649 Loss_dice: 0.3411 Loss_ce: 0.7239\n",
      "Epoch: 0 [740/877] [740/2631] lr:0.0004709 Loss: 0.8192 Loss_dice: 0.2672 Loss_ce: 0.5520\n",
      "Epoch: 0 [741/877] [741/2631] lr:0.0004708 Loss: 0.8098 Loss_dice: 0.2671 Loss_ce: 0.5427\n",
      "Epoch: 0 [742/877] [742/2631] lr:0.0004707 Loss: 0.7154 Loss_dice: 0.2372 Loss_ce: 0.4782\n",
      "Epoch: 0 [743/877] [743/2631] lr:0.0004706 Loss: 0.8680 Loss_dice: 0.2822 Loss_ce: 0.5858\n",
      "Epoch: 0 [744/877] [744/2631] lr:0.0004705 Loss: 0.8719 Loss_dice: 0.2873 Loss_ce: 0.5846\n",
      "Epoch: 0 [745/877] [745/2631] lr:0.0004705 Loss: 0.8064 Loss_dice: 0.2662 Loss_ce: 0.5402\n",
      "Epoch: 0 [746/877] [746/2631] lr:0.0004704 Loss: 0.7367 Loss_dice: 0.2429 Loss_ce: 0.4938\n",
      "Epoch: 0 [747/877] [747/2631] lr:0.0004703 Loss: 0.7935 Loss_dice: 0.2704 Loss_ce: 0.5231\n",
      "Epoch: 0 [748/877] [748/2631] lr:0.0004702 Loss: 0.7656 Loss_dice: 0.2619 Loss_ce: 0.5038\n",
      "Epoch: 0 [749/877] [749/2631] lr:0.0004701 Loss: 0.6894 Loss_dice: 0.2471 Loss_ce: 0.4423\n",
      "Epoch: 0 [750/877] [750/2631] lr:0.0004700 Loss: 0.7290 Loss_dice: 0.2560 Loss_ce: 0.4729\n",
      "Epoch: 0 [751/877] [751/2631] lr:0.0004699 Loss: 0.7278 Loss_dice: 0.2583 Loss_ce: 0.4695\n",
      "Epoch: 0 [752/877] [752/2631] lr:0.0004699 Loss: 0.7940 Loss_dice: 0.2801 Loss_ce: 0.5139\n",
      "Epoch: 0 [753/877] [753/2631] lr:0.0004698 Loss: 0.7419 Loss_dice: 0.2652 Loss_ce: 0.4766\n",
      "Epoch: 0 [754/877] [754/2631] lr:0.0004697 Loss: 0.7806 Loss_dice: 0.2846 Loss_ce: 0.4960\n",
      "Epoch: 0 [755/877] [755/2631] lr:0.0004696 Loss: 0.7182 Loss_dice: 0.2612 Loss_ce: 0.4570\n",
      "Epoch: 0 [756/877] [756/2631] lr:0.0004695 Loss: 0.7376 Loss_dice: 0.2749 Loss_ce: 0.4627\n",
      "Epoch: 0 [757/877] [757/2631] lr:0.0004694 Loss: 0.8466 Loss_dice: 0.3039 Loss_ce: 0.5427\n",
      "Epoch: 0 [758/877] [758/2631] lr:0.0004693 Loss: 0.8845 Loss_dice: 0.3167 Loss_ce: 0.5678\n",
      "Epoch: 0 [759/877] [759/2631] lr:0.0004693 Loss: 0.6719 Loss_dice: 0.2382 Loss_ce: 0.4337\n",
      "Epoch: 0 [760/877] [760/2631] lr:0.0004692 Loss: 0.7083 Loss_dice: 0.2679 Loss_ce: 0.4404\n",
      "Epoch: 0 [761/877] [761/2631] lr:0.0004691 Loss: 0.8401 Loss_dice: 0.2997 Loss_ce: 0.5404\n",
      "Epoch: 0 [762/877] [762/2631] lr:0.0004690 Loss: 0.9441 Loss_dice: 0.3365 Loss_ce: 0.6076\n",
      "Epoch: 0 [763/877] [763/2631] lr:0.0004689 Loss: 0.6941 Loss_dice: 0.2530 Loss_ce: 0.4411\n",
      "Epoch: 0 [764/877] [764/2631] lr:0.0004688 Loss: 0.8236 Loss_dice: 0.3033 Loss_ce: 0.5203\n",
      "Epoch: 0 [765/877] [765/2631] lr:0.0004687 Loss: 0.8225 Loss_dice: 0.2913 Loss_ce: 0.5312\n",
      "Epoch: 0 [766/877] [766/2631] lr:0.0004687 Loss: 0.8289 Loss_dice: 0.2957 Loss_ce: 0.5332\n",
      "Epoch: 0 [767/877] [767/2631] lr:0.0004686 Loss: 0.7208 Loss_dice: 0.2629 Loss_ce: 0.4579\n",
      "Epoch: 0 [768/877] [768/2631] lr:0.0004685 Loss: 0.7093 Loss_dice: 0.2613 Loss_ce: 0.4480\n",
      "Epoch: 0 [769/877] [769/2631] lr:0.0004684 Loss: 0.8183 Loss_dice: 0.2903 Loss_ce: 0.5280\n",
      "Epoch: 0 [770/877] [770/2631] lr:0.0004683 Loss: 0.8120 Loss_dice: 0.2835 Loss_ce: 0.5285\n",
      "Epoch: 0 [771/877] [771/2631] lr:0.0004682 Loss: 0.9658 Loss_dice: 0.3219 Loss_ce: 0.6440\n",
      "Epoch: 0 [772/877] [772/2631] lr:0.0004681 Loss: 0.6891 Loss_dice: 0.2352 Loss_ce: 0.4539\n",
      "Epoch: 0 [773/877] [773/2631] lr:0.0004680 Loss: 0.8656 Loss_dice: 0.3062 Loss_ce: 0.5594\n",
      "Epoch: 0 [774/877] [774/2631] lr:0.0004680 Loss: 0.9671 Loss_dice: 0.3147 Loss_ce: 0.6524\n",
      "Epoch: 0 [775/877] [775/2631] lr:0.0004679 Loss: 0.9434 Loss_dice: 0.3211 Loss_ce: 0.6223\n",
      "Epoch: 0 [776/877] [776/2631] lr:0.0004678 Loss: 0.7553 Loss_dice: 0.2866 Loss_ce: 0.4687\n",
      "Epoch: 0 [777/877] [777/2631] lr:0.0004677 Loss: 0.7765 Loss_dice: 0.2914 Loss_ce: 0.4851\n",
      "Epoch: 0 [778/877] [778/2631] lr:0.0004676 Loss: 0.7787 Loss_dice: 0.2911 Loss_ce: 0.4876\n",
      "Epoch: 0 [779/877] [779/2631] lr:0.0004675 Loss: 0.7323 Loss_dice: 0.2512 Loss_ce: 0.4811\n",
      "Epoch: 0 [780/877] [780/2631] lr:0.0004674 Loss: 0.7450 Loss_dice: 0.2641 Loss_ce: 0.4810\n",
      "Epoch: 0 [781/877] [781/2631] lr:0.0004673 Loss: 0.6807 Loss_dice: 0.2285 Loss_ce: 0.4522\n",
      "Epoch: 0 [782/877] [782/2631] lr:0.0004672 Loss: 0.8506 Loss_dice: 0.2672 Loss_ce: 0.5834\n",
      "Epoch: 0 [783/877] [783/2631] lr:0.0004672 Loss: 0.8407 Loss_dice: 0.2666 Loss_ce: 0.5741\n",
      "Epoch: 0 [784/877] [784/2631] lr:0.0004671 Loss: 0.8846 Loss_dice: 0.2666 Loss_ce: 0.6180\n",
      "Epoch: 0 [785/877] [785/2631] lr:0.0004670 Loss: 0.8590 Loss_dice: 0.2650 Loss_ce: 0.5940\n",
      "Epoch: 0 [786/877] [786/2631] lr:0.0004669 Loss: 0.7026 Loss_dice: 0.2250 Loss_ce: 0.4776\n",
      "Epoch: 0 [787/877] [787/2631] lr:0.0004668 Loss: 0.8925 Loss_dice: 0.2912 Loss_ce: 0.6013\n",
      "Epoch: 0 [788/877] [788/2631] lr:0.0004667 Loss: 0.7924 Loss_dice: 0.2696 Loss_ce: 0.5228\n",
      "Epoch: 0 [789/877] [789/2631] lr:0.0004666 Loss: 0.8394 Loss_dice: 0.2885 Loss_ce: 0.5510\n",
      "Epoch: 0 [790/877] [790/2631] lr:0.0004665 Loss: 0.7601 Loss_dice: 0.2727 Loss_ce: 0.4874\n",
      "Epoch: 0 [791/877] [791/2631] lr:0.0004664 Loss: 0.7703 Loss_dice: 0.2838 Loss_ce: 0.4865\n",
      "Epoch: 0 [792/877] [792/2631] lr:0.0004664 Loss: 0.7862 Loss_dice: 0.2874 Loss_ce: 0.4989\n",
      "Epoch: 0 [793/877] [793/2631] lr:0.0004663 Loss: 0.7423 Loss_dice: 0.2710 Loss_ce: 0.4713\n",
      "Epoch: 0 [794/877] [794/2631] lr:0.0004662 Loss: 0.8708 Loss_dice: 0.3020 Loss_ce: 0.5688\n",
      "Epoch: 0 [795/877] [795/2631] lr:0.0004661 Loss: 0.7059 Loss_dice: 0.2548 Loss_ce: 0.4512\n",
      "Epoch: 0 [796/877] [796/2631] lr:0.0004660 Loss: 0.6863 Loss_dice: 0.2162 Loss_ce: 0.4701\n",
      "Epoch: 0 [797/877] [797/2631] lr:0.0004659 Loss: 0.7337 Loss_dice: 0.2575 Loss_ce: 0.4762\n",
      "Epoch: 0 [798/877] [798/2631] lr:0.0004658 Loss: 0.7082 Loss_dice: 0.2345 Loss_ce: 0.4736\n",
      "Epoch: 0 [799/877] [799/2631] lr:0.0004657 Loss: 0.7978 Loss_dice: 0.2669 Loss_ce: 0.5309\n",
      "Epoch: 0 [800/877] [800/2631] lr:0.0004656 Loss: 0.6897 Loss_dice: 0.2220 Loss_ce: 0.4677\n",
      "Epoch: 0 [801/877] [801/2631] lr:0.0004655 Loss: 0.8595 Loss_dice: 0.2750 Loss_ce: 0.5846\n",
      "Epoch: 0 [802/877] [802/2631] lr:0.0004655 Loss: 0.7597 Loss_dice: 0.2469 Loss_ce: 0.5129\n",
      "Epoch: 0 [803/877] [803/2631] lr:0.0004654 Loss: 0.6509 Loss_dice: 0.1990 Loss_ce: 0.4519\n",
      "Epoch: 0 [804/877] [804/2631] lr:0.0004653 Loss: 0.7451 Loss_dice: 0.2391 Loss_ce: 0.5061\n",
      "Epoch: 0 [805/877] [805/2631] lr:0.0004652 Loss: 0.6797 Loss_dice: 0.2099 Loss_ce: 0.4698\n",
      "Epoch: 0 [806/877] [806/2631] lr:0.0004651 Loss: 0.7195 Loss_dice: 0.2376 Loss_ce: 0.4819\n",
      "Epoch: 0 [807/877] [807/2631] lr:0.0004650 Loss: 0.6909 Loss_dice: 0.2335 Loss_ce: 0.4574\n",
      "Epoch: 0 [808/877] [808/2631] lr:0.0004649 Loss: 0.7095 Loss_dice: 0.2468 Loss_ce: 0.4628\n",
      "Epoch: 0 [809/877] [809/2631] lr:0.0004648 Loss: 0.6989 Loss_dice: 0.2348 Loss_ce: 0.4640\n",
      "Epoch: 0 [810/877] [810/2631] lr:0.0004647 Loss: 0.7328 Loss_dice: 0.2360 Loss_ce: 0.4968\n",
      "Epoch: 0 [811/877] [811/2631] lr:0.0004646 Loss: 0.7672 Loss_dice: 0.2774 Loss_ce: 0.4898\n",
      "Epoch: 0 [812/877] [812/2631] lr:0.0004645 Loss: 0.7018 Loss_dice: 0.2560 Loss_ce: 0.4458\n",
      "Epoch: 0 [813/877] [813/2631] lr:0.0004644 Loss: 0.6317 Loss_dice: 0.2111 Loss_ce: 0.4205\n",
      "Epoch: 0 [814/877] [814/2631] lr:0.0004644 Loss: 0.7550 Loss_dice: 0.2657 Loss_ce: 0.4893\n",
      "Epoch: 0 [815/877] [815/2631] lr:0.0004643 Loss: 0.7333 Loss_dice: 0.2537 Loss_ce: 0.4795\n",
      "Epoch: 0 [816/877] [816/2631] lr:0.0004642 Loss: 0.7130 Loss_dice: 0.2392 Loss_ce: 0.4738\n",
      "Epoch: 0 [817/877] [817/2631] lr:0.0004641 Loss: 0.7196 Loss_dice: 0.2466 Loss_ce: 0.4730\n",
      "Epoch: 0 [818/877] [818/2631] lr:0.0004640 Loss: 0.7599 Loss_dice: 0.2717 Loss_ce: 0.4882\n",
      "Epoch: 0 [819/877] [819/2631] lr:0.0004639 Loss: 0.6439 Loss_dice: 0.2221 Loss_ce: 0.4218\n",
      "Epoch: 0 [820/877] [820/2631] lr:0.0004638 Loss: 0.7732 Loss_dice: 0.2654 Loss_ce: 0.5078\n",
      "Epoch: 0 [821/877] [821/2631] lr:0.0004637 Loss: 0.9828 Loss_dice: 0.3215 Loss_ce: 0.6614\n",
      "Epoch: 0 [822/877] [822/2631] lr:0.0004636 Loss: 0.7465 Loss_dice: 0.2447 Loss_ce: 0.5019\n",
      "Epoch: 0 [823/877] [823/2631] lr:0.0004635 Loss: 0.6850 Loss_dice: 0.2119 Loss_ce: 0.4731\n",
      "Epoch: 0 [824/877] [824/2631] lr:0.0004634 Loss: 0.6469 Loss_dice: 0.2124 Loss_ce: 0.4345\n",
      "Epoch: 0 [825/877] [825/2631] lr:0.0004633 Loss: 0.6746 Loss_dice: 0.2201 Loss_ce: 0.4546\n",
      "Epoch: 0 [826/877] [826/2631] lr:0.0004632 Loss: 0.6364 Loss_dice: 0.2014 Loss_ce: 0.4349\n",
      "Epoch: 0 [827/877] [827/2631] lr:0.0004631 Loss: 0.7430 Loss_dice: 0.2492 Loss_ce: 0.4938\n",
      "Epoch: 0 [828/877] [828/2631] lr:0.0004631 Loss: 0.8820 Loss_dice: 0.3002 Loss_ce: 0.5818\n",
      "Epoch: 0 [829/877] [829/2631] lr:0.0004630 Loss: 0.6885 Loss_dice: 0.2333 Loss_ce: 0.4552\n",
      "Epoch: 0 [830/877] [830/2631] lr:0.0004629 Loss: 0.6324 Loss_dice: 0.1749 Loss_ce: 0.4574\n",
      "Epoch: 0 [831/877] [831/2631] lr:0.0004628 Loss: 0.6496 Loss_dice: 0.2244 Loss_ce: 0.4252\n",
      "Epoch: 0 [832/877] [832/2631] lr:0.0004627 Loss: 0.6664 Loss_dice: 0.2351 Loss_ce: 0.4313\n",
      "Epoch: 0 [833/877] [833/2631] lr:0.0004626 Loss: 1.0163 Loss_dice: 0.3541 Loss_ce: 0.6622\n",
      "Epoch: 0 [834/877] [834/2631] lr:0.0004625 Loss: 1.0581 Loss_dice: 0.3448 Loss_ce: 0.7133\n",
      "Epoch: 0 [835/877] [835/2631] lr:0.0004624 Loss: 0.6572 Loss_dice: 0.2511 Loss_ce: 0.4060\n",
      "Epoch: 0 [836/877] [836/2631] lr:0.0004623 Loss: 0.6745 Loss_dice: 0.2488 Loss_ce: 0.4258\n",
      "Epoch: 0 [837/877] [837/2631] lr:0.0004622 Loss: 0.7346 Loss_dice: 0.2669 Loss_ce: 0.4677\n",
      "Epoch: 0 [838/877] [838/2631] lr:0.0004621 Loss: 0.6092 Loss_dice: 0.2044 Loss_ce: 0.4049\n",
      "Epoch: 0 [839/877] [839/2631] lr:0.0004620 Loss: 0.6569 Loss_dice: 0.2339 Loss_ce: 0.4230\n",
      "Epoch: 0 [840/877] [840/2631] lr:0.0004619 Loss: 0.6934 Loss_dice: 0.2410 Loss_ce: 0.4524\n",
      "Epoch: 0 [841/877] [841/2631] lr:0.0004618 Loss: 0.6659 Loss_dice: 0.2305 Loss_ce: 0.4354\n",
      "Epoch: 0 [842/877] [842/2631] lr:0.0004617 Loss: 0.7093 Loss_dice: 0.2435 Loss_ce: 0.4657\n",
      "Epoch: 0 [843/877] [843/2631] lr:0.0004616 Loss: 0.6440 Loss_dice: 0.1935 Loss_ce: 0.4505\n",
      "Epoch: 0 [844/877] [844/2631] lr:0.0004615 Loss: 0.7284 Loss_dice: 0.2410 Loss_ce: 0.4874\n",
      "Epoch: 0 [845/877] [845/2631] lr:0.0004614 Loss: 0.8243 Loss_dice: 0.2746 Loss_ce: 0.5498\n",
      "Epoch: 0 [846/877] [846/2631] lr:0.0004614 Loss: 0.6517 Loss_dice: 0.2299 Loss_ce: 0.4218\n",
      "Epoch: 0 [847/877] [847/2631] lr:0.0004613 Loss: 0.6995 Loss_dice: 0.2527 Loss_ce: 0.4468\n",
      "Epoch: 0 [848/877] [848/2631] lr:0.0004612 Loss: 0.6091 Loss_dice: 0.2210 Loss_ce: 0.3882\n",
      "Epoch: 0 [849/877] [849/2631] lr:0.0004611 Loss: 0.8805 Loss_dice: 0.2822 Loss_ce: 0.5984\n",
      "Epoch: 0 [850/877] [850/2631] lr:0.0004610 Loss: 0.7027 Loss_dice: 0.2682 Loss_ce: 0.4345\n",
      "Epoch: 0 [851/877] [851/2631] lr:0.0004609 Loss: 0.8335 Loss_dice: 0.2969 Loss_ce: 0.5365\n",
      "Epoch: 0 [852/877] [852/2631] lr:0.0004608 Loss: 0.6669 Loss_dice: 0.2497 Loss_ce: 0.4172\n",
      "Epoch: 0 [853/877] [853/2631] lr:0.0004607 Loss: 0.7313 Loss_dice: 0.2139 Loss_ce: 0.5174\n",
      "Epoch: 0 [854/877] [854/2631] lr:0.0004606 Loss: 0.7555 Loss_dice: 0.2754 Loss_ce: 0.4800\n",
      "Epoch: 0 [855/877] [855/2631] lr:0.0004605 Loss: 0.6654 Loss_dice: 0.2332 Loss_ce: 0.4322\n",
      "Epoch: 0 [856/877] [856/2631] lr:0.0004604 Loss: 0.6368 Loss_dice: 0.2410 Loss_ce: 0.3959\n",
      "Epoch: 0 [857/877] [857/2631] lr:0.0004603 Loss: 0.6787 Loss_dice: 0.2580 Loss_ce: 0.4206\n",
      "Epoch: 0 [858/877] [858/2631] lr:0.0004602 Loss: 0.7087 Loss_dice: 0.2695 Loss_ce: 0.4392\n",
      "Epoch: 0 [859/877] [859/2631] lr:0.0004601 Loss: 0.6444 Loss_dice: 0.2473 Loss_ce: 0.3971\n",
      "Epoch: 0 [860/877] [860/2631] lr:0.0004600 Loss: 0.6346 Loss_dice: 0.2304 Loss_ce: 0.4042\n",
      "Epoch: 0 [861/877] [861/2631] lr:0.0004599 Loss: 0.7537 Loss_dice: 0.2432 Loss_ce: 0.5105\n",
      "Epoch: 0 [862/877] [862/2631] lr:0.0004598 Loss: 0.6492 Loss_dice: 0.2374 Loss_ce: 0.4118\n",
      "Epoch: 0 [863/877] [863/2631] lr:0.0004597 Loss: 0.6548 Loss_dice: 0.2256 Loss_ce: 0.4292\n",
      "Epoch: 0 [864/877] [864/2631] lr:0.0004596 Loss: 0.7556 Loss_dice: 0.2699 Loss_ce: 0.4857\n",
      "Epoch: 0 [865/877] [865/2631] lr:0.0004595 Loss: 0.9175 Loss_dice: 0.3164 Loss_ce: 0.6011\n",
      "Epoch: 0 [866/877] [866/2631] lr:0.0004594 Loss: 0.6977 Loss_dice: 0.2154 Loss_ce: 0.4823\n",
      "Epoch: 0 [867/877] [867/2631] lr:0.0004593 Loss: 0.7126 Loss_dice: 0.2667 Loss_ce: 0.4459\n",
      "Epoch: 0 [868/877] [868/2631] lr:0.0004592 Loss: 0.6249 Loss_dice: 0.1930 Loss_ce: 0.4319\n",
      "Epoch: 0 [869/877] [869/2631] lr:0.0004591 Loss: 0.6117 Loss_dice: 0.2408 Loss_ce: 0.3709\n",
      "Epoch: 0 [870/877] [870/2631] lr:0.0004590 Loss: 0.6019 Loss_dice: 0.1940 Loss_ce: 0.4079\n",
      "Epoch: 0 [871/877] [871/2631] lr:0.0004589 Loss: 0.7835 Loss_dice: 0.3009 Loss_ce: 0.4826\n",
      "Epoch: 0 [872/877] [872/2631] lr:0.0004588 Loss: 0.9834 Loss_dice: 0.3574 Loss_ce: 0.6259\n",
      "Epoch: 0 [873/877] [873/2631] lr:0.0004587 Loss: 0.6234 Loss_dice: 0.2607 Loss_ce: 0.3627\n",
      "Epoch: 0 [874/877] [874/2631] lr:0.0004586 Loss: 0.6438 Loss_dice: 0.2792 Loss_ce: 0.3646\n",
      "Epoch: 0 [875/877] [875/2631] lr:0.0004585 Loss: 0.6256 Loss_dice: 0.2602 Loss_ce: 0.3654\n",
      "Epoch: 0 [876/877] [876/2631] lr:0.0004584 Loss: 0.8731 Loss_dice: 0.3483 Loss_ce: 0.5247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(8379:140121727778944,MainProcess):2022-11-09-04:38:19.944.590 [mindspore/dataset/engine/datasets_user_defined.py:766] GeneratorDataset's num_parallel_workers: 4 is too large which may cause a lot of memory occupation (>85%) or out of memory(OOM) during multiprocessing. Therefore, it is recommended to reduce num_parallel_workers to 2 or smaller.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model to output/./checkpoint//Epoch_0_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(14183:140121727778944,_MPWorker-89):2022-11-09-04:38:43.962.333 [mindspore/dataset/engine/queue.py:120] Using shared memory queue, but rowsize is larger than allocated memory max_rowsize 16777216 current rowsize 77070336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [0/877] [877/2631] lr:0.0004583 Loss: 0.7121 Loss_dice: 0.3131 Loss_ce: 0.3991\n",
      "Epoch: 1 [1/877] [878/2631] lr:0.0004582 Loss: 0.6560 Loss_dice: 0.2901 Loss_ce: 0.3659\n",
      "Epoch: 1 [2/877] [879/2631] lr:0.0004581 Loss: 0.6204 Loss_dice: 0.2755 Loss_ce: 0.3449\n",
      "Epoch: 1 [3/877] [880/2631] lr:0.0004580 Loss: 0.6449 Loss_dice: 0.2739 Loss_ce: 0.3710\n",
      "Epoch: 1 [4/877] [881/2631] lr:0.0004579 Loss: 0.7549 Loss_dice: 0.2869 Loss_ce: 0.4680\n",
      "Epoch: 1 [5/877] [882/2631] lr:0.0004578 Loss: 0.7031 Loss_dice: 0.2751 Loss_ce: 0.4280\n",
      "Epoch: 1 [6/877] [883/2631] lr:0.0004577 Loss: 0.6264 Loss_dice: 0.2454 Loss_ce: 0.3810\n",
      "Epoch: 1 [7/877] [884/2631] lr:0.0004576 Loss: 0.6576 Loss_dice: 0.2445 Loss_ce: 0.4131\n",
      "Epoch: 1 [8/877] [885/2631] lr:0.0004575 Loss: 0.6069 Loss_dice: 0.2354 Loss_ce: 0.3715\n",
      "Epoch: 1 [9/877] [886/2631] lr:0.0004574 Loss: 0.7210 Loss_dice: 0.2661 Loss_ce: 0.4549\n",
      "Epoch: 1 [10/877] [887/2631] lr:0.0004573 Loss: 0.6224 Loss_dice: 0.2435 Loss_ce: 0.3789\n",
      "Epoch: 1 [11/877] [888/2631] lr:0.0004572 Loss: 0.7171 Loss_dice: 0.2600 Loss_ce: 0.4571\n",
      "Epoch: 1 [12/877] [889/2631] lr:0.0004571 Loss: 0.6928 Loss_dice: 0.2595 Loss_ce: 0.4333\n",
      "Epoch: 1 [13/877] [890/2631] lr:0.0004570 Loss: 0.6372 Loss_dice: 0.2525 Loss_ce: 0.3848\n",
      "Epoch: 1 [14/877] [891/2631] lr:0.0004569 Loss: 0.5996 Loss_dice: 0.2441 Loss_ce: 0.3555\n",
      "Epoch: 1 [15/877] [892/2631] lr:0.0004568 Loss: 0.9692 Loss_dice: 0.3316 Loss_ce: 0.6376\n",
      "Epoch: 1 [16/877] [893/2631] lr:0.0004567 Loss: 0.6857 Loss_dice: 0.2548 Loss_ce: 0.4309\n",
      "Epoch: 1 [17/877] [894/2631] lr:0.0004566 Loss: 0.5993 Loss_dice: 0.2477 Loss_ce: 0.3516\n",
      "Epoch: 1 [18/877] [895/2631] lr:0.0004565 Loss: 0.8379 Loss_dice: 0.3186 Loss_ce: 0.5193\n",
      "Epoch: 1 [19/877] [896/2631] lr:0.0004564 Loss: 0.6719 Loss_dice: 0.2416 Loss_ce: 0.4304\n",
      "Epoch: 1 [20/877] [897/2631] lr:0.0004563 Loss: 0.7148 Loss_dice: 0.2843 Loss_ce: 0.4306\n",
      "Epoch: 1 [21/877] [898/2631] lr:0.0004562 Loss: 0.6174 Loss_dice: 0.2571 Loss_ce: 0.3603\n",
      "Epoch: 1 [22/877] [899/2631] lr:0.0004561 Loss: 0.7553 Loss_dice: 0.2892 Loss_ce: 0.4661\n",
      "Epoch: 1 [23/877] [900/2631] lr:0.0004560 Loss: 0.7210 Loss_dice: 0.2899 Loss_ce: 0.4311\n",
      "Epoch: 1 [24/877] [901/2631] lr:0.0004559 Loss: 0.6306 Loss_dice: 0.2582 Loss_ce: 0.3724\n",
      "Epoch: 1 [25/877] [902/2631] lr:0.0004558 Loss: 0.6541 Loss_dice: 0.2723 Loss_ce: 0.3818\n",
      "Epoch: 1 [26/877] [903/2631] lr:0.0004557 Loss: 0.6153 Loss_dice: 0.2513 Loss_ce: 0.3640\n",
      "Epoch: 1 [27/877] [904/2631] lr:0.0004556 Loss: 0.6006 Loss_dice: 0.2455 Loss_ce: 0.3552\n",
      "Epoch: 1 [28/877] [905/2631] lr:0.0004555 Loss: 0.6641 Loss_dice: 0.2669 Loss_ce: 0.3972\n",
      "Epoch: 1 [29/877] [906/2631] lr:0.0004554 Loss: 0.5630 Loss_dice: 0.2270 Loss_ce: 0.3360\n",
      "Epoch: 1 [30/877] [907/2631] lr:0.0004553 Loss: 0.5885 Loss_dice: 0.2227 Loss_ce: 0.3658\n",
      "Epoch: 1 [31/877] [908/2631] lr:0.0004552 Loss: 0.6555 Loss_dice: 0.2465 Loss_ce: 0.4089\n",
      "Epoch: 1 [32/877] [909/2631] lr:0.0004551 Loss: 0.5888 Loss_dice: 0.2337 Loss_ce: 0.3550\n",
      "Epoch: 1 [33/877] [910/2631] lr:0.0004550 Loss: 0.6588 Loss_dice: 0.2205 Loss_ce: 0.4383\n",
      "Epoch: 1 [34/877] [911/2631] lr:0.0004549 Loss: 0.5808 Loss_dice: 0.2260 Loss_ce: 0.3548\n",
      "Epoch: 1 [35/877] [912/2631] lr:0.0004548 Loss: 0.8741 Loss_dice: 0.3262 Loss_ce: 0.5479\n",
      "Epoch: 1 [36/877] [913/2631] lr:0.0004547 Loss: 0.6518 Loss_dice: 0.2560 Loss_ce: 0.3958\n",
      "Epoch: 1 [37/877] [914/2631] lr:0.0004546 Loss: 0.7666 Loss_dice: 0.2871 Loss_ce: 0.4795\n",
      "Epoch: 1 [38/877] [915/2631] lr:0.0004545 Loss: 0.6579 Loss_dice: 0.2661 Loss_ce: 0.3917\n",
      "Epoch: 1 [39/877] [916/2631] lr:0.0004544 Loss: 0.6613 Loss_dice: 0.2595 Loss_ce: 0.4018\n",
      "Epoch: 1 [40/877] [917/2631] lr:0.0004543 Loss: 0.7843 Loss_dice: 0.3097 Loss_ce: 0.4746\n",
      "Epoch: 1 [41/877] [918/2631] lr:0.0004542 Loss: 0.8374 Loss_dice: 0.3250 Loss_ce: 0.5124\n",
      "Epoch: 1 [42/877] [919/2631] lr:0.0004541 Loss: 0.6160 Loss_dice: 0.2597 Loss_ce: 0.3563\n",
      "Epoch: 1 [43/877] [920/2631] lr:0.0004540 Loss: 0.8085 Loss_dice: 0.2943 Loss_ce: 0.5142\n",
      "Epoch: 1 [44/877] [921/2631] lr:0.0004539 Loss: 0.6391 Loss_dice: 0.2704 Loss_ce: 0.3687\n",
      "Epoch: 1 [45/877] [922/2631] lr:0.0004538 Loss: 0.6186 Loss_dice: 0.2658 Loss_ce: 0.3528\n",
      "Epoch: 1 [46/877] [923/2631] lr:0.0004537 Loss: 0.7851 Loss_dice: 0.2992 Loss_ce: 0.4860\n",
      "Epoch: 1 [47/877] [924/2631] lr:0.0004536 Loss: 0.6037 Loss_dice: 0.2519 Loss_ce: 0.3518\n",
      "Epoch: 1 [48/877] [925/2631] lr:0.0004535 Loss: 0.9213 Loss_dice: 0.3451 Loss_ce: 0.5763\n",
      "Epoch: 1 [49/877] [926/2631] lr:0.0004534 Loss: 0.5965 Loss_dice: 0.2575 Loss_ce: 0.3390\n",
      "Epoch: 1 [50/877] [927/2631] lr:0.0004532 Loss: 0.6250 Loss_dice: 0.2453 Loss_ce: 0.3797\n",
      "Epoch: 1 [51/877] [928/2631] lr:0.0004531 Loss: 0.7467 Loss_dice: 0.2998 Loss_ce: 0.4469\n",
      "Epoch: 1 [52/877] [929/2631] lr:0.0004530 Loss: 0.6436 Loss_dice: 0.2611 Loss_ce: 0.3825\n",
      "Epoch: 1 [53/877] [930/2631] lr:0.0004529 Loss: 0.5871 Loss_dice: 0.2472 Loss_ce: 0.3399\n",
      "Epoch: 1 [54/877] [931/2631] lr:0.0004528 Loss: 0.5990 Loss_dice: 0.2477 Loss_ce: 0.3513\n",
      "Epoch: 1 [55/877] [932/2631] lr:0.0004527 Loss: 0.5804 Loss_dice: 0.2396 Loss_ce: 0.3408\n",
      "Epoch: 1 [56/877] [933/2631] lr:0.0004526 Loss: 0.5736 Loss_dice: 0.2216 Loss_ce: 0.3520\n",
      "Epoch: 1 [57/877] [934/2631] lr:0.0004525 Loss: 0.7636 Loss_dice: 0.2942 Loss_ce: 0.4694\n",
      "Epoch: 1 [58/877] [935/2631] lr:0.0004524 Loss: 0.6947 Loss_dice: 0.2706 Loss_ce: 0.4241\n",
      "Epoch: 1 [59/877] [936/2631] lr:0.0004523 Loss: 0.6126 Loss_dice: 0.2374 Loss_ce: 0.3752\n",
      "Epoch: 1 [60/877] [937/2631] lr:0.0004522 Loss: 0.7526 Loss_dice: 0.2848 Loss_ce: 0.4678\n",
      "Epoch: 1 [61/877] [938/2631] lr:0.0004521 Loss: 0.7142 Loss_dice: 0.2713 Loss_ce: 0.4429\n",
      "Epoch: 1 [62/877] [939/2631] lr:0.0004520 Loss: 0.6424 Loss_dice: 0.2456 Loss_ce: 0.3968\n",
      "Epoch: 1 [63/877] [940/2631] lr:0.0004519 Loss: 0.6139 Loss_dice: 0.2450 Loss_ce: 0.3689\n",
      "Epoch: 1 [64/877] [941/2631] lr:0.0004518 Loss: 0.6289 Loss_dice: 0.2479 Loss_ce: 0.3811\n",
      "Epoch: 1 [65/877] [942/2631] lr:0.0004517 Loss: 0.6376 Loss_dice: 0.2509 Loss_ce: 0.3867\n",
      "Epoch: 1 [66/877] [943/2631] lr:0.0004516 Loss: 0.5930 Loss_dice: 0.2377 Loss_ce: 0.3553\n",
      "Epoch: 1 [67/877] [944/2631] lr:0.0004515 Loss: 0.6487 Loss_dice: 0.2361 Loss_ce: 0.4126\n",
      "Epoch: 1 [68/877] [945/2631] lr:0.0004514 Loss: 0.7442 Loss_dice: 0.2742 Loss_ce: 0.4700\n",
      "Epoch: 1 [69/877] [946/2631] lr:0.0004512 Loss: 0.6380 Loss_dice: 0.2654 Loss_ce: 0.3726\n",
      "Epoch: 1 [70/877] [947/2631] lr:0.0004511 Loss: 0.6206 Loss_dice: 0.2615 Loss_ce: 0.3591\n",
      "Epoch: 1 [71/877] [948/2631] lr:0.0004510 Loss: 0.6114 Loss_dice: 0.2570 Loss_ce: 0.3544\n",
      "Epoch: 1 [72/877] [949/2631] lr:0.0004509 Loss: 0.8966 Loss_dice: 0.3471 Loss_ce: 0.5494\n",
      "Epoch: 1 [73/877] [950/2631] lr:0.0004508 Loss: 0.9175 Loss_dice: 0.3473 Loss_ce: 0.5702\n",
      "Epoch: 1 [74/877] [951/2631] lr:0.0004507 Loss: 0.6606 Loss_dice: 0.2928 Loss_ce: 0.3678\n",
      "Epoch: 1 [75/877] [952/2631] lr:0.0004506 Loss: 0.6723 Loss_dice: 0.2940 Loss_ce: 0.3783\n",
      "Epoch: 1 [76/877] [953/2631] lr:0.0004505 Loss: 0.6985 Loss_dice: 0.3014 Loss_ce: 0.3971\n",
      "Epoch: 1 [77/877] [954/2631] lr:0.0004504 Loss: 0.6559 Loss_dice: 0.2822 Loss_ce: 0.3737\n",
      "Epoch: 1 [78/877] [955/2631] lr:0.0004503 Loss: 0.7563 Loss_dice: 0.3124 Loss_ce: 0.4439\n",
      "Epoch: 1 [79/877] [956/2631] lr:0.0004502 Loss: 0.5758 Loss_dice: 0.2604 Loss_ce: 0.3155\n",
      "Epoch: 1 [80/877] [957/2631] lr:0.0004501 Loss: 1.0139 Loss_dice: 0.3745 Loss_ce: 0.6394\n",
      "Epoch: 1 [81/877] [958/2631] lr:0.0004500 Loss: 0.7080 Loss_dice: 0.2825 Loss_ce: 0.4255\n",
      "Epoch: 1 [82/877] [959/2631] lr:0.0004499 Loss: 0.7390 Loss_dice: 0.3046 Loss_ce: 0.4343\n",
      "Epoch: 1 [83/877] [960/2631] lr:0.0004498 Loss: 0.6804 Loss_dice: 0.2845 Loss_ce: 0.3958\n",
      "Epoch: 1 [84/877] [961/2631] lr:0.0004496 Loss: 0.7454 Loss_dice: 0.2842 Loss_ce: 0.4611\n",
      "Epoch: 1 [85/877] [962/2631] lr:0.0004495 Loss: 0.6557 Loss_dice: 0.2649 Loss_ce: 0.3908\n",
      "Epoch: 1 [86/877] [963/2631] lr:0.0004494 Loss: 0.7411 Loss_dice: 0.2850 Loss_ce: 0.4561\n",
      "Epoch: 1 [87/877] [964/2631] lr:0.0004493 Loss: 0.6074 Loss_dice: 0.2525 Loss_ce: 0.3549\n",
      "Epoch: 1 [88/877] [965/2631] lr:0.0004492 Loss: 0.5960 Loss_dice: 0.2478 Loss_ce: 0.3483\n",
      "Epoch: 1 [89/877] [966/2631] lr:0.0004491 Loss: 0.8321 Loss_dice: 0.3011 Loss_ce: 0.5310\n",
      "Epoch: 1 [90/877] [967/2631] lr:0.0004490 Loss: 0.5769 Loss_dice: 0.2281 Loss_ce: 0.3488\n",
      "Epoch: 1 [91/877] [968/2631] lr:0.0004489 Loss: 0.5996 Loss_dice: 0.2391 Loss_ce: 0.3604\n",
      "Epoch: 1 [92/877] [969/2631] lr:0.0004488 Loss: 0.7400 Loss_dice: 0.2800 Loss_ce: 0.4599\n",
      "Epoch: 1 [93/877] [970/2631] lr:0.0004487 Loss: 0.6835 Loss_dice: 0.2591 Loss_ce: 0.4245\n",
      "Epoch: 1 [94/877] [971/2631] lr:0.0004486 Loss: 0.5966 Loss_dice: 0.2401 Loss_ce: 0.3565\n",
      "Epoch: 1 [95/877] [972/2631] lr:0.0004485 Loss: 0.5809 Loss_dice: 0.2353 Loss_ce: 0.3456\n",
      "Epoch: 1 [96/877] [973/2631] lr:0.0004483 Loss: 0.6439 Loss_dice: 0.2582 Loss_ce: 0.3857\n",
      "Epoch: 1 [97/877] [974/2631] lr:0.0004482 Loss: 0.7360 Loss_dice: 0.2761 Loss_ce: 0.4599\n",
      "Epoch: 1 [98/877] [975/2631] lr:0.0004481 Loss: 0.5977 Loss_dice: 0.2226 Loss_ce: 0.3750\n",
      "Epoch: 1 [99/877] [976/2631] lr:0.0004480 Loss: 0.6370 Loss_dice: 0.2520 Loss_ce: 0.3849\n",
      "Epoch: 1 [100/877] [977/2631] lr:0.0004479 Loss: 0.6389 Loss_dice: 0.2573 Loss_ce: 0.3816\n",
      "Epoch: 1 [101/877] [978/2631] lr:0.0004478 Loss: 0.5788 Loss_dice: 0.2299 Loss_ce: 0.3490\n",
      "Epoch: 1 [102/877] [979/2631] lr:0.0004477 Loss: 0.8440 Loss_dice: 0.3050 Loss_ce: 0.5390\n",
      "Epoch: 1 [103/877] [980/2631] lr:0.0004476 Loss: 0.6016 Loss_dice: 0.2395 Loss_ce: 0.3620\n",
      "Epoch: 1 [104/877] [981/2631] lr:0.0004475 Loss: 0.7054 Loss_dice: 0.2712 Loss_ce: 0.4343\n",
      "Epoch: 1 [105/877] [982/2631] lr:0.0004474 Loss: 0.8881 Loss_dice: 0.3313 Loss_ce: 0.5568\n",
      "Epoch: 1 [106/877] [983/2631] lr:0.0004472 Loss: 0.7458 Loss_dice: 0.2794 Loss_ce: 0.4664\n",
      "Epoch: 1 [107/877] [984/2631] lr:0.0004471 Loss: 0.9501 Loss_dice: 0.3593 Loss_ce: 0.5908\n",
      "Epoch: 1 [108/877] [985/2631] lr:0.0004470 Loss: 0.6312 Loss_dice: 0.2770 Loss_ce: 0.3543\n",
      "Epoch: 1 [109/877] [986/2631] lr:0.0004469 Loss: 0.6675 Loss_dice: 0.2770 Loss_ce: 0.3906\n",
      "Epoch: 1 [110/877] [987/2631] lr:0.0004468 Loss: 0.6477 Loss_dice: 0.2677 Loss_ce: 0.3801\n",
      "Epoch: 1 [111/877] [988/2631] lr:0.0004467 Loss: 0.6760 Loss_dice: 0.2962 Loss_ce: 0.3798\n",
      "Epoch: 1 [112/877] [989/2631] lr:0.0004466 Loss: 1.0101 Loss_dice: 0.3922 Loss_ce: 0.6180\n",
      "Epoch: 1 [113/877] [990/2631] lr:0.0004465 Loss: 0.8774 Loss_dice: 0.3460 Loss_ce: 0.5314\n",
      "Epoch: 1 [114/877] [991/2631] lr:0.0004464 Loss: 1.1140 Loss_dice: 0.4096 Loss_ce: 0.7044\n",
      "Epoch: 1 [115/877] [992/2631] lr:0.0004463 Loss: 0.6815 Loss_dice: 0.2511 Loss_ce: 0.4304\n",
      "Epoch: 1 [116/877] [993/2631] lr:0.0004461 Loss: 0.6163 Loss_dice: 0.2557 Loss_ce: 0.3605\n",
      "Epoch: 1 [117/877] [994/2631] lr:0.0004460 Loss: 0.6380 Loss_dice: 0.2571 Loss_ce: 0.3809\n",
      "Epoch: 1 [118/877] [995/2631] lr:0.0004459 Loss: 0.7479 Loss_dice: 0.2890 Loss_ce: 0.4590\n",
      "Epoch: 1 [119/877] [996/2631] lr:0.0004458 Loss: 0.6106 Loss_dice: 0.2491 Loss_ce: 0.3614\n",
      "Epoch: 1 [120/877] [997/2631] lr:0.0004457 Loss: 0.6643 Loss_dice: 0.2363 Loss_ce: 0.4280\n",
      "Epoch: 1 [121/877] [998/2631] lr:0.0004456 Loss: 0.5851 Loss_dice: 0.2307 Loss_ce: 0.3543\n",
      "Epoch: 1 [122/877] [999/2631] lr:0.0004455 Loss: 0.5909 Loss_dice: 0.2332 Loss_ce: 0.3578\n",
      "Epoch: 1 [123/877] [1000/2631] lr:0.0004454 Loss: 0.8431 Loss_dice: 0.2880 Loss_ce: 0.5550\n",
      "Epoch: 1 [124/877] [1001/2631] lr:0.0004453 Loss: 0.5814 Loss_dice: 0.2150 Loss_ce: 0.3665\n",
      "Epoch: 1 [125/877] [1002/2631] lr:0.0004451 Loss: 0.6506 Loss_dice: 0.2456 Loss_ce: 0.4050\n",
      "Epoch: 1 [126/877] [1003/2631] lr:0.0004450 Loss: 0.5604 Loss_dice: 0.2060 Loss_ce: 0.3545\n",
      "Epoch: 1 [127/877] [1004/2631] lr:0.0004449 Loss: 0.5804 Loss_dice: 0.2322 Loss_ce: 0.3482\n",
      "Epoch: 1 [128/877] [1005/2631] lr:0.0004448 Loss: 0.5877 Loss_dice: 0.2326 Loss_ce: 0.3550\n",
      "Epoch: 1 [129/877] [1006/2631] lr:0.0004447 Loss: 0.6256 Loss_dice: 0.2438 Loss_ce: 0.3817\n",
      "Epoch: 1 [130/877] [1007/2631] lr:0.0004446 Loss: 0.5791 Loss_dice: 0.2353 Loss_ce: 0.3438\n",
      "Epoch: 1 [131/877] [1008/2631] lr:0.0004445 Loss: 0.7909 Loss_dice: 0.3000 Loss_ce: 0.4909\n",
      "Epoch: 1 [132/877] [1009/2631] lr:0.0004444 Loss: 0.8568 Loss_dice: 0.3011 Loss_ce: 0.5557\n",
      "Epoch: 1 [133/877] [1010/2631] lr:0.0004442 Loss: 0.5745 Loss_dice: 0.2341 Loss_ce: 0.3404\n",
      "Epoch: 1 [134/877] [1011/2631] lr:0.0004441 Loss: 0.6488 Loss_dice: 0.2463 Loss_ce: 0.4026\n",
      "Epoch: 1 [135/877] [1012/2631] lr:0.0004440 Loss: 0.5702 Loss_dice: 0.2370 Loss_ce: 0.3333\n",
      "Epoch: 1 [136/877] [1013/2631] lr:0.0004439 Loss: 0.6217 Loss_dice: 0.2526 Loss_ce: 0.3690\n",
      "Epoch: 1 [137/877] [1014/2631] lr:0.0004438 Loss: 0.5970 Loss_dice: 0.2477 Loss_ce: 0.3493\n",
      "Epoch: 1 [138/877] [1015/2631] lr:0.0004437 Loss: 0.6082 Loss_dice: 0.2447 Loss_ce: 0.3634\n",
      "Epoch: 1 [139/877] [1016/2631] lr:0.0004436 Loss: 0.5852 Loss_dice: 0.2430 Loss_ce: 0.3421\n",
      "Epoch: 1 [140/877] [1017/2631] lr:0.0004434 Loss: 0.5860 Loss_dice: 0.2294 Loss_ce: 0.3566\n",
      "Epoch: 1 [141/877] [1018/2631] lr:0.0004433 Loss: 0.6172 Loss_dice: 0.2282 Loss_ce: 0.3891\n",
      "Epoch: 1 [142/877] [1019/2631] lr:0.0004432 Loss: 0.7001 Loss_dice: 0.2695 Loss_ce: 0.4306\n",
      "Epoch: 1 [143/877] [1020/2631] lr:0.0004431 Loss: 0.6120 Loss_dice: 0.2441 Loss_ce: 0.3679\n",
      "Epoch: 1 [144/877] [1021/2631] lr:0.0004430 Loss: 0.5583 Loss_dice: 0.2226 Loss_ce: 0.3358\n",
      "Epoch: 1 [145/877] [1022/2631] lr:0.0004429 Loss: 0.5574 Loss_dice: 0.2342 Loss_ce: 0.3232\n",
      "Epoch: 1 [146/877] [1023/2631] lr:0.0004428 Loss: 0.6065 Loss_dice: 0.2527 Loss_ce: 0.3538\n",
      "Epoch: 1 [147/877] [1024/2631] lr:0.0004427 Loss: 0.7556 Loss_dice: 0.2895 Loss_ce: 0.4660\n",
      "Epoch: 1 [148/877] [1025/2631] lr:0.0004425 Loss: 0.8318 Loss_dice: 0.3134 Loss_ce: 0.5184\n",
      "Epoch: 1 [149/877] [1026/2631] lr:0.0004424 Loss: 0.6190 Loss_dice: 0.2597 Loss_ce: 0.3593\n",
      "Epoch: 1 [150/877] [1027/2631] lr:0.0004423 Loss: 0.5555 Loss_dice: 0.2350 Loss_ce: 0.3205\n",
      "Epoch: 1 [151/877] [1028/2631] lr:0.0004422 Loss: 0.6869 Loss_dice: 0.2729 Loss_ce: 0.4140\n",
      "Epoch: 1 [152/877] [1029/2631] lr:0.0004421 Loss: 0.5683 Loss_dice: 0.2453 Loss_ce: 0.3231\n",
      "Epoch: 1 [153/877] [1030/2631] lr:0.0004420 Loss: 0.5964 Loss_dice: 0.2545 Loss_ce: 0.3419\n",
      "Epoch: 1 [154/877] [1031/2631] lr:0.0004418 Loss: 0.5698 Loss_dice: 0.2502 Loss_ce: 0.3196\n",
      "Epoch: 1 [155/877] [1032/2631] lr:0.0004417 Loss: 0.6268 Loss_dice: 0.2187 Loss_ce: 0.4081\n",
      "Epoch: 1 [156/877] [1033/2631] lr:0.0004416 Loss: 0.5784 Loss_dice: 0.2488 Loss_ce: 0.3296\n",
      "Epoch: 1 [157/877] [1034/2631] lr:0.0004415 Loss: 0.7763 Loss_dice: 0.3206 Loss_ce: 0.4557\n",
      "Epoch: 1 [158/877] [1035/2631] lr:0.0004414 Loss: 0.5677 Loss_dice: 0.2369 Loss_ce: 0.3308\n",
      "Epoch: 1 [159/877] [1036/2631] lr:0.0004413 Loss: 0.5786 Loss_dice: 0.2595 Loss_ce: 0.3191\n",
      "Epoch: 1 [160/877] [1037/2631] lr:0.0004412 Loss: 0.6193 Loss_dice: 0.2796 Loss_ce: 0.3397\n",
      "Epoch: 1 [161/877] [1038/2631] lr:0.0004410 Loss: 0.5841 Loss_dice: 0.2629 Loss_ce: 0.3212\n",
      "Epoch: 1 [162/877] [1039/2631] lr:0.0004409 Loss: 0.6923 Loss_dice: 0.2975 Loss_ce: 0.3948\n",
      "Epoch: 1 [163/877] [1040/2631] lr:0.0004408 Loss: 0.7906 Loss_dice: 0.3164 Loss_ce: 0.4741\n",
      "Epoch: 1 [164/877] [1041/2631] lr:0.0004407 Loss: 0.5813 Loss_dice: 0.2649 Loss_ce: 0.3164\n",
      "Epoch: 1 [165/877] [1042/2631] lr:0.0004406 Loss: 0.5959 Loss_dice: 0.2731 Loss_ce: 0.3228\n",
      "Epoch: 1 [166/877] [1043/2631] lr:0.0004405 Loss: 0.5793 Loss_dice: 0.2637 Loss_ce: 0.3156\n",
      "Epoch: 1 [167/877] [1044/2631] lr:0.0004403 Loss: 0.6187 Loss_dice: 0.2735 Loss_ce: 0.3453\n",
      "Epoch: 1 [168/877] [1045/2631] lr:0.0004402 Loss: 0.6037 Loss_dice: 0.2718 Loss_ce: 0.3319\n",
      "Epoch: 1 [169/877] [1046/2631] lr:0.0004401 Loss: 0.5975 Loss_dice: 0.2636 Loss_ce: 0.3339\n",
      "Epoch: 1 [170/877] [1047/2631] lr:0.0004400 Loss: 0.7218 Loss_dice: 0.3073 Loss_ce: 0.4145\n",
      "Epoch: 1 [171/877] [1048/2631] lr:0.0004399 Loss: 0.5835 Loss_dice: 0.2598 Loss_ce: 0.3237\n",
      "Epoch: 1 [172/877] [1049/2631] lr:0.0004398 Loss: 0.6096 Loss_dice: 0.2705 Loss_ce: 0.3391\n",
      "Epoch: 1 [173/877] [1050/2631] lr:0.0004397 Loss: 0.6502 Loss_dice: 0.2762 Loss_ce: 0.3740\n",
      "Epoch: 1 [174/877] [1051/2631] lr:0.0004395 Loss: 0.6159 Loss_dice: 0.2665 Loss_ce: 0.3493\n",
      "Epoch: 1 [175/877] [1052/2631] lr:0.0004394 Loss: 0.7465 Loss_dice: 0.3044 Loss_ce: 0.4421\n",
      "Epoch: 1 [176/877] [1053/2631] lr:0.0004393 Loss: 0.6367 Loss_dice: 0.2784 Loss_ce: 0.3583\n",
      "Epoch: 1 [177/877] [1054/2631] lr:0.0004392 Loss: 0.6021 Loss_dice: 0.2544 Loss_ce: 0.3477\n",
      "Epoch: 1 [178/877] [1055/2631] lr:0.0004391 Loss: 0.6228 Loss_dice: 0.2686 Loss_ce: 0.3543\n",
      "Epoch: 1 [179/877] [1056/2631] lr:0.0004389 Loss: 0.6951 Loss_dice: 0.2833 Loss_ce: 0.4118\n",
      "Epoch: 1 [180/877] [1057/2631] lr:0.0004388 Loss: 0.6073 Loss_dice: 0.2626 Loss_ce: 0.3447\n",
      "Epoch: 1 [181/877] [1058/2631] lr:0.0004387 Loss: 0.7426 Loss_dice: 0.3036 Loss_ce: 0.4391\n",
      "Epoch: 1 [182/877] [1059/2631] lr:0.0004386 Loss: 0.6141 Loss_dice: 0.2749 Loss_ce: 0.3392\n",
      "Epoch: 1 [183/877] [1060/2631] lr:0.0004385 Loss: 0.5744 Loss_dice: 0.2678 Loss_ce: 0.3067\n",
      "Epoch: 1 [184/877] [1061/2631] lr:0.0004384 Loss: 0.8503 Loss_dice: 0.3216 Loss_ce: 0.5286\n",
      "Epoch: 1 [185/877] [1062/2631] lr:0.0004382 Loss: 0.5979 Loss_dice: 0.2707 Loss_ce: 0.3272\n",
      "Epoch: 1 [186/877] [1063/2631] lr:0.0004381 Loss: 0.6015 Loss_dice: 0.2663 Loss_ce: 0.3352\n",
      "Epoch: 1 [187/877] [1064/2631] lr:0.0004380 Loss: 0.9683 Loss_dice: 0.3549 Loss_ce: 0.6135\n",
      "Epoch: 1 [188/877] [1065/2631] lr:0.0004379 Loss: 1.5015 Loss_dice: 0.5108 Loss_ce: 0.9907\n",
      "Epoch: 1 [189/877] [1066/2631] lr:0.0004378 Loss: 0.5802 Loss_dice: 0.2618 Loss_ce: 0.3184\n",
      "Epoch: 1 [190/877] [1067/2631] lr:0.0004377 Loss: 0.7161 Loss_dice: 0.2635 Loss_ce: 0.4526\n",
      "Epoch: 1 [191/877] [1068/2631] lr:0.0004375 Loss: 0.6966 Loss_dice: 0.2862 Loss_ce: 0.4105\n",
      "Epoch: 1 [192/877] [1069/2631] lr:0.0004374 Loss: 0.6558 Loss_dice: 0.2457 Loss_ce: 0.4101\n",
      "Epoch: 1 [193/877] [1070/2631] lr:0.0004373 Loss: 0.6324 Loss_dice: 0.2621 Loss_ce: 0.3703\n",
      "Epoch: 1 [194/877] [1071/2631] lr:0.0004372 Loss: 0.6718 Loss_dice: 0.2707 Loss_ce: 0.4010\n",
      "Epoch: 1 [195/877] [1072/2631] lr:0.0004371 Loss: 0.7372 Loss_dice: 0.2796 Loss_ce: 0.4576\n",
      "Epoch: 1 [196/877] [1073/2631] lr:0.0004369 Loss: 0.5927 Loss_dice: 0.2441 Loss_ce: 0.3487\n",
      "Epoch: 1 [197/877] [1074/2631] lr:0.0004368 Loss: 0.5597 Loss_dice: 0.2155 Loss_ce: 0.3442\n",
      "Epoch: 1 [198/877] [1075/2631] lr:0.0004367 Loss: 0.6123 Loss_dice: 0.2451 Loss_ce: 0.3671\n",
      "Epoch: 1 [199/877] [1076/2631] lr:0.0004366 Loss: 0.6168 Loss_dice: 0.1896 Loss_ce: 0.4272\n",
      "Epoch: 1 [200/877] [1077/2631] lr:0.0004365 Loss: 1.3114 Loss_dice: 0.4412 Loss_ce: 0.8702\n",
      "Epoch: 1 [201/877] [1078/2631] lr:0.0004363 Loss: 0.5804 Loss_dice: 0.2211 Loss_ce: 0.3593\n",
      "Epoch: 1 [202/877] [1079/2631] lr:0.0004362 Loss: 0.5643 Loss_dice: 0.2068 Loss_ce: 0.3575\n",
      "Epoch: 1 [203/877] [1080/2631] lr:0.0004361 Loss: 0.6212 Loss_dice: 0.2001 Loss_ce: 0.4211\n",
      "Epoch: 1 [204/877] [1081/2631] lr:0.0004360 Loss: 0.5795 Loss_dice: 0.2156 Loss_ce: 0.3640\n",
      "Epoch: 1 [205/877] [1082/2631] lr:0.0004359 Loss: 0.5801 Loss_dice: 0.2114 Loss_ce: 0.3686\n",
      "Epoch: 1 [206/877] [1083/2631] lr:0.0004357 Loss: 0.5973 Loss_dice: 0.2353 Loss_ce: 0.3620\n",
      "Epoch: 1 [207/877] [1084/2631] lr:0.0004356 Loss: 0.8018 Loss_dice: 0.2855 Loss_ce: 0.5162\n",
      "Epoch: 1 [208/877] [1085/2631] lr:0.0004355 Loss: 0.5815 Loss_dice: 0.2291 Loss_ce: 0.3524\n",
      "Epoch: 1 [209/877] [1086/2631] lr:0.0004354 Loss: 0.5863 Loss_dice: 0.2360 Loss_ce: 0.3502\n",
      "Epoch: 1 [210/877] [1087/2631] lr:0.0004353 Loss: 0.6031 Loss_dice: 0.2299 Loss_ce: 0.3732\n",
      "Epoch: 1 [211/877] [1088/2631] lr:0.0004351 Loss: 0.6678 Loss_dice: 0.2539 Loss_ce: 0.4139\n",
      "Epoch: 1 [212/877] [1089/2631] lr:0.0004350 Loss: 0.5607 Loss_dice: 0.2318 Loss_ce: 0.3289\n",
      "Epoch: 1 [213/877] [1090/2631] lr:0.0004349 Loss: 0.5750 Loss_dice: 0.2424 Loss_ce: 0.3326\n",
      "Epoch: 1 [214/877] [1091/2631] lr:0.0004348 Loss: 0.5814 Loss_dice: 0.2425 Loss_ce: 0.3389\n",
      "Epoch: 1 [215/877] [1092/2631] lr:0.0004347 Loss: 0.5894 Loss_dice: 0.2467 Loss_ce: 0.3427\n",
      "Epoch: 1 [216/877] [1093/2631] lr:0.0004345 Loss: 0.5641 Loss_dice: 0.2488 Loss_ce: 0.3153\n",
      "Epoch: 1 [217/877] [1094/2631] lr:0.0004344 Loss: 0.5741 Loss_dice: 0.2562 Loss_ce: 0.3180\n",
      "Epoch: 1 [218/877] [1095/2631] lr:0.0004343 Loss: 0.5857 Loss_dice: 0.2510 Loss_ce: 0.3347\n",
      "Epoch: 1 [219/877] [1096/2631] lr:0.0004342 Loss: 0.5863 Loss_dice: 0.2558 Loss_ce: 0.3305\n",
      "Epoch: 1 [220/877] [1097/2631] lr:0.0004341 Loss: 0.5609 Loss_dice: 0.2569 Loss_ce: 0.3040\n",
      "Epoch: 1 [221/877] [1098/2631] lr:0.0004339 Loss: 0.5582 Loss_dice: 0.2564 Loss_ce: 0.3018\n",
      "Epoch: 1 [222/877] [1099/2631] lr:0.0004338 Loss: 0.6084 Loss_dice: 0.2451 Loss_ce: 0.3633\n",
      "Epoch: 1 [223/877] [1100/2631] lr:0.0004337 Loss: 0.5447 Loss_dice: 0.2555 Loss_ce: 0.2892\n",
      "Epoch: 1 [224/877] [1101/2631] lr:0.0004336 Loss: 0.5573 Loss_dice: 0.2534 Loss_ce: 0.3039\n",
      "Epoch: 1 [225/877] [1102/2631] lr:0.0004335 Loss: 0.5762 Loss_dice: 0.2656 Loss_ce: 0.3106\n",
      "Epoch: 1 [226/877] [1103/2631] lr:0.0004333 Loss: 0.5581 Loss_dice: 0.2595 Loss_ce: 0.2987\n",
      "Epoch: 1 [227/877] [1104/2631] lr:0.0004332 Loss: 0.5733 Loss_dice: 0.2665 Loss_ce: 0.3068\n",
      "Epoch: 1 [228/877] [1105/2631] lr:0.0004331 Loss: 0.6397 Loss_dice: 0.2875 Loss_ce: 0.3522\n",
      "Epoch: 1 [229/877] [1106/2631] lr:0.0004330 Loss: 0.7435 Loss_dice: 0.3065 Loss_ce: 0.4370\n",
      "Epoch: 1 [230/877] [1107/2631] lr:0.0004328 Loss: 0.5611 Loss_dice: 0.2564 Loss_ce: 0.3048\n",
      "Epoch: 1 [231/877] [1108/2631] lr:0.0004327 Loss: 0.5746 Loss_dice: 0.2663 Loss_ce: 0.3084\n",
      "Epoch: 1 [232/877] [1109/2631] lr:0.0004326 Loss: 0.5879 Loss_dice: 0.2731 Loss_ce: 0.3149\n",
      "Epoch: 1 [233/877] [1110/2631] lr:0.0004325 Loss: 0.5514 Loss_dice: 0.2497 Loss_ce: 0.3018\n",
      "Epoch: 1 [234/877] [1111/2631] lr:0.0004324 Loss: 0.5689 Loss_dice: 0.2671 Loss_ce: 0.3018\n",
      "Epoch: 1 [235/877] [1112/2631] lr:0.0004322 Loss: 0.5801 Loss_dice: 0.2676 Loss_ce: 0.3125\n",
      "Epoch: 1 [236/877] [1113/2631] lr:0.0004321 Loss: 0.5593 Loss_dice: 0.2586 Loss_ce: 0.3006\n",
      "Epoch: 1 [237/877] [1114/2631] lr:0.0004320 Loss: 0.5809 Loss_dice: 0.2431 Loss_ce: 0.3378\n",
      "Epoch: 1 [238/877] [1115/2631] lr:0.0004319 Loss: 0.5809 Loss_dice: 0.2631 Loss_ce: 0.3178\n",
      "Epoch: 1 [239/877] [1116/2631] lr:0.0004317 Loss: 0.6464 Loss_dice: 0.2814 Loss_ce: 0.3650\n",
      "Epoch: 1 [240/877] [1117/2631] lr:0.0004316 Loss: 0.5735 Loss_dice: 0.2503 Loss_ce: 0.3233\n",
      "Epoch: 1 [241/877] [1118/2631] lr:0.0004315 Loss: 0.5667 Loss_dice: 0.2715 Loss_ce: 0.2952\n",
      "Epoch: 1 [242/877] [1119/2631] lr:0.0004314 Loss: 0.6911 Loss_dice: 0.3047 Loss_ce: 0.3863\n",
      "Epoch: 1 [243/877] [1120/2631] lr:0.0004312 Loss: 0.5607 Loss_dice: 0.2655 Loss_ce: 0.2952\n",
      "Epoch: 1 [244/877] [1121/2631] lr:0.0004311 Loss: 0.5826 Loss_dice: 0.2771 Loss_ce: 0.3055\n",
      "Epoch: 1 [245/877] [1122/2631] lr:0.0004310 Loss: 0.5545 Loss_dice: 0.2635 Loss_ce: 0.2910\n",
      "Epoch: 1 [246/877] [1123/2631] lr:0.0004309 Loss: 0.5677 Loss_dice: 0.2747 Loss_ce: 0.2930\n",
      "Epoch: 1 [247/877] [1124/2631] lr:0.0004308 Loss: 0.6138 Loss_dice: 0.2719 Loss_ce: 0.3419\n",
      "Epoch: 1 [248/877] [1125/2631] lr:0.0004306 Loss: 0.6650 Loss_dice: 0.3063 Loss_ce: 0.3587\n",
      "Epoch: 1 [249/877] [1126/2631] lr:0.0004305 Loss: 0.5573 Loss_dice: 0.2425 Loss_ce: 0.3148\n",
      "Epoch: 1 [250/877] [1127/2631] lr:0.0004304 Loss: 0.5756 Loss_dice: 0.2575 Loss_ce: 0.3180\n",
      "Epoch: 1 [251/877] [1128/2631] lr:0.0004303 Loss: 0.5590 Loss_dice: 0.2677 Loss_ce: 0.2913\n",
      "Epoch: 1 [252/877] [1129/2631] lr:0.0004301 Loss: 0.5806 Loss_dice: 0.2709 Loss_ce: 0.3097\n",
      "Epoch: 1 [253/877] [1130/2631] lr:0.0004300 Loss: 0.5652 Loss_dice: 0.2661 Loss_ce: 0.2992\n",
      "Epoch: 1 [254/877] [1131/2631] lr:0.0004299 Loss: 0.5604 Loss_dice: 0.2534 Loss_ce: 0.3070\n",
      "Epoch: 1 [255/877] [1132/2631] lr:0.0004298 Loss: 0.5763 Loss_dice: 0.2748 Loss_ce: 0.3015\n",
      "Epoch: 1 [256/877] [1133/2631] lr:0.0004296 Loss: 0.6734 Loss_dice: 0.2893 Loss_ce: 0.3841\n",
      "Epoch: 1 [257/877] [1134/2631] lr:0.0004295 Loss: 0.5612 Loss_dice: 0.2709 Loss_ce: 0.2903\n",
      "Epoch: 1 [258/877] [1135/2631] lr:0.0004294 Loss: 0.5859 Loss_dice: 0.2739 Loss_ce: 0.3120\n",
      "Epoch: 1 [259/877] [1136/2631] lr:0.0004293 Loss: 0.5933 Loss_dice: 0.2789 Loss_ce: 0.3144\n",
      "Epoch: 1 [260/877] [1137/2631] lr:0.0004291 Loss: 0.5943 Loss_dice: 0.2806 Loss_ce: 0.3137\n",
      "Epoch: 1 [261/877] [1138/2631] lr:0.0004290 Loss: 0.5505 Loss_dice: 0.2681 Loss_ce: 0.2824\n",
      "Epoch: 1 [262/877] [1139/2631] lr:0.0004289 Loss: 0.5741 Loss_dice: 0.2654 Loss_ce: 0.3086\n",
      "Epoch: 1 [263/877] [1140/2631] lr:0.0004288 Loss: 0.5545 Loss_dice: 0.2734 Loss_ce: 0.2811\n",
      "Epoch: 1 [264/877] [1141/2631] lr:0.0004286 Loss: 0.5615 Loss_dice: 0.2739 Loss_ce: 0.2876\n",
      "Epoch: 1 [265/877] [1142/2631] lr:0.0004285 Loss: 0.7125 Loss_dice: 0.2962 Loss_ce: 0.4163\n",
      "Epoch: 1 [266/877] [1143/2631] lr:0.0004284 Loss: 0.5448 Loss_dice: 0.2676 Loss_ce: 0.2772\n",
      "Epoch: 1 [267/877] [1144/2631] lr:0.0004283 Loss: 0.6217 Loss_dice: 0.2857 Loss_ce: 0.3360\n",
      "Epoch: 1 [268/877] [1145/2631] lr:0.0004281 Loss: 0.7210 Loss_dice: 0.3183 Loss_ce: 0.4027\n",
      "Epoch: 1 [269/877] [1146/2631] lr:0.0004280 Loss: 0.6360 Loss_dice: 0.2690 Loss_ce: 0.3670\n",
      "Epoch: 1 [270/877] [1147/2631] lr:0.0004279 Loss: 0.5680 Loss_dice: 0.2786 Loss_ce: 0.2895\n",
      "Epoch: 1 [271/877] [1148/2631] lr:0.0004278 Loss: 0.5776 Loss_dice: 0.2780 Loss_ce: 0.2996\n",
      "Epoch: 1 [272/877] [1149/2631] lr:0.0004276 Loss: 0.5431 Loss_dice: 0.2736 Loss_ce: 0.2695\n",
      "Epoch: 1 [273/877] [1150/2631] lr:0.0004275 Loss: 1.3239 Loss_dice: 0.4594 Loss_ce: 0.8646\n",
      "Epoch: 1 [274/877] [1151/2631] lr:0.0004274 Loss: 0.5692 Loss_dice: 0.2765 Loss_ce: 0.2928\n",
      "Epoch: 1 [275/877] [1152/2631] lr:0.0004273 Loss: 0.6003 Loss_dice: 0.2779 Loss_ce: 0.3224\n",
      "Epoch: 1 [276/877] [1153/2631] lr:0.0004271 Loss: 0.7925 Loss_dice: 0.3393 Loss_ce: 0.4532\n",
      "Epoch: 1 [277/877] [1154/2631] lr:0.0004270 Loss: 0.6042 Loss_dice: 0.2854 Loss_ce: 0.3188\n",
      "Epoch: 1 [278/877] [1155/2631] lr:0.0004269 Loss: 0.6313 Loss_dice: 0.2955 Loss_ce: 0.3358\n",
      "Epoch: 1 [279/877] [1156/2631] lr:0.0004267 Loss: 0.6862 Loss_dice: 0.2907 Loss_ce: 0.3955\n",
      "Epoch: 1 [280/877] [1157/2631] lr:0.0004266 Loss: 0.7414 Loss_dice: 0.3259 Loss_ce: 0.4156\n",
      "Epoch: 1 [281/877] [1158/2631] lr:0.0004265 Loss: 0.5985 Loss_dice: 0.2710 Loss_ce: 0.3276\n",
      "Epoch: 1 [282/877] [1159/2631] lr:0.0004264 Loss: 0.5915 Loss_dice: 0.2864 Loss_ce: 0.3051\n",
      "Epoch: 1 [283/877] [1160/2631] lr:0.0004262 Loss: 0.5764 Loss_dice: 0.2601 Loss_ce: 0.3163\n",
      "Epoch: 1 [284/877] [1161/2631] lr:0.0004261 Loss: 0.6247 Loss_dice: 0.2589 Loss_ce: 0.3658\n",
      "Epoch: 1 [285/877] [1162/2631] lr:0.0004260 Loss: 0.5707 Loss_dice: 0.2667 Loss_ce: 0.3040\n",
      "Epoch: 1 [286/877] [1163/2631] lr:0.0004259 Loss: 0.6518 Loss_dice: 0.2817 Loss_ce: 0.3701\n",
      "Epoch: 1 [287/877] [1164/2631] lr:0.0004257 Loss: 0.6741 Loss_dice: 0.2773 Loss_ce: 0.3969\n",
      "Epoch: 1 [288/877] [1165/2631] lr:0.0004256 Loss: 0.5744 Loss_dice: 0.2619 Loss_ce: 0.3126\n",
      "Epoch: 1 [289/877] [1166/2631] lr:0.0004255 Loss: 0.5592 Loss_dice: 0.2334 Loss_ce: 0.3257\n",
      "Epoch: 1 [290/877] [1167/2631] lr:0.0004253 Loss: 0.5560 Loss_dice: 0.2311 Loss_ce: 0.3249\n",
      "Epoch: 1 [291/877] [1168/2631] lr:0.0004252 Loss: 0.6148 Loss_dice: 0.2628 Loss_ce: 0.3520\n",
      "Epoch: 1 [292/877] [1169/2631] lr:0.0004251 Loss: 0.6079 Loss_dice: 0.2629 Loss_ce: 0.3450\n",
      "Epoch: 1 [293/877] [1170/2631] lr:0.0004250 Loss: 0.5754 Loss_dice: 0.2407 Loss_ce: 0.3346\n",
      "Epoch: 1 [294/877] [1171/2631] lr:0.0004248 Loss: 0.5612 Loss_dice: 0.2556 Loss_ce: 0.3056\n",
      "Epoch: 1 [295/877] [1172/2631] lr:0.0004247 Loss: 0.6407 Loss_dice: 0.2877 Loss_ce: 0.3530\n",
      "Epoch: 1 [296/877] [1173/2631] lr:0.0004246 Loss: 0.6064 Loss_dice: 0.2656 Loss_ce: 0.3408\n",
      "Epoch: 1 [297/877] [1174/2631] lr:0.0004245 Loss: 0.6180 Loss_dice: 0.2631 Loss_ce: 0.3548\n",
      "Epoch: 1 [298/877] [1175/2631] lr:0.0004243 Loss: 0.5746 Loss_dice: 0.2384 Loss_ce: 0.3362\n",
      "Epoch: 1 [299/877] [1176/2631] lr:0.0004242 Loss: 0.6494 Loss_dice: 0.2719 Loss_ce: 0.3775\n",
      "Epoch: 1 [300/877] [1177/2631] lr:0.0004241 Loss: 0.5551 Loss_dice: 0.2392 Loss_ce: 0.3159\n",
      "Epoch: 1 [301/877] [1178/2631] lr:0.0004239 Loss: 0.5573 Loss_dice: 0.2572 Loss_ce: 0.3001\n",
      "Epoch: 1 [302/877] [1179/2631] lr:0.0004238 Loss: 0.5972 Loss_dice: 0.2282 Loss_ce: 0.3690\n",
      "Epoch: 1 [303/877] [1180/2631] lr:0.0004237 Loss: 0.5585 Loss_dice: 0.2467 Loss_ce: 0.3119\n",
      "Epoch: 1 [304/877] [1181/2631] lr:0.0004236 Loss: 0.5632 Loss_dice: 0.2662 Loss_ce: 0.2970\n",
      "Epoch: 1 [305/877] [1182/2631] lr:0.0004234 Loss: 0.5861 Loss_dice: 0.2729 Loss_ce: 0.3131\n",
      "Epoch: 1 [306/877] [1183/2631] lr:0.0004233 Loss: 0.6917 Loss_dice: 0.2974 Loss_ce: 0.3944\n",
      "Epoch: 1 [307/877] [1184/2631] lr:0.0004232 Loss: 0.5534 Loss_dice: 0.2566 Loss_ce: 0.2969\n",
      "Epoch: 1 [308/877] [1185/2631] lr:0.0004230 Loss: 0.5539 Loss_dice: 0.2605 Loss_ce: 0.2935\n",
      "Epoch: 1 [309/877] [1186/2631] lr:0.0004229 Loss: 0.6434 Loss_dice: 0.2859 Loss_ce: 0.3575\n",
      "Epoch: 1 [310/877] [1187/2631] lr:0.0004228 Loss: 0.5724 Loss_dice: 0.2693 Loss_ce: 0.3030\n",
      "Epoch: 1 [311/877] [1188/2631] lr:0.0004226 Loss: 0.5756 Loss_dice: 0.2712 Loss_ce: 0.3043\n",
      "Epoch: 1 [312/877] [1189/2631] lr:0.0004225 Loss: 0.5647 Loss_dice: 0.2582 Loss_ce: 0.3065\n",
      "Epoch: 1 [313/877] [1190/2631] lr:0.0004224 Loss: 0.6151 Loss_dice: 0.2797 Loss_ce: 0.3354\n",
      "Epoch: 1 [314/877] [1191/2631] lr:0.0004223 Loss: 0.6253 Loss_dice: 0.2792 Loss_ce: 0.3461\n",
      "Epoch: 1 [315/877] [1192/2631] lr:0.0004221 Loss: 0.5602 Loss_dice: 0.2491 Loss_ce: 0.3111\n",
      "Epoch: 1 [316/877] [1193/2631] lr:0.0004220 Loss: 0.6416 Loss_dice: 0.2825 Loss_ce: 0.3591\n",
      "Epoch: 1 [317/877] [1194/2631] lr:0.0004219 Loss: 0.5449 Loss_dice: 0.2634 Loss_ce: 0.2815\n",
      "Epoch: 1 [318/877] [1195/2631] lr:0.0004217 Loss: 0.5560 Loss_dice: 0.2639 Loss_ce: 0.2921\n",
      "Epoch: 1 [319/877] [1196/2631] lr:0.0004216 Loss: 0.5808 Loss_dice: 0.2704 Loss_ce: 0.3104\n",
      "Epoch: 1 [320/877] [1197/2631] lr:0.0004215 Loss: 0.6214 Loss_dice: 0.2848 Loss_ce: 0.3366\n",
      "Epoch: 1 [321/877] [1198/2631] lr:0.0004213 Loss: 0.6896 Loss_dice: 0.2894 Loss_ce: 0.4002\n",
      "Epoch: 1 [322/877] [1199/2631] lr:0.0004212 Loss: 0.5797 Loss_dice: 0.2817 Loss_ce: 0.2979\n",
      "Epoch: 1 [323/877] [1200/2631] lr:0.0004211 Loss: 0.6438 Loss_dice: 0.2318 Loss_ce: 0.4119\n",
      "Epoch: 1 [324/877] [1201/2631] lr:0.0004210 Loss: 0.5788 Loss_dice: 0.2733 Loss_ce: 0.3055\n",
      "Epoch: 1 [325/877] [1202/2631] lr:0.0004208 Loss: 0.5527 Loss_dice: 0.2755 Loss_ce: 0.2772\n",
      "Epoch: 1 [326/877] [1203/2631] lr:0.0004207 Loss: 0.5623 Loss_dice: 0.2692 Loss_ce: 0.2931\n",
      "Epoch: 1 [327/877] [1204/2631] lr:0.0004206 Loss: 0.5454 Loss_dice: 0.2706 Loss_ce: 0.2748\n",
      "Epoch: 1 [328/877] [1205/2631] lr:0.0004204 Loss: 0.5704 Loss_dice: 0.2780 Loss_ce: 0.2924\n",
      "Epoch: 1 [329/877] [1206/2631] lr:0.0004203 Loss: 0.5560 Loss_dice: 0.2393 Loss_ce: 0.3168\n",
      "Epoch: 1 [330/877] [1207/2631] lr:0.0004202 Loss: 0.5437 Loss_dice: 0.2671 Loss_ce: 0.2766\n",
      "Epoch: 1 [331/877] [1208/2631] lr:0.0004200 Loss: 0.5353 Loss_dice: 0.2705 Loss_ce: 0.2648\n",
      "Epoch: 1 [332/877] [1209/2631] lr:0.0004199 Loss: 0.6542 Loss_dice: 0.2934 Loss_ce: 0.3608\n",
      "Epoch: 1 [333/877] [1210/2631] lr:0.0004198 Loss: 0.5589 Loss_dice: 0.2461 Loss_ce: 0.3128\n",
      "Epoch: 1 [334/877] [1211/2631] lr:0.0004196 Loss: 0.6045 Loss_dice: 0.2843 Loss_ce: 0.3202\n",
      "Epoch: 1 [335/877] [1212/2631] lr:0.0004195 Loss: 0.7578 Loss_dice: 0.3199 Loss_ce: 0.4379\n",
      "Epoch: 1 [336/877] [1213/2631] lr:0.0004194 Loss: 0.6130 Loss_dice: 0.2807 Loss_ce: 0.3323\n",
      "Epoch: 1 [337/877] [1214/2631] lr:0.0004192 Loss: 0.5655 Loss_dice: 0.2747 Loss_ce: 0.2908\n",
      "Epoch: 1 [338/877] [1215/2631] lr:0.0004191 Loss: 0.5536 Loss_dice: 0.2674 Loss_ce: 0.2863\n",
      "Epoch: 1 [339/877] [1216/2631] lr:0.0004190 Loss: 0.6548 Loss_dice: 0.2886 Loss_ce: 0.3661\n",
      "Epoch: 1 [340/877] [1217/2631] lr:0.0004189 Loss: 0.5663 Loss_dice: 0.2490 Loss_ce: 0.3173\n",
      "Epoch: 1 [341/877] [1218/2631] lr:0.0004187 Loss: 0.5991 Loss_dice: 0.2820 Loss_ce: 0.3172\n",
      "Epoch: 1 [342/877] [1219/2631] lr:0.0004186 Loss: 0.6060 Loss_dice: 0.2761 Loss_ce: 0.3299\n",
      "Epoch: 1 [343/877] [1220/2631] lr:0.0004185 Loss: 0.6172 Loss_dice: 0.2888 Loss_ce: 0.3284\n",
      "Epoch: 1 [344/877] [1221/2631] lr:0.0004183 Loss: 0.5960 Loss_dice: 0.2812 Loss_ce: 0.3148\n",
      "Epoch: 1 [345/877] [1222/2631] lr:0.0004182 Loss: 0.5655 Loss_dice: 0.2566 Loss_ce: 0.3089\n",
      "Epoch: 1 [346/877] [1223/2631] lr:0.0004181 Loss: 0.5545 Loss_dice: 0.2553 Loss_ce: 0.2992\n",
      "Epoch: 1 [347/877] [1224/2631] lr:0.0004179 Loss: 0.5454 Loss_dice: 0.2529 Loss_ce: 0.2926\n",
      "Epoch: 1 [348/877] [1225/2631] lr:0.0004178 Loss: 0.7102 Loss_dice: 0.3047 Loss_ce: 0.4055\n",
      "Epoch: 1 [349/877] [1226/2631] lr:0.0004177 Loss: 0.5588 Loss_dice: 0.2715 Loss_ce: 0.2872\n",
      "Epoch: 1 [350/877] [1227/2631] lr:0.0004175 Loss: 0.5830 Loss_dice: 0.2783 Loss_ce: 0.3047\n",
      "Epoch: 1 [351/877] [1228/2631] lr:0.0004174 Loss: 0.5455 Loss_dice: 0.2503 Loss_ce: 0.2952\n",
      "Epoch: 1 [352/877] [1229/2631] lr:0.0004173 Loss: 0.6117 Loss_dice: 0.2852 Loss_ce: 0.3266\n",
      "Epoch: 1 [353/877] [1230/2631] lr:0.0004171 Loss: 0.9919 Loss_dice: 0.3667 Loss_ce: 0.6252\n",
      "Epoch: 1 [354/877] [1231/2631] lr:0.0004170 Loss: 0.5518 Loss_dice: 0.2650 Loss_ce: 0.2868\n",
      "Epoch: 1 [355/877] [1232/2631] lr:0.0004169 Loss: 1.0397 Loss_dice: 0.3838 Loss_ce: 0.6559\n",
      "Epoch: 1 [356/877] [1233/2631] lr:0.0004167 Loss: 0.6566 Loss_dice: 0.2875 Loss_ce: 0.3691\n",
      "Epoch: 1 [357/877] [1234/2631] lr:0.0004166 Loss: 0.5461 Loss_dice: 0.2520 Loss_ce: 0.2941\n",
      "Epoch: 1 [358/877] [1235/2631] lr:0.0004165 Loss: 0.5641 Loss_dice: 0.2626 Loss_ce: 0.3016\n",
      "Epoch: 1 [359/877] [1236/2631] lr:0.0004163 Loss: 0.5685 Loss_dice: 0.2586 Loss_ce: 0.3098\n",
      "Epoch: 1 [360/877] [1237/2631] lr:0.0004162 Loss: 0.5552 Loss_dice: 0.2509 Loss_ce: 0.3043\n",
      "Epoch: 1 [361/877] [1238/2631] lr:0.0004161 Loss: 0.6155 Loss_dice: 0.2660 Loss_ce: 0.3496\n",
      "Epoch: 1 [362/877] [1239/2631] lr:0.0004159 Loss: 0.5916 Loss_dice: 0.2590 Loss_ce: 0.3326\n",
      "Epoch: 1 [363/877] [1240/2631] lr:0.0004158 Loss: 0.6688 Loss_dice: 0.2269 Loss_ce: 0.4419\n",
      "Epoch: 1 [364/877] [1241/2631] lr:0.0004157 Loss: 0.5730 Loss_dice: 0.2393 Loss_ce: 0.3337\n",
      "Epoch: 1 [365/877] [1242/2631] lr:0.0004155 Loss: 0.5557 Loss_dice: 0.2404 Loss_ce: 0.3153\n",
      "Epoch: 1 [366/877] [1243/2631] lr:0.0004154 Loss: 0.5958 Loss_dice: 0.2689 Loss_ce: 0.3269\n",
      "Epoch: 1 [367/877] [1244/2631] lr:0.0004153 Loss: 0.6179 Loss_dice: 0.2475 Loss_ce: 0.3705\n",
      "Epoch: 1 [368/877] [1245/2631] lr:0.0004151 Loss: 0.5740 Loss_dice: 0.2551 Loss_ce: 0.3189\n",
      "Epoch: 1 [369/877] [1246/2631] lr:0.0004150 Loss: 0.6174 Loss_dice: 0.2432 Loss_ce: 0.3742\n",
      "Epoch: 1 [370/877] [1247/2631] lr:0.0004149 Loss: 0.5794 Loss_dice: 0.2587 Loss_ce: 0.3207\n",
      "Epoch: 1 [371/877] [1248/2631] lr:0.0004147 Loss: 0.5901 Loss_dice: 0.2454 Loss_ce: 0.3446\n",
      "Epoch: 1 [372/877] [1249/2631] lr:0.0004146 Loss: 0.5710 Loss_dice: 0.2479 Loss_ce: 0.3231\n",
      "Epoch: 1 [373/877] [1250/2631] lr:0.0004144 Loss: 0.5660 Loss_dice: 0.2570 Loss_ce: 0.3090\n",
      "Epoch: 1 [374/877] [1251/2631] lr:0.0004143 Loss: 0.5746 Loss_dice: 0.2603 Loss_ce: 0.3143\n",
      "Epoch: 1 [375/877] [1252/2631] lr:0.0004142 Loss: 0.5608 Loss_dice: 0.2285 Loss_ce: 0.3323\n",
      "Epoch: 1 [376/877] [1253/2631] lr:0.0004140 Loss: 0.5516 Loss_dice: 0.2375 Loss_ce: 0.3142\n",
      "Epoch: 1 [377/877] [1254/2631] lr:0.0004139 Loss: 0.8159 Loss_dice: 0.3169 Loss_ce: 0.4990\n",
      "Epoch: 1 [378/877] [1255/2631] lr:0.0004138 Loss: 0.6804 Loss_dice: 0.2562 Loss_ce: 0.4242\n",
      "Epoch: 1 [379/877] [1256/2631] lr:0.0004136 Loss: 0.7001 Loss_dice: 0.2914 Loss_ce: 0.4087\n",
      "Epoch: 1 [380/877] [1257/2631] lr:0.0004135 Loss: 0.5568 Loss_dice: 0.2426 Loss_ce: 0.3142\n",
      "Epoch: 1 [381/877] [1258/2631] lr:0.0004134 Loss: 0.5795 Loss_dice: 0.2429 Loss_ce: 0.3366\n",
      "Epoch: 1 [382/877] [1259/2631] lr:0.0004132 Loss: 0.5567 Loss_dice: 0.2367 Loss_ce: 0.3201\n",
      "Epoch: 1 [383/877] [1260/2631] lr:0.0004131 Loss: 0.5552 Loss_dice: 0.2535 Loss_ce: 0.3017\n",
      "Epoch: 1 [384/877] [1261/2631] lr:0.0004130 Loss: 0.6044 Loss_dice: 0.2826 Loss_ce: 0.3218\n",
      "Epoch: 1 [385/877] [1262/2631] lr:0.0004128 Loss: 0.7992 Loss_dice: 0.3189 Loss_ce: 0.4803\n",
      "Epoch: 1 [386/877] [1263/2631] lr:0.0004127 Loss: 1.1619 Loss_dice: 0.4309 Loss_ce: 0.7310\n",
      "Epoch: 1 [387/877] [1264/2631] lr:0.0004125 Loss: 0.7167 Loss_dice: 0.2459 Loss_ce: 0.4708\n",
      "Epoch: 1 [388/877] [1265/2631] lr:0.0004124 Loss: 0.7028 Loss_dice: 0.2871 Loss_ce: 0.4157\n",
      "Epoch: 1 [389/877] [1266/2631] lr:0.0004123 Loss: 0.6130 Loss_dice: 0.2470 Loss_ce: 0.3660\n",
      "Epoch: 1 [390/877] [1267/2631] lr:0.0004121 Loss: 0.5566 Loss_dice: 0.2269 Loss_ce: 0.3297\n",
      "Epoch: 1 [391/877] [1268/2631] lr:0.0004120 Loss: 0.5769 Loss_dice: 0.2517 Loss_ce: 0.3252\n",
      "Epoch: 1 [392/877] [1269/2631] lr:0.0004119 Loss: 0.5782 Loss_dice: 0.2514 Loss_ce: 0.3268\n",
      "Epoch: 1 [393/877] [1270/2631] lr:0.0004117 Loss: 0.5699 Loss_dice: 0.2238 Loss_ce: 0.3461\n",
      "Epoch: 1 [394/877] [1271/2631] lr:0.0004116 Loss: 0.6344 Loss_dice: 0.2804 Loss_ce: 0.3540\n",
      "Epoch: 1 [395/877] [1272/2631] lr:0.0004115 Loss: 0.7441 Loss_dice: 0.2997 Loss_ce: 0.4444\n",
      "Epoch: 1 [396/877] [1273/2631] lr:0.0004113 Loss: 0.5956 Loss_dice: 0.2728 Loss_ce: 0.3228\n",
      "Epoch: 1 [397/877] [1274/2631] lr:0.0004112 Loss: 0.8988 Loss_dice: 0.3414 Loss_ce: 0.5574\n",
      "Epoch: 1 [398/877] [1275/2631] lr:0.0004110 Loss: 0.5721 Loss_dice: 0.2701 Loss_ce: 0.3020\n",
      "Epoch: 1 [399/877] [1276/2631] lr:0.0004109 Loss: 0.7086 Loss_dice: 0.3065 Loss_ce: 0.4021\n",
      "Epoch: 1 [400/877] [1277/2631] lr:0.0004108 Loss: 0.5829 Loss_dice: 0.2735 Loss_ce: 0.3094\n",
      "Epoch: 1 [401/877] [1278/2631] lr:0.0004106 Loss: 0.5917 Loss_dice: 0.2734 Loss_ce: 0.3183\n",
      "Epoch: 1 [402/877] [1279/2631] lr:0.0004105 Loss: 0.5689 Loss_dice: 0.2748 Loss_ce: 0.2940\n",
      "Epoch: 1 [403/877] [1280/2631] lr:0.0004104 Loss: 0.7794 Loss_dice: 0.3155 Loss_ce: 0.4639\n",
      "Epoch: 1 [404/877] [1281/2631] lr:0.0004102 Loss: 0.6071 Loss_dice: 0.2506 Loss_ce: 0.3566\n",
      "Epoch: 1 [405/877] [1282/2631] lr:0.0004101 Loss: 0.6482 Loss_dice: 0.2890 Loss_ce: 0.3593\n",
      "Epoch: 1 [406/877] [1283/2631] lr:0.0004099 Loss: 0.6212 Loss_dice: 0.2793 Loss_ce: 0.3419\n",
      "Epoch: 1 [407/877] [1284/2631] lr:0.0004098 Loss: 0.6147 Loss_dice: 0.2777 Loss_ce: 0.3370\n",
      "Epoch: 1 [408/877] [1285/2631] lr:0.0004097 Loss: 0.5477 Loss_dice: 0.2688 Loss_ce: 0.2789\n",
      "Epoch: 1 [409/877] [1286/2631] lr:0.0004095 Loss: 0.5623 Loss_dice: 0.2555 Loss_ce: 0.3068\n",
      "Epoch: 1 [410/877] [1287/2631] lr:0.0004094 Loss: 0.5502 Loss_dice: 0.2541 Loss_ce: 0.2961\n",
      "Epoch: 1 [411/877] [1288/2631] lr:0.0004093 Loss: 0.5687 Loss_dice: 0.2713 Loss_ce: 0.2974\n",
      "Epoch: 1 [412/877] [1289/2631] lr:0.0004091 Loss: 0.6320 Loss_dice: 0.2857 Loss_ce: 0.3462\n",
      "Epoch: 1 [413/877] [1290/2631] lr:0.0004090 Loss: 0.8065 Loss_dice: 0.3269 Loss_ce: 0.4796\n",
      "Epoch: 1 [414/877] [1291/2631] lr:0.0004088 Loss: 0.5943 Loss_dice: 0.2764 Loss_ce: 0.3179\n",
      "Epoch: 1 [415/877] [1292/2631] lr:0.0004087 Loss: 0.5809 Loss_dice: 0.2236 Loss_ce: 0.3573\n",
      "Epoch: 1 [416/877] [1293/2631] lr:0.0004086 Loss: 0.5565 Loss_dice: 0.2439 Loss_ce: 0.3125\n",
      "Epoch: 1 [417/877] [1294/2631] lr:0.0004084 Loss: 0.5545 Loss_dice: 0.2609 Loss_ce: 0.2936\n",
      "Epoch: 1 [418/877] [1295/2631] lr:0.0004083 Loss: 0.5490 Loss_dice: 0.2272 Loss_ce: 0.3218\n",
      "Epoch: 1 [419/877] [1296/2631] lr:0.0004082 Loss: 0.5490 Loss_dice: 0.2326 Loss_ce: 0.3165\n",
      "Epoch: 1 [420/877] [1297/2631] lr:0.0004080 Loss: 0.5502 Loss_dice: 0.2512 Loss_ce: 0.2990\n",
      "Epoch: 1 [421/877] [1298/2631] lr:0.0004079 Loss: 0.5654 Loss_dice: 0.2595 Loss_ce: 0.3059\n",
      "Epoch: 1 [422/877] [1299/2631] lr:0.0004077 Loss: 0.6338 Loss_dice: 0.2799 Loss_ce: 0.3540\n",
      "Epoch: 1 [423/877] [1300/2631] lr:0.0004076 Loss: 0.5720 Loss_dice: 0.2681 Loss_ce: 0.3039\n",
      "Epoch: 1 [424/877] [1301/2631] lr:0.0004075 Loss: 0.5510 Loss_dice: 0.2621 Loss_ce: 0.2889\n",
      "Epoch: 1 [425/877] [1302/2631] lr:0.0004073 Loss: 0.6200 Loss_dice: 0.2707 Loss_ce: 0.3493\n",
      "Epoch: 1 [426/877] [1303/2631] lr:0.0004072 Loss: 0.5465 Loss_dice: 0.2483 Loss_ce: 0.2983\n",
      "Epoch: 1 [427/877] [1304/2631] lr:0.0004070 Loss: 0.5756 Loss_dice: 0.2689 Loss_ce: 0.3067\n",
      "Epoch: 1 [428/877] [1305/2631] lr:0.0004069 Loss: 0.5534 Loss_dice: 0.2672 Loss_ce: 0.2862\n",
      "Epoch: 1 [429/877] [1306/2631] lr:0.0004068 Loss: 0.5412 Loss_dice: 0.2606 Loss_ce: 0.2806\n",
      "Epoch: 1 [430/877] [1307/2631] lr:0.0004066 Loss: 0.5423 Loss_dice: 0.2564 Loss_ce: 0.2859\n",
      "Epoch: 1 [431/877] [1308/2631] lr:0.0004065 Loss: 0.5535 Loss_dice: 0.2742 Loss_ce: 0.2793\n",
      "Epoch: 1 [432/877] [1309/2631] lr:0.0004063 Loss: 0.5516 Loss_dice: 0.2719 Loss_ce: 0.2797\n",
      "Epoch: 1 [433/877] [1310/2631] lr:0.0004062 Loss: 0.7692 Loss_dice: 0.2952 Loss_ce: 0.4740\n",
      "Epoch: 1 [434/877] [1311/2631] lr:0.0004061 Loss: 0.5469 Loss_dice: 0.2710 Loss_ce: 0.2759\n",
      "Epoch: 1 [435/877] [1312/2631] lr:0.0004059 Loss: 0.6276 Loss_dice: 0.2643 Loss_ce: 0.3633\n",
      "Epoch: 1 [436/877] [1313/2631] lr:0.0004058 Loss: 0.5491 Loss_dice: 0.2667 Loss_ce: 0.2825\n",
      "Epoch: 1 [437/877] [1314/2631] lr:0.0004056 Loss: 0.5708 Loss_dice: 0.2744 Loss_ce: 0.2963\n",
      "Epoch: 1 [438/877] [1315/2631] lr:0.0004055 Loss: 0.5528 Loss_dice: 0.2556 Loss_ce: 0.2972\n",
      "Epoch: 1 [439/877] [1316/2631] lr:0.0004054 Loss: 0.6094 Loss_dice: 0.2832 Loss_ce: 0.3262\n",
      "Epoch: 1 [440/877] [1317/2631] lr:0.0004052 Loss: 0.5738 Loss_dice: 0.2428 Loss_ce: 0.3310\n",
      "Epoch: 1 [441/877] [1318/2631] lr:0.0004051 Loss: 0.7168 Loss_dice: 0.3096 Loss_ce: 0.4072\n",
      "Epoch: 1 [442/877] [1319/2631] lr:0.0004049 Loss: 0.5652 Loss_dice: 0.2538 Loss_ce: 0.3114\n",
      "Epoch: 1 [443/877] [1320/2631] lr:0.0004048 Loss: 0.5599 Loss_dice: 0.2675 Loss_ce: 0.2924\n",
      "Epoch: 1 [444/877] [1321/2631] lr:0.0004047 Loss: 0.6107 Loss_dice: 0.2810 Loss_ce: 0.3296\n",
      "Epoch: 1 [445/877] [1322/2631] lr:0.0004045 Loss: 0.6369 Loss_dice: 0.2742 Loss_ce: 0.3627\n",
      "Epoch: 1 [446/877] [1323/2631] lr:0.0004044 Loss: 0.7544 Loss_dice: 0.3144 Loss_ce: 0.4401\n",
      "Epoch: 1 [447/877] [1324/2631] lr:0.0004042 Loss: 0.5465 Loss_dice: 0.2679 Loss_ce: 0.2786\n",
      "Epoch: 1 [448/877] [1325/2631] lr:0.0004041 Loss: 0.5418 Loss_dice: 0.2469 Loss_ce: 0.2949\n",
      "Epoch: 1 [449/877] [1326/2631] lr:0.0004040 Loss: 0.5593 Loss_dice: 0.2716 Loss_ce: 0.2877\n",
      "Epoch: 1 [450/877] [1327/2631] lr:0.0004038 Loss: 0.5561 Loss_dice: 0.2691 Loss_ce: 0.2870\n",
      "Epoch: 1 [451/877] [1328/2631] lr:0.0004037 Loss: 0.5732 Loss_dice: 0.2748 Loss_ce: 0.2984\n",
      "Epoch: 1 [452/877] [1329/2631] lr:0.0004035 Loss: 0.5353 Loss_dice: 0.2486 Loss_ce: 0.2867\n",
      "Epoch: 1 [453/877] [1330/2631] lr:0.0004034 Loss: 0.5537 Loss_dice: 0.2311 Loss_ce: 0.3226\n",
      "Epoch: 1 [454/877] [1331/2631] lr:0.0004032 Loss: 0.6199 Loss_dice: 0.2898 Loss_ce: 0.3301\n",
      "Epoch: 1 [455/877] [1332/2631] lr:0.0004031 Loss: 0.8249 Loss_dice: 0.3399 Loss_ce: 0.4850\n",
      "Epoch: 1 [456/877] [1333/2631] lr:0.0004030 Loss: 0.5533 Loss_dice: 0.2712 Loss_ce: 0.2821\n",
      "Epoch: 1 [457/877] [1334/2631] lr:0.0004028 Loss: 0.5644 Loss_dice: 0.2780 Loss_ce: 0.2864\n",
      "Epoch: 1 [458/877] [1335/2631] lr:0.0004027 Loss: 0.5755 Loss_dice: 0.2783 Loss_ce: 0.2972\n",
      "Epoch: 1 [459/877] [1336/2631] lr:0.0004025 Loss: 0.5861 Loss_dice: 0.2885 Loss_ce: 0.2976\n",
      "Epoch: 1 [460/877] [1337/2631] lr:0.0004024 Loss: 0.5881 Loss_dice: 0.2863 Loss_ce: 0.3018\n",
      "Epoch: 1 [461/877] [1338/2631] lr:0.0004023 Loss: 0.5595 Loss_dice: 0.2795 Loss_ce: 0.2800\n",
      "Epoch: 1 [462/877] [1339/2631] lr:0.0004021 Loss: 0.5682 Loss_dice: 0.2686 Loss_ce: 0.2996\n",
      "Epoch: 1 [463/877] [1340/2631] lr:0.0004020 Loss: 0.6326 Loss_dice: 0.2928 Loss_ce: 0.3398\n",
      "Epoch: 1 [464/877] [1341/2631] lr:0.0004018 Loss: 0.6148 Loss_dice: 0.2893 Loss_ce: 0.3255\n",
      "Epoch: 1 [465/877] [1342/2631] lr:0.0004017 Loss: 0.5771 Loss_dice: 0.2585 Loss_ce: 0.3185\n",
      "Epoch: 1 [466/877] [1343/2631] lr:0.0004015 Loss: 0.5675 Loss_dice: 0.2673 Loss_ce: 0.3001\n",
      "Epoch: 1 [467/877] [1344/2631] lr:0.0004014 Loss: 0.6382 Loss_dice: 0.2755 Loss_ce: 0.3627\n",
      "Epoch: 1 [468/877] [1345/2631] lr:0.0004013 Loss: 0.5672 Loss_dice: 0.2712 Loss_ce: 0.2959\n",
      "Epoch: 1 [469/877] [1346/2631] lr:0.0004011 Loss: 0.5549 Loss_dice: 0.2333 Loss_ce: 0.3216\n",
      "Epoch: 1 [470/877] [1347/2631] lr:0.0004010 Loss: 0.6055 Loss_dice: 0.2837 Loss_ce: 0.3218\n",
      "Epoch: 1 [471/877] [1348/2631] lr:0.0004008 Loss: 0.5473 Loss_dice: 0.2582 Loss_ce: 0.2891\n",
      "Epoch: 1 [472/877] [1349/2631] lr:0.0004007 Loss: 0.5478 Loss_dice: 0.2691 Loss_ce: 0.2787\n",
      "Epoch: 1 [473/877] [1350/2631] lr:0.0004005 Loss: 0.5471 Loss_dice: 0.2708 Loss_ce: 0.2763\n",
      "Epoch: 1 [474/877] [1351/2631] lr:0.0004004 Loss: 0.5725 Loss_dice: 0.2773 Loss_ce: 0.2952\n",
      "Epoch: 1 [475/877] [1352/2631] lr:0.0004003 Loss: 0.5469 Loss_dice: 0.2541 Loss_ce: 0.2929\n",
      "Epoch: 1 [476/877] [1353/2631] lr:0.0004001 Loss: 0.6596 Loss_dice: 0.2844 Loss_ce: 0.3752\n",
      "Epoch: 1 [477/877] [1354/2631] lr:0.0004000 Loss: 0.5317 Loss_dice: 0.2505 Loss_ce: 0.2812\n",
      "Epoch: 1 [478/877] [1355/2631] lr:0.0003998 Loss: 0.5727 Loss_dice: 0.2758 Loss_ce: 0.2970\n",
      "Epoch: 1 [479/877] [1356/2631] lr:0.0003997 Loss: 0.6197 Loss_dice: 0.2972 Loss_ce: 0.3224\n",
      "Epoch: 1 [480/877] [1357/2631] lr:0.0003995 Loss: 0.5785 Loss_dice: 0.2765 Loss_ce: 0.3020\n",
      "Epoch: 1 [481/877] [1358/2631] lr:0.0003994 Loss: 0.5745 Loss_dice: 0.2756 Loss_ce: 0.2990\n",
      "Epoch: 1 [482/877] [1359/2631] lr:0.0003993 Loss: 0.6002 Loss_dice: 0.2852 Loss_ce: 0.3150\n",
      "Epoch: 1 [483/877] [1360/2631] lr:0.0003991 Loss: 0.6512 Loss_dice: 0.2711 Loss_ce: 0.3801\n",
      "Epoch: 1 [484/877] [1361/2631] lr:0.0003990 Loss: 0.6925 Loss_dice: 0.2612 Loss_ce: 0.4313\n",
      "Epoch: 1 [485/877] [1362/2631] lr:0.0003988 Loss: 0.5517 Loss_dice: 0.2665 Loss_ce: 0.2852\n",
      "Epoch: 1 [486/877] [1363/2631] lr:0.0003987 Loss: 0.6506 Loss_dice: 0.2946 Loss_ce: 0.3560\n",
      "Epoch: 1 [487/877] [1364/2631] lr:0.0003985 Loss: 0.5906 Loss_dice: 0.2809 Loss_ce: 0.3097\n",
      "Epoch: 1 [488/877] [1365/2631] lr:0.0003984 Loss: 0.5791 Loss_dice: 0.2760 Loss_ce: 0.3031\n",
      "Epoch: 1 [489/877] [1366/2631] lr:0.0003982 Loss: 0.5719 Loss_dice: 0.2647 Loss_ce: 0.3072\n",
      "Epoch: 1 [490/877] [1367/2631] lr:0.0003981 Loss: 0.5685 Loss_dice: 0.2773 Loss_ce: 0.2912\n",
      "Epoch: 1 [491/877] [1368/2631] lr:0.0003980 Loss: 0.6517 Loss_dice: 0.2926 Loss_ce: 0.3591\n",
      "Epoch: 1 [492/877] [1369/2631] lr:0.0003978 Loss: 0.5843 Loss_dice: 0.2239 Loss_ce: 0.3605\n",
      "Epoch: 1 [493/877] [1370/2631] lr:0.0003977 Loss: 0.5508 Loss_dice: 0.2648 Loss_ce: 0.2860\n",
      "Epoch: 1 [494/877] [1371/2631] lr:0.0003975 Loss: 0.5390 Loss_dice: 0.2614 Loss_ce: 0.2776\n",
      "Epoch: 1 [495/877] [1372/2631] lr:0.0003974 Loss: 0.9126 Loss_dice: 0.3258 Loss_ce: 0.5868\n",
      "Epoch: 1 [496/877] [1373/2631] lr:0.0003972 Loss: 0.5659 Loss_dice: 0.2702 Loss_ce: 0.2958\n",
      "Epoch: 1 [497/877] [1374/2631] lr:0.0003971 Loss: 0.5651 Loss_dice: 0.2678 Loss_ce: 0.2973\n",
      "Epoch: 1 [498/877] [1375/2631] lr:0.0003969 Loss: 0.5455 Loss_dice: 0.2570 Loss_ce: 0.2885\n",
      "Epoch: 1 [499/877] [1376/2631] lr:0.0003968 Loss: 0.5456 Loss_dice: 0.2607 Loss_ce: 0.2849\n",
      "Epoch: 1 [500/877] [1377/2631] lr:0.0003967 Loss: 0.5684 Loss_dice: 0.2687 Loss_ce: 0.2997\n",
      "Epoch: 1 [501/877] [1378/2631] lr:0.0003965 Loss: 0.5768 Loss_dice: 0.2716 Loss_ce: 0.3052\n",
      "Epoch: 1 [502/877] [1379/2631] lr:0.0003964 Loss: 0.8045 Loss_dice: 0.3334 Loss_ce: 0.4711\n",
      "Epoch: 1 [503/877] [1380/2631] lr:0.0003962 Loss: 0.5549 Loss_dice: 0.2638 Loss_ce: 0.2911\n",
      "Epoch: 1 [504/877] [1381/2631] lr:0.0003961 Loss: 0.5547 Loss_dice: 0.2707 Loss_ce: 0.2840\n",
      "Epoch: 1 [505/877] [1382/2631] lr:0.0003959 Loss: 0.5597 Loss_dice: 0.2771 Loss_ce: 0.2826\n",
      "Epoch: 1 [506/877] [1383/2631] lr:0.0003958 Loss: 0.6428 Loss_dice: 0.2793 Loss_ce: 0.3635\n",
      "Epoch: 1 [507/877] [1384/2631] lr:0.0003956 Loss: 0.5407 Loss_dice: 0.2665 Loss_ce: 0.2742\n",
      "Epoch: 1 [508/877] [1385/2631] lr:0.0003955 Loss: 0.5462 Loss_dice: 0.2485 Loss_ce: 0.2976\n",
      "Epoch: 1 [509/877] [1386/2631] lr:0.0003953 Loss: 0.5494 Loss_dice: 0.2736 Loss_ce: 0.2758\n",
      "Epoch: 1 [510/877] [1387/2631] lr:0.0003952 Loss: 0.5946 Loss_dice: 0.2788 Loss_ce: 0.3158\n",
      "Epoch: 1 [511/877] [1388/2631] lr:0.0003951 Loss: 0.5605 Loss_dice: 0.2693 Loss_ce: 0.2911\n",
      "Epoch: 1 [512/877] [1389/2631] lr:0.0003949 Loss: 0.5721 Loss_dice: 0.2737 Loss_ce: 0.2984\n",
      "Epoch: 1 [513/877] [1390/2631] lr:0.0003948 Loss: 0.5561 Loss_dice: 0.2340 Loss_ce: 0.3222\n",
      "Epoch: 1 [514/877] [1391/2631] lr:0.0003946 Loss: 0.5401 Loss_dice: 0.2686 Loss_ce: 0.2715\n",
      "Epoch: 1 [515/877] [1392/2631] lr:0.0003945 Loss: 0.5716 Loss_dice: 0.2762 Loss_ce: 0.2954\n",
      "Epoch: 1 [516/877] [1393/2631] lr:0.0003943 Loss: 0.5398 Loss_dice: 0.2505 Loss_ce: 0.2893\n",
      "Epoch: 1 [517/877] [1394/2631] lr:0.0003942 Loss: 0.5746 Loss_dice: 0.2824 Loss_ce: 0.2922\n",
      "Epoch: 1 [518/877] [1395/2631] lr:0.0003940 Loss: 0.6006 Loss_dice: 0.2979 Loss_ce: 0.3028\n",
      "Epoch: 1 [519/877] [1396/2631] lr:0.0003939 Loss: 0.5364 Loss_dice: 0.2633 Loss_ce: 0.2731\n",
      "Epoch: 1 [520/877] [1397/2631] lr:0.0003937 Loss: 0.6378 Loss_dice: 0.3003 Loss_ce: 0.3375\n",
      "Epoch: 1 [521/877] [1398/2631] lr:0.0003936 Loss: 0.5760 Loss_dice: 0.2835 Loss_ce: 0.2925\n",
      "Epoch: 1 [522/877] [1399/2631] lr:0.0003934 Loss: 0.5422 Loss_dice: 0.2792 Loss_ce: 0.2630\n",
      "Epoch: 1 [523/877] [1400/2631] lr:0.0003933 Loss: 0.5812 Loss_dice: 0.2825 Loss_ce: 0.2987\n",
      "Epoch: 1 [524/877] [1401/2631] lr:0.0003932 Loss: 1.5141 Loss_dice: 0.5216 Loss_ce: 0.9925\n",
      "Epoch: 1 [525/877] [1402/2631] lr:0.0003930 Loss: 0.5576 Loss_dice: 0.2457 Loss_ce: 0.3119\n",
      "Epoch: 1 [526/877] [1403/2631] lr:0.0003929 Loss: 0.5849 Loss_dice: 0.2465 Loss_ce: 0.3384\n",
      "Epoch: 1 [527/877] [1404/2631] lr:0.0003927 Loss: 0.6100 Loss_dice: 0.2786 Loss_ce: 0.3314\n",
      "Epoch: 1 [528/877] [1405/2631] lr:0.0003926 Loss: 0.6295 Loss_dice: 0.2825 Loss_ce: 0.3470\n",
      "Epoch: 1 [529/877] [1406/2631] lr:0.0003924 Loss: 0.5576 Loss_dice: 0.2626 Loss_ce: 0.2950\n",
      "Epoch: 1 [530/877] [1407/2631] lr:0.0003923 Loss: 0.5776 Loss_dice: 0.2635 Loss_ce: 0.3141\n",
      "Epoch: 1 [531/877] [1408/2631] lr:0.0003921 Loss: 0.5562 Loss_dice: 0.2767 Loss_ce: 0.2795\n",
      "Epoch: 1 [532/877] [1409/2631] lr:0.0003920 Loss: 0.5484 Loss_dice: 0.2390 Loss_ce: 0.3094\n",
      "Epoch: 1 [533/877] [1410/2631] lr:0.0003918 Loss: 0.5582 Loss_dice: 0.2738 Loss_ce: 0.2845\n",
      "Epoch: 1 [534/877] [1411/2631] lr:0.0003917 Loss: 0.5428 Loss_dice: 0.2600 Loss_ce: 0.2827\n",
      "Epoch: 1 [535/877] [1412/2631] lr:0.0003915 Loss: 0.5512 Loss_dice: 0.2629 Loss_ce: 0.2883\n",
      "Epoch: 1 [536/877] [1413/2631] lr:0.0003914 Loss: 0.5395 Loss_dice: 0.2576 Loss_ce: 0.2819\n",
      "Epoch: 1 [537/877] [1414/2631] lr:0.0003912 Loss: 0.5494 Loss_dice: 0.2639 Loss_ce: 0.2854\n",
      "Epoch: 1 [538/877] [1415/2631] lr:0.0003911 Loss: 0.5627 Loss_dice: 0.2272 Loss_ce: 0.3355\n",
      "Epoch: 1 [539/877] [1416/2631] lr:0.0003909 Loss: 0.5670 Loss_dice: 0.2595 Loss_ce: 0.3075\n",
      "Epoch: 1 [540/877] [1417/2631] lr:0.0003908 Loss: 0.5583 Loss_dice: 0.2618 Loss_ce: 0.2965\n",
      "Epoch: 1 [541/877] [1418/2631] lr:0.0003906 Loss: 0.8930 Loss_dice: 0.3407 Loss_ce: 0.5523\n",
      "Epoch: 1 [542/877] [1419/2631] lr:0.0003905 Loss: 0.6149 Loss_dice: 0.2793 Loss_ce: 0.3357\n",
      "Epoch: 1 [543/877] [1420/2631] lr:0.0003904 Loss: 0.5625 Loss_dice: 0.2435 Loss_ce: 0.3190\n",
      "Epoch: 1 [544/877] [1421/2631] lr:0.0003902 Loss: 0.5559 Loss_dice: 0.2342 Loss_ce: 0.3217\n",
      "Epoch: 1 [545/877] [1422/2631] lr:0.0003901 Loss: 0.6508 Loss_dice: 0.2696 Loss_ce: 0.3812\n",
      "Epoch: 1 [546/877] [1423/2631] lr:0.0003899 Loss: 0.5761 Loss_dice: 0.2094 Loss_ce: 0.3668\n",
      "Epoch: 1 [547/877] [1424/2631] lr:0.0003898 Loss: 0.5484 Loss_dice: 0.2289 Loss_ce: 0.3195\n",
      "Epoch: 1 [548/877] [1425/2631] lr:0.0003896 Loss: 0.5635 Loss_dice: 0.2573 Loss_ce: 0.3063\n",
      "Epoch: 1 [549/877] [1426/2631] lr:0.0003895 Loss: 0.5578 Loss_dice: 0.2558 Loss_ce: 0.3020\n",
      "Epoch: 1 [550/877] [1427/2631] lr:0.0003893 Loss: 0.5724 Loss_dice: 0.2679 Loss_ce: 0.3045\n",
      "Epoch: 1 [551/877] [1428/2631] lr:0.0003892 Loss: 0.6714 Loss_dice: 0.2826 Loss_ce: 0.3887\n",
      "Epoch: 1 [552/877] [1429/2631] lr:0.0003890 Loss: 0.6182 Loss_dice: 0.2832 Loss_ce: 0.3350\n",
      "Epoch: 1 [553/877] [1430/2631] lr:0.0003889 Loss: 0.6018 Loss_dice: 0.2739 Loss_ce: 0.3280\n",
      "Epoch: 1 [554/877] [1431/2631] lr:0.0003887 Loss: 0.5958 Loss_dice: 0.2741 Loss_ce: 0.3217\n",
      "Epoch: 1 [555/877] [1432/2631] lr:0.0003886 Loss: 0.5762 Loss_dice: 0.2758 Loss_ce: 0.3005\n",
      "Epoch: 1 [556/877] [1433/2631] lr:0.0003884 Loss: 0.7414 Loss_dice: 0.3164 Loss_ce: 0.4250\n",
      "Epoch: 1 [557/877] [1434/2631] lr:0.0003883 Loss: 0.6845 Loss_dice: 0.3009 Loss_ce: 0.3836\n",
      "Epoch: 1 [558/877] [1435/2631] lr:0.0003881 Loss: 0.5468 Loss_dice: 0.2670 Loss_ce: 0.2798\n",
      "Epoch: 1 [559/877] [1436/2631] lr:0.0003880 Loss: 0.5535 Loss_dice: 0.2701 Loss_ce: 0.2834\n",
      "Epoch: 1 [560/877] [1437/2631] lr:0.0003878 Loss: 0.5542 Loss_dice: 0.2586 Loss_ce: 0.2956\n",
      "Epoch: 1 [561/877] [1438/2631] lr:0.0003877 Loss: 0.5578 Loss_dice: 0.2572 Loss_ce: 0.3005\n",
      "Epoch: 1 [562/877] [1439/2631] lr:0.0003875 Loss: 0.5734 Loss_dice: 0.2783 Loss_ce: 0.2951\n",
      "Epoch: 1 [563/877] [1440/2631] lr:0.0003874 Loss: 0.5728 Loss_dice: 0.2778 Loss_ce: 0.2950\n",
      "Epoch: 1 [564/877] [1441/2631] lr:0.0003872 Loss: 0.7171 Loss_dice: 0.3025 Loss_ce: 0.4146\n",
      "Epoch: 1 [565/877] [1442/2631] lr:0.0003871 Loss: 0.5537 Loss_dice: 0.2650 Loss_ce: 0.2888\n",
      "Epoch: 1 [566/877] [1443/2631] lr:0.0003869 Loss: 0.5349 Loss_dice: 0.2704 Loss_ce: 0.2645\n",
      "Epoch: 1 [567/877] [1444/2631] lr:0.0003868 Loss: 0.6287 Loss_dice: 0.2519 Loss_ce: 0.3768\n",
      "Epoch: 1 [568/877] [1445/2631] lr:0.0003866 Loss: 0.5552 Loss_dice: 0.2775 Loss_ce: 0.2777\n",
      "Epoch: 1 [569/877] [1446/2631] lr:0.0003865 Loss: 0.5444 Loss_dice: 0.2444 Loss_ce: 0.2999\n",
      "Epoch: 1 [570/877] [1447/2631] lr:0.0003863 Loss: 0.5578 Loss_dice: 0.2814 Loss_ce: 0.2764\n",
      "Epoch: 1 [571/877] [1448/2631] lr:0.0003862 Loss: 0.6506 Loss_dice: 0.2986 Loss_ce: 0.3520\n",
      "Epoch: 1 [572/877] [1449/2631] lr:0.0003860 Loss: 0.5402 Loss_dice: 0.2491 Loss_ce: 0.2912\n",
      "Epoch: 1 [573/877] [1450/2631] lr:0.0003859 Loss: 0.5555 Loss_dice: 0.2816 Loss_ce: 0.2739\n",
      "Epoch: 1 [574/877] [1451/2631] lr:0.0003857 Loss: 0.5465 Loss_dice: 0.2618 Loss_ce: 0.2847\n",
      "Epoch: 1 [575/877] [1452/2631] lr:0.0003856 Loss: 0.6041 Loss_dice: 0.2662 Loss_ce: 0.3379\n",
      "Epoch: 1 [576/877] [1453/2631] lr:0.0003854 Loss: 0.5562 Loss_dice: 0.2630 Loss_ce: 0.2932\n",
      "Epoch: 1 [577/877] [1454/2631] lr:0.0003853 Loss: 0.5575 Loss_dice: 0.2735 Loss_ce: 0.2841\n",
      "Epoch: 1 [578/877] [1455/2631] lr:0.0003851 Loss: 0.5622 Loss_dice: 0.2777 Loss_ce: 0.2845\n",
      "Epoch: 1 [579/877] [1456/2631] lr:0.0003850 Loss: 0.5640 Loss_dice: 0.2149 Loss_ce: 0.3492\n",
      "Epoch: 1 [580/877] [1457/2631] lr:0.0003848 Loss: 0.6742 Loss_dice: 0.2903 Loss_ce: 0.3839\n",
      "Epoch: 1 [581/877] [1458/2631] lr:0.0003847 Loss: 0.5510 Loss_dice: 0.2735 Loss_ce: 0.2775\n",
      "Epoch: 1 [582/877] [1459/2631] lr:0.0003845 Loss: 0.5496 Loss_dice: 0.2694 Loss_ce: 0.2803\n",
      "Epoch: 1 [583/877] [1460/2631] lr:0.0003844 Loss: 0.5369 Loss_dice: 0.2544 Loss_ce: 0.2825\n",
      "Epoch: 1 [584/877] [1461/2631] lr:0.0003842 Loss: 0.5387 Loss_dice: 0.2741 Loss_ce: 0.2646\n",
      "Epoch: 1 [585/877] [1462/2631] lr:0.0003841 Loss: 0.5747 Loss_dice: 0.2571 Loss_ce: 0.3176\n",
      "Epoch: 1 [586/877] [1463/2631] lr:0.0003839 Loss: 0.5511 Loss_dice: 0.2458 Loss_ce: 0.3053\n",
      "Epoch: 1 [587/877] [1464/2631] lr:0.0003838 Loss: 0.5405 Loss_dice: 0.2413 Loss_ce: 0.2992\n",
      "Epoch: 1 [588/877] [1465/2631] lr:0.0003836 Loss: 0.5970 Loss_dice: 0.2917 Loss_ce: 0.3053\n",
      "Epoch: 1 [589/877] [1466/2631] lr:0.0003835 Loss: 0.5460 Loss_dice: 0.2814 Loss_ce: 0.2646\n",
      "Epoch: 1 [590/877] [1467/2631] lr:0.0003833 Loss: 0.5659 Loss_dice: 0.2724 Loss_ce: 0.2934\n",
      "Epoch: 1 [591/877] [1468/2631] lr:0.0003832 Loss: 0.5946 Loss_dice: 0.2853 Loss_ce: 0.3093\n",
      "Epoch: 1 [592/877] [1469/2631] lr:0.0003830 Loss: 0.5432 Loss_dice: 0.2362 Loss_ce: 0.3070\n",
      "Epoch: 1 [593/877] [1470/2631] lr:0.0003829 Loss: 0.6636 Loss_dice: 0.3031 Loss_ce: 0.3605\n",
      "Epoch: 1 [594/877] [1471/2631] lr:0.0003827 Loss: 0.7048 Loss_dice: 0.3000 Loss_ce: 0.4048\n",
      "Epoch: 1 [595/877] [1472/2631] lr:0.0003825 Loss: 0.5583 Loss_dice: 0.2808 Loss_ce: 0.2775\n",
      "Epoch: 1 [596/877] [1473/2631] lr:0.0003824 Loss: 0.5492 Loss_dice: 0.2819 Loss_ce: 0.2672\n",
      "Epoch: 1 [597/877] [1474/2631] lr:0.0003822 Loss: 0.5701 Loss_dice: 0.2833 Loss_ce: 0.2867\n",
      "Epoch: 1 [598/877] [1475/2631] lr:0.0003821 Loss: 0.5574 Loss_dice: 0.2887 Loss_ce: 0.2686\n",
      "Epoch: 1 [599/877] [1476/2631] lr:0.0003819 Loss: 0.5753 Loss_dice: 0.2890 Loss_ce: 0.2863\n",
      "Epoch: 1 [600/877] [1477/2631] lr:0.0003818 Loss: 0.5874 Loss_dice: 0.2410 Loss_ce: 0.3464\n",
      "Epoch: 1 [601/877] [1478/2631] lr:0.0003816 Loss: 0.5530 Loss_dice: 0.2906 Loss_ce: 0.2624\n",
      "Epoch: 1 [602/877] [1479/2631] lr:0.0003815 Loss: 0.7494 Loss_dice: 0.3247 Loss_ce: 0.4247\n",
      "Epoch: 1 [603/877] [1480/2631] lr:0.0003813 Loss: 0.5736 Loss_dice: 0.2863 Loss_ce: 0.2873\n",
      "Epoch: 1 [604/877] [1481/2631] lr:0.0003812 Loss: 0.5411 Loss_dice: 0.2779 Loss_ce: 0.2632\n",
      "Epoch: 1 [605/877] [1482/2631] lr:0.0003810 Loss: 0.5784 Loss_dice: 0.2817 Loss_ce: 0.2967\n",
      "Epoch: 1 [606/877] [1483/2631] lr:0.0003809 Loss: 0.5521 Loss_dice: 0.2741 Loss_ce: 0.2780\n",
      "Epoch: 1 [607/877] [1484/2631] lr:0.0003807 Loss: 0.5722 Loss_dice: 0.2957 Loss_ce: 0.2765\n",
      "Epoch: 1 [608/877] [1485/2631] lr:0.0003806 Loss: 0.5371 Loss_dice: 0.2352 Loss_ce: 0.3018\n",
      "Epoch: 1 [609/877] [1486/2631] lr:0.0003804 Loss: 0.5510 Loss_dice: 0.2777 Loss_ce: 0.2733\n",
      "Epoch: 1 [610/877] [1487/2631] lr:0.0003803 Loss: 0.5439 Loss_dice: 0.2636 Loss_ce: 0.2802\n",
      "Epoch: 1 [611/877] [1488/2631] lr:0.0003801 Loss: 0.5398 Loss_dice: 0.2627 Loss_ce: 0.2771\n",
      "Epoch: 1 [612/877] [1489/2631] lr:0.0003800 Loss: 0.5973 Loss_dice: 0.2836 Loss_ce: 0.3138\n",
      "Epoch: 1 [613/877] [1490/2631] lr:0.0003798 Loss: 0.5533 Loss_dice: 0.2798 Loss_ce: 0.2735\n",
      "Epoch: 1 [614/877] [1491/2631] lr:0.0003796 Loss: 0.6198 Loss_dice: 0.2307 Loss_ce: 0.3891\n",
      "Epoch: 1 [615/877] [1492/2631] lr:0.0003795 Loss: 0.5450 Loss_dice: 0.2695 Loss_ce: 0.2754\n",
      "Epoch: 1 [616/877] [1493/2631] lr:0.0003793 Loss: 0.5362 Loss_dice: 0.2624 Loss_ce: 0.2738\n",
      "Epoch: 1 [617/877] [1494/2631] lr:0.0003792 Loss: 0.5548 Loss_dice: 0.2672 Loss_ce: 0.2876\n",
      "Epoch: 1 [618/877] [1495/2631] lr:0.0003790 Loss: 0.6730 Loss_dice: 0.2852 Loss_ce: 0.3878\n",
      "Epoch: 1 [619/877] [1496/2631] lr:0.0003789 Loss: 0.5496 Loss_dice: 0.2469 Loss_ce: 0.3027\n",
      "Epoch: 1 [620/877] [1497/2631] lr:0.0003787 Loss: 0.5545 Loss_dice: 0.2739 Loss_ce: 0.2806\n",
      "Epoch: 1 [621/877] [1498/2631] lr:0.0003786 Loss: 0.5385 Loss_dice: 0.2472 Loss_ce: 0.2914\n",
      "Epoch: 1 [622/877] [1499/2631] lr:0.0003784 Loss: 1.1251 Loss_dice: 0.4219 Loss_ce: 0.7032\n",
      "Epoch: 1 [623/877] [1500/2631] lr:0.0003783 Loss: 0.6245 Loss_dice: 0.2927 Loss_ce: 0.3318\n",
      "Epoch: 1 [624/877] [1501/2631] lr:0.0003781 Loss: 0.5511 Loss_dice: 0.2658 Loss_ce: 0.2852\n",
      "Epoch: 1 [625/877] [1502/2631] lr:0.0003780 Loss: 0.5352 Loss_dice: 0.2414 Loss_ce: 0.2938\n",
      "Epoch: 1 [626/877] [1503/2631] lr:0.0003778 Loss: 0.5758 Loss_dice: 0.2447 Loss_ce: 0.3311\n",
      "Epoch: 1 [627/877] [1504/2631] lr:0.0003777 Loss: 0.5904 Loss_dice: 0.2736 Loss_ce: 0.3169\n",
      "Epoch: 1 [628/877] [1505/2631] lr:0.0003775 Loss: 0.5601 Loss_dice: 0.2770 Loss_ce: 0.2831\n",
      "Epoch: 1 [629/877] [1506/2631] lr:0.0003773 Loss: 0.5512 Loss_dice: 0.2418 Loss_ce: 0.3094\n",
      "Epoch: 1 [630/877] [1507/2631] lr:0.0003772 Loss: 0.5634 Loss_dice: 0.2772 Loss_ce: 0.2862\n",
      "Epoch: 1 [631/877] [1508/2631] lr:0.0003770 Loss: 0.6088 Loss_dice: 0.2866 Loss_ce: 0.3222\n",
      "Epoch: 1 [632/877] [1509/2631] lr:0.0003769 Loss: 0.5574 Loss_dice: 0.2788 Loss_ce: 0.2786\n",
      "Epoch: 1 [633/877] [1510/2631] lr:0.0003767 Loss: 0.5404 Loss_dice: 0.2511 Loss_ce: 0.2893\n",
      "Epoch: 1 [634/877] [1511/2631] lr:0.0003766 Loss: 0.5780 Loss_dice: 0.2513 Loss_ce: 0.3267\n",
      "Epoch: 1 [635/877] [1512/2631] lr:0.0003764 Loss: 0.5972 Loss_dice: 0.2879 Loss_ce: 0.3094\n",
      "Epoch: 1 [636/877] [1513/2631] lr:0.0003763 Loss: 0.5392 Loss_dice: 0.2741 Loss_ce: 0.2651\n",
      "Epoch: 1 [637/877] [1514/2631] lr:0.0003761 Loss: 0.5725 Loss_dice: 0.2625 Loss_ce: 0.3101\n",
      "Epoch: 1 [638/877] [1515/2631] lr:0.0003760 Loss: 0.5398 Loss_dice: 0.2561 Loss_ce: 0.2837\n",
      "Epoch: 1 [639/877] [1516/2631] lr:0.0003758 Loss: 0.5368 Loss_dice: 0.2791 Loss_ce: 0.2577\n",
      "Epoch: 1 [640/877] [1517/2631] lr:0.0003756 Loss: 0.5419 Loss_dice: 0.2742 Loss_ce: 0.2677\n",
      "Epoch: 1 [641/877] [1518/2631] lr:0.0003755 Loss: 0.6233 Loss_dice: 0.2862 Loss_ce: 0.3371\n",
      "Epoch: 1 [642/877] [1519/2631] lr:0.0003753 Loss: 0.5543 Loss_dice: 0.2662 Loss_ce: 0.2880\n",
      "Epoch: 1 [643/877] [1520/2631] lr:0.0003752 Loss: 0.5267 Loss_dice: 0.2691 Loss_ce: 0.2576\n",
      "Epoch: 1 [644/877] [1521/2631] lr:0.0003750 Loss: 0.5515 Loss_dice: 0.2791 Loss_ce: 0.2724\n",
      "Epoch: 1 [645/877] [1522/2631] lr:0.0003749 Loss: 0.5872 Loss_dice: 0.2640 Loss_ce: 0.3232\n",
      "Epoch: 1 [646/877] [1523/2631] lr:0.0003747 Loss: 0.5405 Loss_dice: 0.2361 Loss_ce: 0.3044\n",
      "Epoch: 1 [647/877] [1524/2631] lr:0.0003746 Loss: 0.7861 Loss_dice: 0.3367 Loss_ce: 0.4494\n",
      "Epoch: 1 [648/877] [1525/2631] lr:0.0003744 Loss: 0.5528 Loss_dice: 0.2454 Loss_ce: 0.3074\n",
      "Epoch: 1 [649/877] [1526/2631] lr:0.0003742 Loss: 0.5294 Loss_dice: 0.2657 Loss_ce: 0.2638\n",
      "Epoch: 1 [650/877] [1527/2631] lr:0.0003741 Loss: 0.5718 Loss_dice: 0.2845 Loss_ce: 0.2873\n",
      "Epoch: 1 [651/877] [1528/2631] lr:0.0003739 Loss: 0.5413 Loss_dice: 0.2747 Loss_ce: 0.2666\n",
      "Epoch: 1 [652/877] [1529/2631] lr:0.0003738 Loss: 0.8617 Loss_dice: 0.3321 Loss_ce: 0.5296\n",
      "Epoch: 1 [653/877] [1530/2631] lr:0.0003736 Loss: 0.6829 Loss_dice: 0.3110 Loss_ce: 0.3719\n",
      "Epoch: 1 [654/877] [1531/2631] lr:0.0003735 Loss: 0.5417 Loss_dice: 0.2415 Loss_ce: 0.3002\n",
      "Epoch: 1 [655/877] [1532/2631] lr:0.0003733 Loss: 0.5391 Loss_dice: 0.2728 Loss_ce: 0.2663\n",
      "Epoch: 1 [656/877] [1533/2631] lr:0.0003732 Loss: 0.6038 Loss_dice: 0.2946 Loss_ce: 0.3092\n",
      "Epoch: 1 [657/877] [1534/2631] lr:0.0003730 Loss: 0.5726 Loss_dice: 0.2791 Loss_ce: 0.2936\n",
      "Epoch: 1 [658/877] [1535/2631] lr:0.0003728 Loss: 0.6593 Loss_dice: 0.3114 Loss_ce: 0.3479\n",
      "Epoch: 1 [659/877] [1536/2631] lr:0.0003727 Loss: 0.5677 Loss_dice: 0.2910 Loss_ce: 0.2767\n",
      "Epoch: 1 [660/877] [1537/2631] lr:0.0003725 Loss: 0.6156 Loss_dice: 0.2563 Loss_ce: 0.3592\n",
      "Epoch: 1 [661/877] [1538/2631] lr:0.0003724 Loss: 0.5501 Loss_dice: 0.2478 Loss_ce: 0.3023\n",
      "Epoch: 1 [662/877] [1539/2631] lr:0.0003722 Loss: 0.5721 Loss_dice: 0.2272 Loss_ce: 0.3449\n",
      "Epoch: 1 [663/877] [1540/2631] lr:0.0003721 Loss: 0.5431 Loss_dice: 0.2721 Loss_ce: 0.2710\n",
      "Epoch: 1 [664/877] [1541/2631] lr:0.0003719 Loss: 0.5998 Loss_dice: 0.2912 Loss_ce: 0.3087\n",
      "Epoch: 1 [665/877] [1542/2631] lr:0.0003718 Loss: 0.6027 Loss_dice: 0.2901 Loss_ce: 0.3126\n",
      "Epoch: 1 [666/877] [1543/2631] lr:0.0003716 Loss: 0.5698 Loss_dice: 0.2125 Loss_ce: 0.3573\n",
      "Epoch: 1 [667/877] [1544/2631] lr:0.0003714 Loss: 0.7430 Loss_dice: 0.2921 Loss_ce: 0.4509\n",
      "Epoch: 1 [668/877] [1545/2631] lr:0.0003713 Loss: 0.5861 Loss_dice: 0.2785 Loss_ce: 0.3076\n",
      "Epoch: 1 [669/877] [1546/2631] lr:0.0003711 Loss: 0.5744 Loss_dice: 0.2729 Loss_ce: 0.3015\n",
      "Epoch: 1 [670/877] [1547/2631] lr:0.0003710 Loss: 0.6143 Loss_dice: 0.2789 Loss_ce: 0.3354\n",
      "Epoch: 1 [671/877] [1548/2631] lr:0.0003708 Loss: 0.7929 Loss_dice: 0.3305 Loss_ce: 0.4624\n",
      "Epoch: 1 [672/877] [1549/2631] lr:0.0003707 Loss: 0.5621 Loss_dice: 0.2582 Loss_ce: 0.3038\n",
      "Epoch: 1 [673/877] [1550/2631] lr:0.0003705 Loss: 0.5714 Loss_dice: 0.2427 Loss_ce: 0.3287\n",
      "Epoch: 1 [674/877] [1551/2631] lr:0.0003703 Loss: 0.5686 Loss_dice: 0.2792 Loss_ce: 0.2894\n",
      "Epoch: 1 [675/877] [1552/2631] lr:0.0003702 Loss: 0.6175 Loss_dice: 0.2471 Loss_ce: 0.3705\n",
      "Epoch: 1 [676/877] [1553/2631] lr:0.0003700 Loss: 0.5427 Loss_dice: 0.2674 Loss_ce: 0.2753\n",
      "Epoch: 1 [677/877] [1554/2631] lr:0.0003699 Loss: 0.5464 Loss_dice: 0.2736 Loss_ce: 0.2729\n",
      "Epoch: 1 [678/877] [1555/2631] lr:0.0003697 Loss: 0.5706 Loss_dice: 0.2419 Loss_ce: 0.3287\n",
      "Epoch: 1 [679/877] [1556/2631] lr:0.0003696 Loss: 0.5489 Loss_dice: 0.2653 Loss_ce: 0.2836\n",
      "Epoch: 1 [680/877] [1557/2631] lr:0.0003694 Loss: 0.5666 Loss_dice: 0.2757 Loss_ce: 0.2909\n",
      "Epoch: 1 [681/877] [1558/2631] lr:0.0003692 Loss: 0.7312 Loss_dice: 0.3160 Loss_ce: 0.4152\n",
      "Epoch: 1 [682/877] [1559/2631] lr:0.0003691 Loss: 0.5554 Loss_dice: 0.2793 Loss_ce: 0.2761\n",
      "Epoch: 1 [683/877] [1560/2631] lr:0.0003689 Loss: 0.5501 Loss_dice: 0.2688 Loss_ce: 0.2813\n",
      "Epoch: 1 [684/877] [1561/2631] lr:0.0003688 Loss: 0.5843 Loss_dice: 0.2652 Loss_ce: 0.3191\n",
      "Epoch: 1 [685/877] [1562/2631] lr:0.0003686 Loss: 0.5941 Loss_dice: 0.2864 Loss_ce: 0.3077\n",
      "Epoch: 1 [686/877] [1563/2631] lr:0.0003685 Loss: 0.6077 Loss_dice: 0.2903 Loss_ce: 0.3174\n",
      "Epoch: 1 [687/877] [1564/2631] lr:0.0003683 Loss: 0.6276 Loss_dice: 0.2871 Loss_ce: 0.3405\n",
      "Epoch: 1 [688/877] [1565/2631] lr:0.0003681 Loss: 0.5327 Loss_dice: 0.2682 Loss_ce: 0.2645\n",
      "Epoch: 1 [689/877] [1566/2631] lr:0.0003680 Loss: 0.5427 Loss_dice: 0.2702 Loss_ce: 0.2725\n",
      "Epoch: 1 [690/877] [1567/2631] lr:0.0003678 Loss: 0.5765 Loss_dice: 0.2823 Loss_ce: 0.2942\n",
      "Epoch: 1 [691/877] [1568/2631] lr:0.0003677 Loss: 0.5489 Loss_dice: 0.2622 Loss_ce: 0.2868\n",
      "Epoch: 1 [692/877] [1569/2631] lr:0.0003675 Loss: 0.5657 Loss_dice: 0.2870 Loss_ce: 0.2787\n",
      "Epoch: 1 [693/877] [1570/2631] lr:0.0003674 Loss: 0.5782 Loss_dice: 0.2978 Loss_ce: 0.2803\n",
      "Epoch: 1 [694/877] [1571/2631] lr:0.0003672 Loss: 0.5457 Loss_dice: 0.2840 Loss_ce: 0.2617\n",
      "Epoch: 1 [695/877] [1572/2631] lr:0.0003670 Loss: 0.5746 Loss_dice: 0.2920 Loss_ce: 0.2826\n",
      "Epoch: 1 [696/877] [1573/2631] lr:0.0003669 Loss: 0.6308 Loss_dice: 0.2950 Loss_ce: 0.3358\n",
      "Epoch: 1 [697/877] [1574/2631] lr:0.0003667 Loss: 0.5980 Loss_dice: 0.2763 Loss_ce: 0.3217\n",
      "Epoch: 1 [698/877] [1575/2631] lr:0.0003666 Loss: 0.5499 Loss_dice: 0.2853 Loss_ce: 0.2646\n",
      "Epoch: 1 [699/877] [1576/2631] lr:0.0003664 Loss: 0.5459 Loss_dice: 0.2649 Loss_ce: 0.2810\n",
      "Epoch: 1 [700/877] [1577/2631] lr:0.0003662 Loss: 0.6192 Loss_dice: 0.2960 Loss_ce: 0.3232\n",
      "Epoch: 1 [701/877] [1578/2631] lr:0.0003661 Loss: 0.5418 Loss_dice: 0.2851 Loss_ce: 0.2567\n",
      "Epoch: 1 [702/877] [1579/2631] lr:0.0003659 Loss: 0.6310 Loss_dice: 0.2727 Loss_ce: 0.3583\n",
      "Epoch: 1 [703/877] [1580/2631] lr:0.0003658 Loss: 0.5531 Loss_dice: 0.2834 Loss_ce: 0.2697\n",
      "Epoch: 1 [704/877] [1581/2631] lr:0.0003656 Loss: 0.6233 Loss_dice: 0.3061 Loss_ce: 0.3172\n",
      "Epoch: 1 [705/877] [1582/2631] lr:0.0003654 Loss: 0.5835 Loss_dice: 0.2885 Loss_ce: 0.2950\n",
      "Epoch: 1 [706/877] [1583/2631] lr:0.0003653 Loss: 0.5496 Loss_dice: 0.2557 Loss_ce: 0.2939\n",
      "Epoch: 1 [707/877] [1584/2631] lr:0.0003651 Loss: 0.5696 Loss_dice: 0.2796 Loss_ce: 0.2900\n",
      "Epoch: 1 [708/877] [1585/2631] lr:0.0003650 Loss: 0.5433 Loss_dice: 0.2633 Loss_ce: 0.2801\n",
      "Epoch: 1 [709/877] [1586/2631] lr:0.0003648 Loss: 0.6463 Loss_dice: 0.3025 Loss_ce: 0.3437\n",
      "Epoch: 1 [710/877] [1587/2631] lr:0.0003647 Loss: 0.5700 Loss_dice: 0.2805 Loss_ce: 0.2895\n",
      "Epoch: 1 [711/877] [1588/2631] lr:0.0003645 Loss: 0.5525 Loss_dice: 0.2759 Loss_ce: 0.2766\n",
      "Epoch: 1 [712/877] [1589/2631] lr:0.0003643 Loss: 0.5628 Loss_dice: 0.2577 Loss_ce: 0.3051\n",
      "Epoch: 1 [713/877] [1590/2631] lr:0.0003642 Loss: 0.5883 Loss_dice: 0.2806 Loss_ce: 0.3077\n",
      "Epoch: 1 [714/877] [1591/2631] lr:0.0003640 Loss: 0.6492 Loss_dice: 0.2875 Loss_ce: 0.3618\n",
      "Epoch: 1 [715/877] [1592/2631] lr:0.0003639 Loss: 0.5871 Loss_dice: 0.2840 Loss_ce: 0.3031\n",
      "Epoch: 1 [716/877] [1593/2631] lr:0.0003637 Loss: 0.5522 Loss_dice: 0.2216 Loss_ce: 0.3307\n",
      "Epoch: 1 [717/877] [1594/2631] lr:0.0003635 Loss: 0.5807 Loss_dice: 0.2712 Loss_ce: 0.3095\n",
      "Epoch: 1 [718/877] [1595/2631] lr:0.0003634 Loss: 0.5441 Loss_dice: 0.2754 Loss_ce: 0.2686\n",
      "Epoch: 1 [719/877] [1596/2631] lr:0.0003632 Loss: 0.5428 Loss_dice: 0.2788 Loss_ce: 0.2640\n",
      "Epoch: 1 [720/877] [1597/2631] lr:0.0003631 Loss: 0.5677 Loss_dice: 0.2324 Loss_ce: 0.3353\n",
      "Epoch: 1 [721/877] [1598/2631] lr:0.0003629 Loss: 0.5617 Loss_dice: 0.2844 Loss_ce: 0.2773\n",
      "Epoch: 1 [722/877] [1599/2631] lr:0.0003627 Loss: 0.5699 Loss_dice: 0.2819 Loss_ce: 0.2880\n",
      "Epoch: 1 [723/877] [1600/2631] lr:0.0003626 Loss: 0.5522 Loss_dice: 0.2833 Loss_ce: 0.2688\n",
      "Epoch: 1 [724/877] [1601/2631] lr:0.0003624 Loss: 0.5385 Loss_dice: 0.2458 Loss_ce: 0.2927\n",
      "Epoch: 1 [725/877] [1602/2631] lr:0.0003623 Loss: 0.6080 Loss_dice: 0.2946 Loss_ce: 0.3133\n",
      "Epoch: 1 [726/877] [1603/2631] lr:0.0003621 Loss: 0.5486 Loss_dice: 0.2795 Loss_ce: 0.2691\n",
      "Epoch: 1 [727/877] [1604/2631] lr:0.0003619 Loss: 0.7221 Loss_dice: 0.3161 Loss_ce: 0.4060\n",
      "Epoch: 1 [728/877] [1605/2631] lr:0.0003618 Loss: 0.5631 Loss_dice: 0.2472 Loss_ce: 0.3158\n",
      "Epoch: 1 [729/877] [1606/2631] lr:0.0003616 Loss: 0.5541 Loss_dice: 0.2591 Loss_ce: 0.2950\n",
      "Epoch: 1 [730/877] [1607/2631] lr:0.0003615 Loss: 0.5879 Loss_dice: 0.2842 Loss_ce: 0.3037\n",
      "Epoch: 1 [731/877] [1608/2631] lr:0.0003613 Loss: 0.5534 Loss_dice: 0.2866 Loss_ce: 0.2668\n",
      "Epoch: 1 [732/877] [1609/2631] lr:0.0003611 Loss: 0.6111 Loss_dice: 0.2991 Loss_ce: 0.3120\n",
      "Epoch: 1 [733/877] [1610/2631] lr:0.0003610 Loss: 0.6264 Loss_dice: 0.3079 Loss_ce: 0.3186\n",
      "Epoch: 1 [734/877] [1611/2631] lr:0.0003608 Loss: 0.5433 Loss_dice: 0.2178 Loss_ce: 0.3255\n",
      "Epoch: 1 [735/877] [1612/2631] lr:0.0003607 Loss: 0.6129 Loss_dice: 0.2834 Loss_ce: 0.3296\n",
      "Epoch: 1 [736/877] [1613/2631] lr:0.0003605 Loss: 0.6014 Loss_dice: 0.2614 Loss_ce: 0.3400\n",
      "Epoch: 1 [737/877] [1614/2631] lr:0.0003603 Loss: 0.5581 Loss_dice: 0.2645 Loss_ce: 0.2936\n",
      "Epoch: 1 [738/877] [1615/2631] lr:0.0003602 Loss: 0.5328 Loss_dice: 0.2295 Loss_ce: 0.3033\n",
      "Epoch: 1 [739/877] [1616/2631] lr:0.0003600 Loss: 0.9348 Loss_dice: 0.3790 Loss_ce: 0.5558\n",
      "Epoch: 1 [740/877] [1617/2631] lr:0.0003599 Loss: 0.5536 Loss_dice: 0.2827 Loss_ce: 0.2710\n",
      "Epoch: 1 [741/877] [1618/2631] lr:0.0003597 Loss: 0.5815 Loss_dice: 0.2875 Loss_ce: 0.2940\n",
      "Epoch: 1 [742/877] [1619/2631] lr:0.0003595 Loss: 0.5426 Loss_dice: 0.2649 Loss_ce: 0.2777\n",
      "Epoch: 1 [743/877] [1620/2631] lr:0.0003594 Loss: 0.5498 Loss_dice: 0.2830 Loss_ce: 0.2668\n",
      "Epoch: 1 [744/877] [1621/2631] lr:0.0003592 Loss: 0.6823 Loss_dice: 0.3025 Loss_ce: 0.3798\n",
      "Epoch: 1 [745/877] [1622/2631] lr:0.0003590 Loss: 0.5383 Loss_dice: 0.2747 Loss_ce: 0.2636\n",
      "Epoch: 1 [746/877] [1623/2631] lr:0.0003589 Loss: 0.5362 Loss_dice: 0.2554 Loss_ce: 0.2808\n",
      "Epoch: 1 [747/877] [1624/2631] lr:0.0003587 Loss: 0.5596 Loss_dice: 0.2648 Loss_ce: 0.2948\n",
      "Epoch: 1 [748/877] [1625/2631] lr:0.0003586 Loss: 0.5399 Loss_dice: 0.2598 Loss_ce: 0.2801\n",
      "Epoch: 1 [749/877] [1626/2631] lr:0.0003584 Loss: 0.5363 Loss_dice: 0.2447 Loss_ce: 0.2916\n",
      "Epoch: 1 [750/877] [1627/2631] lr:0.0003582 Loss: 0.5320 Loss_dice: 0.2530 Loss_ce: 0.2790\n",
      "Epoch: 1 [751/877] [1628/2631] lr:0.0003581 Loss: 0.5460 Loss_dice: 0.2401 Loss_ce: 0.3060\n",
      "Epoch: 1 [752/877] [1629/2631] lr:0.0003579 Loss: 0.5362 Loss_dice: 0.2687 Loss_ce: 0.2675\n",
      "Epoch: 1 [753/877] [1630/2631] lr:0.0003578 Loss: 0.5470 Loss_dice: 0.2605 Loss_ce: 0.2866\n",
      "Epoch: 1 [754/877] [1631/2631] lr:0.0003576 Loss: 0.5538 Loss_dice: 0.2775 Loss_ce: 0.2763\n",
      "Epoch: 1 [755/877] [1632/2631] lr:0.0003574 Loss: 0.5348 Loss_dice: 0.2590 Loss_ce: 0.2758\n",
      "Epoch: 1 [756/877] [1633/2631] lr:0.0003573 Loss: 0.5566 Loss_dice: 0.2678 Loss_ce: 0.2888\n",
      "Epoch: 1 [757/877] [1634/2631] lr:0.0003571 Loss: 0.5438 Loss_dice: 0.2851 Loss_ce: 0.2587\n",
      "Epoch: 1 [758/877] [1635/2631] lr:0.0003569 Loss: 0.5633 Loss_dice: 0.2925 Loss_ce: 0.2708\n",
      "Epoch: 1 [759/877] [1636/2631] lr:0.0003568 Loss: 0.5268 Loss_dice: 0.2547 Loss_ce: 0.2721\n",
      "Epoch: 1 [760/877] [1637/2631] lr:0.0003566 Loss: 0.5384 Loss_dice: 0.2740 Loss_ce: 0.2645\n",
      "Epoch: 1 [761/877] [1638/2631] lr:0.0003565 Loss: 0.5592 Loss_dice: 0.2905 Loss_ce: 0.2687\n",
      "Epoch: 1 [762/877] [1639/2631] lr:0.0003563 Loss: 0.6190 Loss_dice: 0.3114 Loss_ce: 0.3076\n",
      "Epoch: 1 [763/877] [1640/2631] lr:0.0003561 Loss: 0.5386 Loss_dice: 0.2678 Loss_ce: 0.2708\n",
      "Epoch: 1 [764/877] [1641/2631] lr:0.0003560 Loss: 0.5668 Loss_dice: 0.2950 Loss_ce: 0.2717\n",
      "Epoch: 1 [765/877] [1642/2631] lr:0.0003558 Loss: 0.5589 Loss_dice: 0.2912 Loss_ce: 0.2677\n",
      "Epoch: 1 [766/877] [1643/2631] lr:0.0003557 Loss: 0.7527 Loss_dice: 0.3385 Loss_ce: 0.4142\n",
      "Epoch: 1 [767/877] [1644/2631] lr:0.0003555 Loss: 0.5457 Loss_dice: 0.2751 Loss_ce: 0.2705\n",
      "Epoch: 1 [768/877] [1645/2631] lr:0.0003553 Loss: 0.5858 Loss_dice: 0.3053 Loss_ce: 0.2805\n",
      "Epoch: 1 [769/877] [1646/2631] lr:0.0003552 Loss: 0.5916 Loss_dice: 0.2963 Loss_ce: 0.2953\n",
      "Epoch: 1 [770/877] [1647/2631] lr:0.0003550 Loss: 0.5616 Loss_dice: 0.2875 Loss_ce: 0.2741\n",
      "Epoch: 1 [771/877] [1648/2631] lr:0.0003548 Loss: 0.5755 Loss_dice: 0.2946 Loss_ce: 0.2809\n",
      "Epoch: 1 [772/877] [1649/2631] lr:0.0003547 Loss: 0.5707 Loss_dice: 0.2468 Loss_ce: 0.3239\n",
      "Epoch: 1 [773/877] [1650/2631] lr:0.0003545 Loss: 0.6414 Loss_dice: 0.3033 Loss_ce: 0.3381\n",
      "Epoch: 1 [774/877] [1651/2631] lr:0.0003543 Loss: 0.5939 Loss_dice: 0.2820 Loss_ce: 0.3119\n",
      "Epoch: 1 [775/877] [1652/2631] lr:0.0003542 Loss: 0.6239 Loss_dice: 0.2947 Loss_ce: 0.3292\n",
      "Epoch: 1 [776/877] [1653/2631] lr:0.0003540 Loss: 0.5329 Loss_dice: 0.2738 Loss_ce: 0.2592\n",
      "Epoch: 1 [777/877] [1654/2631] lr:0.0003539 Loss: 0.5918 Loss_dice: 0.2754 Loss_ce: 0.3164\n",
      "Epoch: 1 [778/877] [1655/2631] lr:0.0003537 Loss: 0.5446 Loss_dice: 0.2777 Loss_ce: 0.2669\n",
      "Epoch: 1 [779/877] [1656/2631] lr:0.0003535 Loss: 0.5950 Loss_dice: 0.2341 Loss_ce: 0.3610\n",
      "Epoch: 1 [780/877] [1657/2631] lr:0.0003534 Loss: 0.5976 Loss_dice: 0.2526 Loss_ce: 0.3450\n",
      "Epoch: 1 [781/877] [1658/2631] lr:0.0003532 Loss: 0.5515 Loss_dice: 0.2500 Loss_ce: 0.3015\n",
      "Epoch: 1 [782/877] [1659/2631] lr:0.0003530 Loss: 0.5641 Loss_dice: 0.2865 Loss_ce: 0.2776\n",
      "Epoch: 1 [783/877] [1660/2631] lr:0.0003529 Loss: 0.5601 Loss_dice: 0.2872 Loss_ce: 0.2729\n",
      "Epoch: 1 [784/877] [1661/2631] lr:0.0003527 Loss: 0.6010 Loss_dice: 0.2934 Loss_ce: 0.3076\n",
      "Epoch: 1 [785/877] [1662/2631] lr:0.0003526 Loss: 0.5515 Loss_dice: 0.2856 Loss_ce: 0.2659\n",
      "Epoch: 1 [786/877] [1663/2631] lr:0.0003524 Loss: 0.5335 Loss_dice: 0.2667 Loss_ce: 0.2668\n",
      "Epoch: 1 [787/877] [1664/2631] lr:0.0003522 Loss: 0.5964 Loss_dice: 0.2987 Loss_ce: 0.2977\n",
      "Epoch: 1 [788/877] [1665/2631] lr:0.0003521 Loss: 0.5486 Loss_dice: 0.2859 Loss_ce: 0.2627\n",
      "Epoch: 1 [789/877] [1666/2631] lr:0.0003519 Loss: 0.6165 Loss_dice: 0.3043 Loss_ce: 0.3122\n",
      "Epoch: 1 [790/877] [1667/2631] lr:0.0003517 Loss: 0.5371 Loss_dice: 0.2773 Loss_ce: 0.2599\n",
      "Epoch: 1 [791/877] [1668/2631] lr:0.0003516 Loss: 0.5498 Loss_dice: 0.2865 Loss_ce: 0.2634\n",
      "Epoch: 1 [792/877] [1669/2631] lr:0.0003514 Loss: 0.5501 Loss_dice: 0.2799 Loss_ce: 0.2702\n",
      "Epoch: 1 [793/877] [1670/2631] lr:0.0003512 Loss: 0.5426 Loss_dice: 0.2786 Loss_ce: 0.2641\n",
      "Epoch: 1 [794/877] [1671/2631] lr:0.0003511 Loss: 0.5650 Loss_dice: 0.2917 Loss_ce: 0.2733\n",
      "Epoch: 1 [795/877] [1672/2631] lr:0.0003509 Loss: 0.5504 Loss_dice: 0.2728 Loss_ce: 0.2776\n",
      "Epoch: 1 [796/877] [1673/2631] lr:0.0003508 Loss: 0.5593 Loss_dice: 0.2338 Loss_ce: 0.3255\n",
      "Epoch: 1 [797/877] [1674/2631] lr:0.0003506 Loss: 0.5582 Loss_dice: 0.2870 Loss_ce: 0.2712\n",
      "Epoch: 1 [798/877] [1675/2631] lr:0.0003504 Loss: 0.5441 Loss_dice: 0.2759 Loss_ce: 0.2682\n",
      "Epoch: 1 [799/877] [1676/2631] lr:0.0003503 Loss: 0.5627 Loss_dice: 0.2884 Loss_ce: 0.2743\n",
      "Epoch: 1 [800/877] [1677/2631] lr:0.0003501 Loss: 0.5516 Loss_dice: 0.2782 Loss_ce: 0.2735\n",
      "Epoch: 1 [801/877] [1678/2631] lr:0.0003499 Loss: 0.5588 Loss_dice: 0.2926 Loss_ce: 0.2662\n",
      "Epoch: 1 [802/877] [1679/2631] lr:0.0003498 Loss: 0.5321 Loss_dice: 0.2742 Loss_ce: 0.2579\n",
      "Epoch: 1 [803/877] [1680/2631] lr:0.0003496 Loss: 0.5349 Loss_dice: 0.2415 Loss_ce: 0.2934\n",
      "Epoch: 1 [804/877] [1681/2631] lr:0.0003494 Loss: 0.5518 Loss_dice: 0.2819 Loss_ce: 0.2700\n",
      "Epoch: 1 [805/877] [1682/2631] lr:0.0003493 Loss: 0.5457 Loss_dice: 0.2645 Loss_ce: 0.2811\n",
      "Epoch: 1 [806/877] [1683/2631] lr:0.0003491 Loss: 0.5390 Loss_dice: 0.2802 Loss_ce: 0.2587\n",
      "Epoch: 1 [807/877] [1684/2631] lr:0.0003489 Loss: 0.5419 Loss_dice: 0.2807 Loss_ce: 0.2611\n",
      "Epoch: 1 [808/877] [1685/2631] lr:0.0003488 Loss: 0.5307 Loss_dice: 0.2795 Loss_ce: 0.2512\n",
      "Epoch: 1 [809/877] [1686/2631] lr:0.0003486 Loss: 0.5349 Loss_dice: 0.2676 Loss_ce: 0.2674\n",
      "Epoch: 1 [810/877] [1687/2631] lr:0.0003485 Loss: 0.5570 Loss_dice: 0.2688 Loss_ce: 0.2882\n",
      "Epoch: 1 [811/877] [1688/2631] lr:0.0003483 Loss: 0.6520 Loss_dice: 0.3116 Loss_ce: 0.3404\n",
      "Epoch: 1 [812/877] [1689/2631] lr:0.0003481 Loss: 0.5363 Loss_dice: 0.2846 Loss_ce: 0.2517\n",
      "Epoch: 1 [813/877] [1690/2631] lr:0.0003480 Loss: 0.5326 Loss_dice: 0.2489 Loss_ce: 0.2836\n",
      "Epoch: 1 [814/877] [1691/2631] lr:0.0003478 Loss: 0.5584 Loss_dice: 0.2883 Loss_ce: 0.2700\n",
      "Epoch: 1 [815/877] [1692/2631] lr:0.0003476 Loss: 0.5791 Loss_dice: 0.2628 Loss_ce: 0.3163\n",
      "Epoch: 1 [816/877] [1693/2631] lr:0.0003475 Loss: 0.5556 Loss_dice: 0.2776 Loss_ce: 0.2781\n",
      "Epoch: 1 [817/877] [1694/2631] lr:0.0003473 Loss: 0.5454 Loss_dice: 0.2834 Loss_ce: 0.2620\n",
      "Epoch: 1 [818/877] [1695/2631] lr:0.0003471 Loss: 0.5443 Loss_dice: 0.2947 Loss_ce: 0.2497\n",
      "Epoch: 1 [819/877] [1696/2631] lr:0.0003470 Loss: 0.5367 Loss_dice: 0.2718 Loss_ce: 0.2650\n",
      "Epoch: 1 [820/877] [1697/2631] lr:0.0003468 Loss: 0.5805 Loss_dice: 0.2815 Loss_ce: 0.2990\n",
      "Epoch: 1 [821/877] [1698/2631] lr:0.0003466 Loss: 0.6544 Loss_dice: 0.3103 Loss_ce: 0.3442\n",
      "Epoch: 1 [822/877] [1699/2631] lr:0.0003465 Loss: 0.6065 Loss_dice: 0.2970 Loss_ce: 0.3095\n",
      "Epoch: 1 [823/877] [1700/2631] lr:0.0003463 Loss: 0.5818 Loss_dice: 0.2479 Loss_ce: 0.3339\n",
      "Epoch: 1 [824/877] [1701/2631] lr:0.0003461 Loss: 0.5440 Loss_dice: 0.2640 Loss_ce: 0.2800\n",
      "Epoch: 1 [825/877] [1702/2631] lr:0.0003460 Loss: 0.5435 Loss_dice: 0.2865 Loss_ce: 0.2570\n",
      "Epoch: 1 [826/877] [1703/2631] lr:0.0003458 Loss: 0.5398 Loss_dice: 0.2532 Loss_ce: 0.2867\n",
      "Epoch: 1 [827/877] [1704/2631] lr:0.0003457 Loss: 0.5706 Loss_dice: 0.2855 Loss_ce: 0.2850\n",
      "Epoch: 1 [828/877] [1705/2631] lr:0.0003455 Loss: 0.5993 Loss_dice: 0.2922 Loss_ce: 0.3072\n",
      "Epoch: 1 [829/877] [1706/2631] lr:0.0003453 Loss: 0.6720 Loss_dice: 0.3011 Loss_ce: 0.3709\n",
      "Epoch: 1 [830/877] [1707/2631] lr:0.0003452 Loss: 0.5765 Loss_dice: 0.2294 Loss_ce: 0.3471\n",
      "Epoch: 1 [831/877] [1708/2631] lr:0.0003450 Loss: 0.5407 Loss_dice: 0.2422 Loss_ce: 0.2985\n",
      "Epoch: 1 [832/877] [1709/2631] lr:0.0003448 Loss: 0.5408 Loss_dice: 0.2621 Loss_ce: 0.2787\n",
      "Epoch: 1 [833/877] [1710/2631] lr:0.0003447 Loss: 0.6078 Loss_dice: 0.2844 Loss_ce: 0.3234\n",
      "Epoch: 1 [834/877] [1711/2631] lr:0.0003445 Loss: 1.0971 Loss_dice: 0.3885 Loss_ce: 0.7086\n",
      "Epoch: 1 [835/877] [1712/2631] lr:0.0003443 Loss: 0.5505 Loss_dice: 0.2777 Loss_ce: 0.2728\n",
      "Epoch: 1 [836/877] [1713/2631] lr:0.0003442 Loss: 0.5722 Loss_dice: 0.2579 Loss_ce: 0.3144\n",
      "Epoch: 1 [837/877] [1714/2631] lr:0.0003440 Loss: 0.5476 Loss_dice: 0.2637 Loss_ce: 0.2840\n",
      "Epoch: 1 [838/877] [1715/2631] lr:0.0003438 Loss: 0.5468 Loss_dice: 0.2583 Loss_ce: 0.2885\n",
      "Epoch: 1 [839/877] [1716/2631] lr:0.0003437 Loss: 0.5420 Loss_dice: 0.2771 Loss_ce: 0.2649\n",
      "Epoch: 1 [840/877] [1717/2631] lr:0.0003435 Loss: 0.5618 Loss_dice: 0.2880 Loss_ce: 0.2738\n",
      "Epoch: 1 [841/877] [1718/2631] lr:0.0003433 Loss: 0.5560 Loss_dice: 0.2881 Loss_ce: 0.2678\n",
      "Epoch: 1 [842/877] [1719/2631] lr:0.0003432 Loss: 0.5549 Loss_dice: 0.2886 Loss_ce: 0.2663\n",
      "Epoch: 1 [843/877] [1720/2631] lr:0.0003430 Loss: 0.5358 Loss_dice: 0.2609 Loss_ce: 0.2749\n",
      "Epoch: 1 [844/877] [1721/2631] lr:0.0003428 Loss: 0.5693 Loss_dice: 0.2955 Loss_ce: 0.2738\n",
      "Epoch: 1 [845/877] [1722/2631] lr:0.0003427 Loss: 0.5970 Loss_dice: 0.3052 Loss_ce: 0.2918\n",
      "Epoch: 1 [846/877] [1723/2631] lr:0.0003425 Loss: 0.5564 Loss_dice: 0.2963 Loss_ce: 0.2602\n",
      "Epoch: 1 [847/877] [1724/2631] lr:0.0003423 Loss: 0.5461 Loss_dice: 0.2927 Loss_ce: 0.2535\n",
      "Epoch: 1 [848/877] [1725/2631] lr:0.0003422 Loss: 0.5356 Loss_dice: 0.2788 Loss_ce: 0.2568\n",
      "Epoch: 1 [849/877] [1726/2631] lr:0.0003420 Loss: 0.5560 Loss_dice: 0.2190 Loss_ce: 0.3370\n",
      "Epoch: 1 [850/877] [1727/2631] lr:0.0003418 Loss: 0.5548 Loss_dice: 0.2946 Loss_ce: 0.2603\n",
      "Epoch: 1 [851/877] [1728/2631] lr:0.0003417 Loss: 0.6080 Loss_dice: 0.3037 Loss_ce: 0.3042\n",
      "Epoch: 1 [852/877] [1729/2631] lr:0.0003415 Loss: 0.5286 Loss_dice: 0.2844 Loss_ce: 0.2441\n",
      "Epoch: 1 [853/877] [1730/2631] lr:0.0003413 Loss: 0.5915 Loss_dice: 0.2413 Loss_ce: 0.3503\n",
      "Epoch: 1 [854/877] [1731/2631] lr:0.0003412 Loss: 0.6158 Loss_dice: 0.3066 Loss_ce: 0.3092\n",
      "Epoch: 1 [855/877] [1732/2631] lr:0.0003410 Loss: 0.5353 Loss_dice: 0.2523 Loss_ce: 0.2830\n",
      "Epoch: 1 [856/877] [1733/2631] lr:0.0003408 Loss: 0.5258 Loss_dice: 0.2740 Loss_ce: 0.2517\n",
      "Epoch: 1 [857/877] [1734/2631] lr:0.0003407 Loss: 0.5657 Loss_dice: 0.2890 Loss_ce: 0.2767\n",
      "Epoch: 1 [858/877] [1735/2631] lr:0.0003405 Loss: 0.5432 Loss_dice: 0.2850 Loss_ce: 0.2582\n",
      "Epoch: 1 [859/877] [1736/2631] lr:0.0003403 Loss: 0.5460 Loss_dice: 0.2748 Loss_ce: 0.2712\n",
      "Epoch: 1 [860/877] [1737/2631] lr:0.0003402 Loss: 0.5370 Loss_dice: 0.2625 Loss_ce: 0.2745\n",
      "Epoch: 1 [861/877] [1738/2631] lr:0.0003400 Loss: 0.6521 Loss_dice: 0.2381 Loss_ce: 0.4140\n",
      "Epoch: 1 [862/877] [1739/2631] lr:0.0003398 Loss: 0.5630 Loss_dice: 0.2800 Loss_ce: 0.2830\n",
      "Epoch: 1 [863/877] [1740/2631] lr:0.0003397 Loss: 0.5383 Loss_dice: 0.2563 Loss_ce: 0.2820\n",
      "Epoch: 1 [864/877] [1741/2631] lr:0.0003395 Loss: 0.5893 Loss_dice: 0.2869 Loss_ce: 0.3023\n",
      "Epoch: 1 [865/877] [1742/2631] lr:0.0003393 Loss: 0.5929 Loss_dice: 0.2902 Loss_ce: 0.3027\n",
      "Epoch: 1 [866/877] [1743/2631] lr:0.0003392 Loss: 0.5869 Loss_dice: 0.2218 Loss_ce: 0.3651\n",
      "Epoch: 1 [867/877] [1744/2631] lr:0.0003390 Loss: 0.5482 Loss_dice: 0.2823 Loss_ce: 0.2659\n",
      "Epoch: 1 [868/877] [1745/2631] lr:0.0003388 Loss: 0.5407 Loss_dice: 0.2195 Loss_ce: 0.3212\n",
      "Epoch: 1 [869/877] [1746/2631] lr:0.0003387 Loss: 0.5505 Loss_dice: 0.2825 Loss_ce: 0.2680\n",
      "Epoch: 1 [870/877] [1747/2631] lr:0.0003385 Loss: 0.5360 Loss_dice: 0.2231 Loss_ce: 0.3129\n",
      "Epoch: 1 [871/877] [1748/2631] lr:0.0003383 Loss: 0.6239 Loss_dice: 0.3108 Loss_ce: 0.3131\n",
      "Epoch: 1 [872/877] [1749/2631] lr:0.0003382 Loss: 0.8440 Loss_dice: 0.3512 Loss_ce: 0.4928\n",
      "Epoch: 1 [873/877] [1750/2631] lr:0.0003380 Loss: 0.5375 Loss_dice: 0.2817 Loss_ce: 0.2558\n",
      "Epoch: 1 [874/877] [1751/2631] lr:0.0003378 Loss: 0.5352 Loss_dice: 0.2888 Loss_ce: 0.2464\n",
      "Epoch: 1 [875/877] [1752/2631] lr:0.0003377 Loss: 0.5326 Loss_dice: 0.2544 Loss_ce: 0.2781\n",
      "Epoch: 1 [876/877] [1753/2631] lr:0.0003375 Loss: 0.5983 Loss_dice: 0.2959 Loss_ce: 0.3024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(8379:140121727778944,MainProcess):2022-11-09-06:08:42.886.046 [mindspore/dataset/engine/datasets_user_defined.py:766] GeneratorDataset's num_parallel_workers: 4 is too large which may cause a lot of memory occupation (>85%) or out of memory(OOM) during multiprocessing. Therefore, it is recommended to reduce num_parallel_workers to 2 or smaller.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model to output/./checkpoint//Epoch_1_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(14469:140121727778944,_MPWorker-100):2022-11-09-06:09:07.237.620 [mindspore/dataset/engine/queue.py:120] Using shared memory queue, but rowsize is larger than allocated memory max_rowsize 16777216 current rowsize 77070336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 [0/877] [1754/2631] lr:0.0003373 Loss: 0.5599 Loss_dice: 0.2976 Loss_ce: 0.2623\n",
      "Epoch: 2 [1/877] [1755/2631] lr:0.0003371 Loss: 0.5535 Loss_dice: 0.2873 Loss_ce: 0.2662\n",
      "Epoch: 2 [2/877] [1756/2631] lr:0.0003370 Loss: 0.5350 Loss_dice: 0.2869 Loss_ce: 0.2481\n",
      "Epoch: 2 [3/877] [1757/2631] lr:0.0003368 Loss: 0.5481 Loss_dice: 0.2872 Loss_ce: 0.2609\n",
      "Epoch: 2 [4/877] [1758/2631] lr:0.0003366 Loss: 0.5650 Loss_dice: 0.2609 Loss_ce: 0.3041\n",
      "Epoch: 2 [5/877] [1759/2631] lr:0.0003365 Loss: 0.5511 Loss_dice: 0.2890 Loss_ce: 0.2621\n",
      "Epoch: 2 [6/877] [1760/2631] lr:0.0003363 Loss: 0.5289 Loss_dice: 0.2739 Loss_ce: 0.2550\n",
      "Epoch: 2 [7/877] [1761/2631] lr:0.0003361 Loss: 0.5699 Loss_dice: 0.2927 Loss_ce: 0.2772\n",
      "Epoch: 2 [8/877] [1762/2631] lr:0.0003360 Loss: 0.5328 Loss_dice: 0.2820 Loss_ce: 0.2508\n",
      "Epoch: 2 [9/877] [1763/2631] lr:0.0003358 Loss: 0.5601 Loss_dice: 0.2947 Loss_ce: 0.2654\n",
      "Epoch: 2 [10/877] [1764/2631] lr:0.0003356 Loss: 0.5359 Loss_dice: 0.2844 Loss_ce: 0.2515\n",
      "Epoch: 2 [11/877] [1765/2631] lr:0.0003355 Loss: 0.5649 Loss_dice: 0.2875 Loss_ce: 0.2774\n",
      "Epoch: 2 [12/877] [1766/2631] lr:0.0003353 Loss: 0.5521 Loss_dice: 0.2921 Loss_ce: 0.2600\n",
      "Epoch: 2 [13/877] [1767/2631] lr:0.0003351 Loss: 0.5289 Loss_dice: 0.2797 Loss_ce: 0.2492\n",
      "Epoch: 2 [14/877] [1768/2631] lr:0.0003350 Loss: 0.5341 Loss_dice: 0.2831 Loss_ce: 0.2510\n",
      "Epoch: 2 [15/877] [1769/2631] lr:0.0003348 Loss: 0.9248 Loss_dice: 0.3276 Loss_ce: 0.5972\n",
      "Epoch: 2 [16/877] [1770/2631] lr:0.0003346 Loss: 0.5311 Loss_dice: 0.2531 Loss_ce: 0.2780\n",
      "Epoch: 2 [17/877] [1771/2631] lr:0.0003345 Loss: 0.5362 Loss_dice: 0.2808 Loss_ce: 0.2554\n",
      "Epoch: 2 [18/877] [1772/2631] lr:0.0003343 Loss: 0.6643 Loss_dice: 0.3007 Loss_ce: 0.3636\n",
      "Epoch: 2 [19/877] [1773/2631] lr:0.0003341 Loss: 0.5446 Loss_dice: 0.2594 Loss_ce: 0.2852\n",
      "Epoch: 2 [20/877] [1774/2631] lr:0.0003340 Loss: 0.5517 Loss_dice: 0.2886 Loss_ce: 0.2631\n",
      "Epoch: 2 [21/877] [1775/2631] lr:0.0003338 Loss: 0.5518 Loss_dice: 0.2856 Loss_ce: 0.2662\n",
      "Epoch: 2 [22/877] [1776/2631] lr:0.0003336 Loss: 0.5690 Loss_dice: 0.2852 Loss_ce: 0.2838\n",
      "Epoch: 2 [23/877] [1777/2631] lr:0.0003334 Loss: 0.5779 Loss_dice: 0.2958 Loss_ce: 0.2821\n",
      "Epoch: 2 [24/877] [1778/2631] lr:0.0003333 Loss: 0.5390 Loss_dice: 0.2710 Loss_ce: 0.2679\n",
      "Epoch: 2 [25/877] [1779/2631] lr:0.0003331 Loss: 0.5414 Loss_dice: 0.2774 Loss_ce: 0.2640\n",
      "Epoch: 2 [26/877] [1780/2631] lr:0.0003329 Loss: 0.5360 Loss_dice: 0.2696 Loss_ce: 0.2664\n",
      "Epoch: 2 [27/877] [1781/2631] lr:0.0003328 Loss: 0.5377 Loss_dice: 0.2696 Loss_ce: 0.2680\n",
      "Epoch: 2 [28/877] [1782/2631] lr:0.0003326 Loss: 0.5323 Loss_dice: 0.2752 Loss_ce: 0.2571\n",
      "Epoch: 2 [29/877] [1783/2631] lr:0.0003324 Loss: 0.5303 Loss_dice: 0.2674 Loss_ce: 0.2629\n",
      "Epoch: 2 [30/877] [1784/2631] lr:0.0003323 Loss: 0.5402 Loss_dice: 0.2537 Loss_ce: 0.2866\n",
      "Epoch: 2 [31/877] [1785/2631] lr:0.0003321 Loss: 0.5426 Loss_dice: 0.2623 Loss_ce: 0.2803\n",
      "Epoch: 2 [32/877] [1786/2631] lr:0.0003319 Loss: 0.5487 Loss_dice: 0.2714 Loss_ce: 0.2773\n",
      "Epoch: 2 [33/877] [1787/2631] lr:0.0003318 Loss: 0.5624 Loss_dice: 0.2281 Loss_ce: 0.3344\n",
      "Epoch: 2 [34/877] [1788/2631] lr:0.0003316 Loss: 0.5283 Loss_dice: 0.2578 Loss_ce: 0.2705\n",
      "Epoch: 2 [35/877] [1789/2631] lr:0.0003314 Loss: 0.6410 Loss_dice: 0.3103 Loss_ce: 0.3307\n",
      "Epoch: 2 [36/877] [1790/2631] lr:0.0003312 Loss: 0.5372 Loss_dice: 0.2696 Loss_ce: 0.2675\n",
      "Epoch: 2 [37/877] [1791/2631] lr:0.0003311 Loss: 0.5431 Loss_dice: 0.2607 Loss_ce: 0.2824\n",
      "Epoch: 2 [38/877] [1792/2631] lr:0.0003309 Loss: 0.5331 Loss_dice: 0.2852 Loss_ce: 0.2479\n",
      "Epoch: 2 [39/877] [1793/2631] lr:0.0003307 Loss: 0.5382 Loss_dice: 0.2677 Loss_ce: 0.2704\n",
      "Epoch: 2 [40/877] [1794/2631] lr:0.0003306 Loss: 0.6055 Loss_dice: 0.3004 Loss_ce: 0.3052\n",
      "Epoch: 2 [41/877] [1795/2631] lr:0.0003304 Loss: 0.5445 Loss_dice: 0.2908 Loss_ce: 0.2537\n",
      "Epoch: 2 [42/877] [1796/2631] lr:0.0003302 Loss: 0.5470 Loss_dice: 0.2862 Loss_ce: 0.2607\n",
      "Epoch: 2 [43/877] [1797/2631] lr:0.0003301 Loss: 0.5779 Loss_dice: 0.2479 Loss_ce: 0.3300\n",
      "Epoch: 2 [44/877] [1798/2631] lr:0.0003299 Loss: 0.5319 Loss_dice: 0.2854 Loss_ce: 0.2465\n",
      "Epoch: 2 [45/877] [1799/2631] lr:0.0003297 Loss: 0.5363 Loss_dice: 0.2861 Loss_ce: 0.2502\n",
      "Epoch: 2 [46/877] [1800/2631] lr:0.0003296 Loss: 0.5776 Loss_dice: 0.2922 Loss_ce: 0.2854\n",
      "Epoch: 2 [47/877] [1801/2631] lr:0.0003294 Loss: 0.5482 Loss_dice: 0.2546 Loss_ce: 0.2935\n",
      "Epoch: 2 [48/877] [1802/2631] lr:0.0003292 Loss: 0.5719 Loss_dice: 0.2999 Loss_ce: 0.2720\n",
      "Epoch: 2 [49/877] [1803/2631] lr:0.0003290 Loss: 0.5303 Loss_dice: 0.2886 Loss_ce: 0.2417\n",
      "Epoch: 2 [50/877] [1804/2631] lr:0.0003289 Loss: 0.5659 Loss_dice: 0.2557 Loss_ce: 0.3102\n",
      "Epoch: 2 [51/877] [1805/2631] lr:0.0003287 Loss: 0.5357 Loss_dice: 0.2716 Loss_ce: 0.2641\n",
      "Epoch: 2 [52/877] [1806/2631] lr:0.0003285 Loss: 0.5478 Loss_dice: 0.2596 Loss_ce: 0.2882\n",
      "Epoch: 2 [53/877] [1807/2631] lr:0.0003284 Loss: 0.5308 Loss_dice: 0.2812 Loss_ce: 0.2496\n",
      "Epoch: 2 [54/877] [1808/2631] lr:0.0003282 Loss: 0.5378 Loss_dice: 0.2663 Loss_ce: 0.2715\n",
      "Epoch: 2 [55/877] [1809/2631] lr:0.0003280 Loss: 0.5299 Loss_dice: 0.2830 Loss_ce: 0.2469\n",
      "Epoch: 2 [56/877] [1810/2631] lr:0.0003279 Loss: 0.5292 Loss_dice: 0.2561 Loss_ce: 0.2731\n",
      "Epoch: 2 [57/877] [1811/2631] lr:0.0003277 Loss: 0.5588 Loss_dice: 0.3034 Loss_ce: 0.2554\n",
      "Epoch: 2 [58/877] [1812/2631] lr:0.0003275 Loss: 0.5442 Loss_dice: 0.3015 Loss_ce: 0.2427\n",
      "Epoch: 2 [59/877] [1813/2631] lr:0.0003273 Loss: 0.5287 Loss_dice: 0.2714 Loss_ce: 0.2573\n",
      "Epoch: 2 [60/877] [1814/2631] lr:0.0003272 Loss: 0.5879 Loss_dice: 0.3060 Loss_ce: 0.2819\n",
      "Epoch: 2 [61/877] [1815/2631] lr:0.0003270 Loss: 0.5548 Loss_dice: 0.3000 Loss_ce: 0.2548\n",
      "Epoch: 2 [62/877] [1816/2631] lr:0.0003268 Loss: 0.5318 Loss_dice: 0.2821 Loss_ce: 0.2497\n",
      "Epoch: 2 [63/877] [1817/2631] lr:0.0003267 Loss: 0.5325 Loss_dice: 0.2998 Loss_ce: 0.2327\n",
      "Epoch: 2 [64/877] [1818/2631] lr:0.0003265 Loss: 0.5508 Loss_dice: 0.2957 Loss_ce: 0.2551\n",
      "Epoch: 2 [65/877] [1819/2631] lr:0.0003263 Loss: 0.5464 Loss_dice: 0.2833 Loss_ce: 0.2631\n",
      "Epoch: 2 [66/877] [1820/2631] lr:0.0003261 Loss: 0.5374 Loss_dice: 0.2714 Loss_ce: 0.2660\n",
      "Epoch: 2 [67/877] [1821/2631] lr:0.0003260 Loss: 0.5405 Loss_dice: 0.2268 Loss_ce: 0.3137\n",
      "Epoch: 2 [68/877] [1822/2631] lr:0.0003258 Loss: 0.6183 Loss_dice: 0.2491 Loss_ce: 0.3693\n",
      "Epoch: 2 [69/877] [1823/2631] lr:0.0003256 Loss: 0.5256 Loss_dice: 0.2803 Loss_ce: 0.2453\n",
      "Epoch: 2 [70/877] [1824/2631] lr:0.0003255 Loss: 0.5247 Loss_dice: 0.2739 Loss_ce: 0.2507\n",
      "Epoch: 2 [71/877] [1825/2631] lr:0.0003253 Loss: 0.5461 Loss_dice: 0.2973 Loss_ce: 0.2488\n",
      "Epoch: 2 [72/877] [1826/2631] lr:0.0003251 Loss: 0.6203 Loss_dice: 0.3122 Loss_ce: 0.3081\n",
      "Epoch: 2 [73/877] [1827/2631] lr:0.0003250 Loss: 0.9170 Loss_dice: 0.3881 Loss_ce: 0.5289\n",
      "Epoch: 2 [74/877] [1828/2631] lr:0.0003248 Loss: 0.5999 Loss_dice: 0.3000 Loss_ce: 0.3000\n",
      "Epoch: 2 [75/877] [1829/2631] lr:0.0003246 Loss: 0.5686 Loss_dice: 0.2990 Loss_ce: 0.2697\n",
      "Epoch: 2 [76/877] [1830/2631] lr:0.0003244 Loss: 0.5394 Loss_dice: 0.2899 Loss_ce: 0.2494\n",
      "Epoch: 2 [77/877] [1831/2631] lr:0.0003243 Loss: 0.5551 Loss_dice: 0.2879 Loss_ce: 0.2672\n",
      "Epoch: 2 [78/877] [1832/2631] lr:0.0003241 Loss: 0.5483 Loss_dice: 0.2811 Loss_ce: 0.2672\n",
      "Epoch: 2 [79/877] [1833/2631] lr:0.0003239 Loss: 0.5400 Loss_dice: 0.2826 Loss_ce: 0.2574\n",
      "Epoch: 2 [80/877] [1834/2631] lr:0.0003238 Loss: 0.7196 Loss_dice: 0.3190 Loss_ce: 0.4006\n",
      "Epoch: 2 [81/877] [1835/2631] lr:0.0003236 Loss: 0.5710 Loss_dice: 0.2900 Loss_ce: 0.2810\n",
      "Epoch: 2 [82/877] [1836/2631] lr:0.0003234 Loss: 0.5377 Loss_dice: 0.2699 Loss_ce: 0.2678\n",
      "Epoch: 2 [83/877] [1837/2631] lr:0.0003232 Loss: 0.5344 Loss_dice: 0.2814 Loss_ce: 0.2530\n",
      "Epoch: 2 [84/877] [1838/2631] lr:0.0003231 Loss: 0.5836 Loss_dice: 0.2679 Loss_ce: 0.3157\n",
      "Epoch: 2 [85/877] [1839/2631] lr:0.0003229 Loss: 0.5405 Loss_dice: 0.2478 Loss_ce: 0.2927\n",
      "Epoch: 2 [86/877] [1840/2631] lr:0.0003227 Loss: 0.5792 Loss_dice: 0.2911 Loss_ce: 0.2880\n",
      "Epoch: 2 [87/877] [1841/2631] lr:0.0003226 Loss: 0.5504 Loss_dice: 0.2930 Loss_ce: 0.2574\n",
      "Epoch: 2 [88/877] [1842/2631] lr:0.0003224 Loss: 0.5382 Loss_dice: 0.2740 Loss_ce: 0.2642\n",
      "Epoch: 2 [89/877] [1843/2631] lr:0.0003222 Loss: 0.6162 Loss_dice: 0.2987 Loss_ce: 0.3175\n",
      "Epoch: 2 [90/877] [1844/2631] lr:0.0003220 Loss: 0.5294 Loss_dice: 0.2498 Loss_ce: 0.2796\n",
      "Epoch: 2 [91/877] [1845/2631] lr:0.0003219 Loss: 0.5532 Loss_dice: 0.2713 Loss_ce: 0.2819\n",
      "Epoch: 2 [92/877] [1846/2631] lr:0.0003217 Loss: 0.5335 Loss_dice: 0.2869 Loss_ce: 0.2466\n",
      "Epoch: 2 [93/877] [1847/2631] lr:0.0003215 Loss: 0.5731 Loss_dice: 0.2796 Loss_ce: 0.2935\n",
      "Epoch: 2 [94/877] [1848/2631] lr:0.0003214 Loss: 0.5281 Loss_dice: 0.2846 Loss_ce: 0.2435\n",
      "Epoch: 2 [95/877] [1849/2631] lr:0.0003212 Loss: 0.5341 Loss_dice: 0.2846 Loss_ce: 0.2495\n",
      "Epoch: 2 [96/877] [1850/2631] lr:0.0003210 Loss: 0.5484 Loss_dice: 0.2953 Loss_ce: 0.2531\n",
      "Epoch: 2 [97/877] [1851/2631] lr:0.0003208 Loss: 0.5823 Loss_dice: 0.2877 Loss_ce: 0.2947\n",
      "Epoch: 2 [98/877] [1852/2631] lr:0.0003207 Loss: 0.5444 Loss_dice: 0.2573 Loss_ce: 0.2871\n",
      "Epoch: 2 [99/877] [1853/2631] lr:0.0003205 Loss: 0.5478 Loss_dice: 0.2875 Loss_ce: 0.2603\n",
      "Epoch: 2 [100/877] [1854/2631] lr:0.0003203 Loss: 0.5400 Loss_dice: 0.2945 Loss_ce: 0.2455\n",
      "Epoch: 2 [101/877] [1855/2631] lr:0.0003202 Loss: 0.5290 Loss_dice: 0.2674 Loss_ce: 0.2616\n",
      "Epoch: 2 [102/877] [1856/2631] lr:0.0003200 Loss: 0.6102 Loss_dice: 0.3032 Loss_ce: 0.3070\n",
      "Epoch: 2 [103/877] [1857/2631] lr:0.0003198 Loss: 0.5431 Loss_dice: 0.2603 Loss_ce: 0.2828\n",
      "Epoch: 2 [104/877] [1858/2631] lr:0.0003196 Loss: 0.5515 Loss_dice: 0.2754 Loss_ce: 0.2761\n",
      "Epoch: 2 [105/877] [1859/2631] lr:0.0003195 Loss: 0.7042 Loss_dice: 0.3265 Loss_ce: 0.3777\n",
      "Epoch: 2 [106/877] [1860/2631] lr:0.0003193 Loss: 0.5360 Loss_dice: 0.2531 Loss_ce: 0.2829\n",
      "Epoch: 2 [107/877] [1861/2631] lr:0.0003191 Loss: 0.5976 Loss_dice: 0.3083 Loss_ce: 0.2893\n",
      "Epoch: 2 [108/877] [1862/2631] lr:0.0003189 Loss: 0.5314 Loss_dice: 0.2895 Loss_ce: 0.2419\n",
      "Epoch: 2 [109/877] [1863/2631] lr:0.0003188 Loss: 0.5393 Loss_dice: 0.2519 Loss_ce: 0.2874\n",
      "Epoch: 2 [110/877] [1864/2631] lr:0.0003186 Loss: 0.5338 Loss_dice: 0.2744 Loss_ce: 0.2594\n",
      "Epoch: 2 [111/877] [1865/2631] lr:0.0003184 Loss: 0.5396 Loss_dice: 0.2914 Loss_ce: 0.2482\n",
      "Epoch: 2 [112/877] [1866/2631] lr:0.0003183 Loss: 0.6232 Loss_dice: 0.3164 Loss_ce: 0.3068\n",
      "Epoch: 2 [113/877] [1867/2631] lr:0.0003181 Loss: 0.6283 Loss_dice: 0.2996 Loss_ce: 0.3287\n",
      "Epoch: 2 [114/877] [1868/2631] lr:0.0003179 Loss: 1.4621 Loss_dice: 0.5100 Loss_ce: 0.9521\n",
      "Epoch: 2 [115/877] [1869/2631] lr:0.0003177 Loss: 0.5376 Loss_dice: 0.2382 Loss_ce: 0.2994\n",
      "Epoch: 2 [116/877] [1870/2631] lr:0.0003176 Loss: 0.5424 Loss_dice: 0.2632 Loss_ce: 0.2791\n",
      "Epoch: 2 [117/877] [1871/2631] lr:0.0003174 Loss: 0.5679 Loss_dice: 0.2893 Loss_ce: 0.2786\n",
      "Epoch: 2 [118/877] [1872/2631] lr:0.0003172 Loss: 0.6476 Loss_dice: 0.3121 Loss_ce: 0.3355\n",
      "Epoch: 2 [119/877] [1873/2631] lr:0.0003171 Loss: 0.5436 Loss_dice: 0.2800 Loss_ce: 0.2636\n",
      "Epoch: 2 [120/877] [1874/2631] lr:0.0003169 Loss: 0.5418 Loss_dice: 0.2267 Loss_ce: 0.3151\n",
      "Epoch: 2 [121/877] [1875/2631] lr:0.0003167 Loss: 0.5370 Loss_dice: 0.2857 Loss_ce: 0.2514\n",
      "Epoch: 2 [122/877] [1876/2631] lr:0.0003165 Loss: 0.5433 Loss_dice: 0.2791 Loss_ce: 0.2642\n",
      "Epoch: 2 [123/877] [1877/2631] lr:0.0003164 Loss: 0.6087 Loss_dice: 0.2970 Loss_ce: 0.3118\n",
      "Epoch: 2 [124/877] [1878/2631] lr:0.0003162 Loss: 0.5477 Loss_dice: 0.2664 Loss_ce: 0.2813\n",
      "Epoch: 2 [125/877] [1879/2631] lr:0.0003160 Loss: 0.5521 Loss_dice: 0.2893 Loss_ce: 0.2628\n",
      "Epoch: 2 [126/877] [1880/2631] lr:0.0003158 Loss: 0.5262 Loss_dice: 0.2459 Loss_ce: 0.2803\n",
      "Epoch: 2 [127/877] [1881/2631] lr:0.0003157 Loss: 0.5428 Loss_dice: 0.2717 Loss_ce: 0.2711\n",
      "Epoch: 2 [128/877] [1882/2631] lr:0.0003155 Loss: 0.5473 Loss_dice: 0.2814 Loss_ce: 0.2660\n",
      "Epoch: 2 [129/877] [1883/2631] lr:0.0003153 Loss: 0.5521 Loss_dice: 0.2727 Loss_ce: 0.2795\n",
      "Epoch: 2 [130/877] [1884/2631] lr:0.0003152 Loss: 0.5407 Loss_dice: 0.2782 Loss_ce: 0.2625\n",
      "Epoch: 2 [131/877] [1885/2631] lr:0.0003150 Loss: 0.5654 Loss_dice: 0.2704 Loss_ce: 0.2951\n",
      "Epoch: 2 [132/877] [1886/2631] lr:0.0003148 Loss: 0.6725 Loss_dice: 0.2383 Loss_ce: 0.4342\n",
      "Epoch: 2 [133/877] [1887/2631] lr:0.0003146 Loss: 0.5355 Loss_dice: 0.2499 Loss_ce: 0.2856\n",
      "Epoch: 2 [134/877] [1888/2631] lr:0.0003145 Loss: 0.6186 Loss_dice: 0.2222 Loss_ce: 0.3964\n",
      "Epoch: 2 [135/877] [1889/2631] lr:0.0003143 Loss: 0.5334 Loss_dice: 0.2657 Loss_ce: 0.2677\n",
      "Epoch: 2 [136/877] [1890/2631] lr:0.0003141 Loss: 0.5624 Loss_dice: 0.2834 Loss_ce: 0.2790\n",
      "Epoch: 2 [137/877] [1891/2631] lr:0.0003139 Loss: 0.5488 Loss_dice: 0.2770 Loss_ce: 0.2718\n",
      "Epoch: 2 [138/877] [1892/2631] lr:0.0003138 Loss: 0.5490 Loss_dice: 0.2748 Loss_ce: 0.2742\n",
      "Epoch: 2 [139/877] [1893/2631] lr:0.0003136 Loss: 0.5520 Loss_dice: 0.2799 Loss_ce: 0.2721\n",
      "Epoch: 2 [140/877] [1894/2631] lr:0.0003134 Loss: 0.5291 Loss_dice: 0.2486 Loss_ce: 0.2805\n",
      "Epoch: 2 [141/877] [1895/2631] lr:0.0003132 Loss: 0.5356 Loss_dice: 0.2466 Loss_ce: 0.2891\n",
      "Epoch: 2 [142/877] [1896/2631] lr:0.0003131 Loss: 0.5734 Loss_dice: 0.2944 Loss_ce: 0.2789\n",
      "Epoch: 2 [143/877] [1897/2631] lr:0.0003129 Loss: 0.5488 Loss_dice: 0.2596 Loss_ce: 0.2892\n",
      "Epoch: 2 [144/877] [1898/2631] lr:0.0003127 Loss: 0.5322 Loss_dice: 0.2524 Loss_ce: 0.2798\n",
      "Epoch: 2 [145/877] [1899/2631] lr:0.0003126 Loss: 0.5324 Loss_dice: 0.2729 Loss_ce: 0.2595\n",
      "Epoch: 2 [146/877] [1900/2631] lr:0.0003124 Loss: 0.5397 Loss_dice: 0.2942 Loss_ce: 0.2455\n",
      "Epoch: 2 [147/877] [1901/2631] lr:0.0003122 Loss: 0.6351 Loss_dice: 0.3105 Loss_ce: 0.3246\n",
      "Epoch: 2 [148/877] [1902/2631] lr:0.0003120 Loss: 0.5878 Loss_dice: 0.2986 Loss_ce: 0.2892\n",
      "Epoch: 2 [149/877] [1903/2631] lr:0.0003119 Loss: 0.5542 Loss_dice: 0.3009 Loss_ce: 0.2533\n",
      "Epoch: 2 [150/877] [1904/2631] lr:0.0003117 Loss: 0.5376 Loss_dice: 0.2707 Loss_ce: 0.2669\n",
      "Epoch: 2 [151/877] [1905/2631] lr:0.0003115 Loss: 0.5761 Loss_dice: 0.2865 Loss_ce: 0.2896\n",
      "Epoch: 2 [152/877] [1906/2631] lr:0.0003113 Loss: 0.5474 Loss_dice: 0.2910 Loss_ce: 0.2564\n",
      "Epoch: 2 [153/877] [1907/2631] lr:0.0003112 Loss: 0.5657 Loss_dice: 0.2951 Loss_ce: 0.2706\n",
      "Epoch: 2 [154/877] [1908/2631] lr:0.0003110 Loss: 0.5505 Loss_dice: 0.2819 Loss_ce: 0.2686\n",
      "Epoch: 2 [155/877] [1909/2631] lr:0.0003108 Loss: 0.5575 Loss_dice: 0.2089 Loss_ce: 0.3486\n",
      "Epoch: 2 [156/877] [1910/2631] lr:0.0003106 Loss: 0.5376 Loss_dice: 0.2769 Loss_ce: 0.2607\n",
      "Epoch: 2 [157/877] [1911/2631] lr:0.0003105 Loss: 0.6034 Loss_dice: 0.3139 Loss_ce: 0.2895\n",
      "Epoch: 2 [158/877] [1912/2631] lr:0.0003103 Loss: 0.5381 Loss_dice: 0.2511 Loss_ce: 0.2870\n",
      "Epoch: 2 [159/877] [1913/2631] lr:0.0003101 Loss: 0.5427 Loss_dice: 0.2895 Loss_ce: 0.2532\n",
      "Epoch: 2 [160/877] [1914/2631] lr:0.0003099 Loss: 0.5543 Loss_dice: 0.2991 Loss_ce: 0.2552\n",
      "Epoch: 2 [161/877] [1915/2631] lr:0.0003098 Loss: 0.5306 Loss_dice: 0.2841 Loss_ce: 0.2464\n",
      "Epoch: 2 [162/877] [1916/2631] lr:0.0003096 Loss: 0.5716 Loss_dice: 0.3007 Loss_ce: 0.2709\n",
      "Epoch: 2 [163/877] [1917/2631] lr:0.0003094 Loss: 0.5639 Loss_dice: 0.2894 Loss_ce: 0.2745\n",
      "Epoch: 2 [164/877] [1918/2631] lr:0.0003093 Loss: 0.5489 Loss_dice: 0.2901 Loss_ce: 0.2588\n",
      "Epoch: 2 [165/877] [1919/2631] lr:0.0003091 Loss: 0.5459 Loss_dice: 0.2880 Loss_ce: 0.2578\n",
      "Epoch: 2 [166/877] [1920/2631] lr:0.0003089 Loss: 0.5423 Loss_dice: 0.2897 Loss_ce: 0.2526\n",
      "Epoch: 2 [167/877] [1921/2631] lr:0.0003087 Loss: 0.5527 Loss_dice: 0.2926 Loss_ce: 0.2601\n",
      "Epoch: 2 [168/877] [1922/2631] lr:0.0003086 Loss: 0.5640 Loss_dice: 0.2953 Loss_ce: 0.2688\n",
      "Epoch: 2 [169/877] [1923/2631] lr:0.0003084 Loss: 0.5530 Loss_dice: 0.2949 Loss_ce: 0.2581\n",
      "Epoch: 2 [170/877] [1924/2631] lr:0.0003082 Loss: 0.6009 Loss_dice: 0.3048 Loss_ce: 0.2961\n",
      "Epoch: 2 [171/877] [1925/2631] lr:0.0003080 Loss: 0.5384 Loss_dice: 0.2625 Loss_ce: 0.2760\n",
      "Epoch: 2 [172/877] [1926/2631] lr:0.0003079 Loss: 0.5406 Loss_dice: 0.2836 Loss_ce: 0.2570\n",
      "Epoch: 2 [173/877] [1927/2631] lr:0.0003077 Loss: 0.5510 Loss_dice: 0.2947 Loss_ce: 0.2563\n",
      "Epoch: 2 [174/877] [1928/2631] lr:0.0003075 Loss: 0.5469 Loss_dice: 0.2926 Loss_ce: 0.2543\n",
      "Epoch: 2 [175/877] [1929/2631] lr:0.0003073 Loss: 0.6404 Loss_dice: 0.2833 Loss_ce: 0.3571\n",
      "Epoch: 2 [176/877] [1930/2631] lr:0.0003072 Loss: 0.5376 Loss_dice: 0.2721 Loss_ce: 0.2656\n",
      "Epoch: 2 [177/877] [1931/2631] lr:0.0003070 Loss: 0.5317 Loss_dice: 0.2500 Loss_ce: 0.2817\n",
      "Epoch: 2 [178/877] [1932/2631] lr:0.0003068 Loss: 0.5350 Loss_dice: 0.2660 Loss_ce: 0.2690\n",
      "Epoch: 2 [179/877] [1933/2631] lr:0.0003066 Loss: 0.5868 Loss_dice: 0.2679 Loss_ce: 0.3189\n",
      "Epoch: 2 [180/877] [1934/2631] lr:0.0003065 Loss: 0.5515 Loss_dice: 0.2925 Loss_ce: 0.2591\n",
      "Epoch: 2 [181/877] [1935/2631] lr:0.0003063 Loss: 0.6598 Loss_dice: 0.3153 Loss_ce: 0.3445\n",
      "Epoch: 2 [182/877] [1936/2631] lr:0.0003061 Loss: 0.5527 Loss_dice: 0.2916 Loss_ce: 0.2611\n",
      "Epoch: 2 [183/877] [1937/2631] lr:0.0003059 Loss: 0.5454 Loss_dice: 0.2822 Loss_ce: 0.2632\n",
      "Epoch: 2 [184/877] [1938/2631] lr:0.0003058 Loss: 0.6532 Loss_dice: 0.2975 Loss_ce: 0.3557\n",
      "Epoch: 2 [185/877] [1939/2631] lr:0.0003056 Loss: 0.5449 Loss_dice: 0.2931 Loss_ce: 0.2519\n",
      "Epoch: 2 [186/877] [1940/2631] lr:0.0003054 Loss: 0.5431 Loss_dice: 0.2957 Loss_ce: 0.2474\n",
      "Epoch: 2 [187/877] [1941/2631] lr:0.0003052 Loss: 0.7462 Loss_dice: 0.3116 Loss_ce: 0.4347\n",
      "Epoch: 2 [188/877] [1942/2631] lr:0.0003051 Loss: 1.7083 Loss_dice: 0.5772 Loss_ce: 1.1311\n",
      "Epoch: 2 [189/877] [1943/2631] lr:0.0003049 Loss: 0.5455 Loss_dice: 0.2782 Loss_ce: 0.2673\n",
      "Epoch: 2 [190/877] [1944/2631] lr:0.0003047 Loss: 0.5775 Loss_dice: 0.1873 Loss_ce: 0.3901\n",
      "Epoch: 2 [191/877] [1945/2631] lr:0.0003045 Loss: 0.5564 Loss_dice: 0.2896 Loss_ce: 0.2669\n",
      "Epoch: 2 [192/877] [1946/2631] lr:0.0003044 Loss: 0.5552 Loss_dice: 0.2163 Loss_ce: 0.3389\n",
      "Epoch: 2 [193/877] [1947/2631] lr:0.0003042 Loss: 0.5545 Loss_dice: 0.2796 Loss_ce: 0.2749\n",
      "Epoch: 2 [194/877] [1948/2631] lr:0.0003040 Loss: 0.6022 Loss_dice: 0.2887 Loss_ce: 0.3136\n",
      "Epoch: 2 [195/877] [1949/2631] lr:0.0003038 Loss: 0.6087 Loss_dice: 0.2509 Loss_ce: 0.3578\n",
      "Epoch: 2 [196/877] [1950/2631] lr:0.0003037 Loss: 0.5694 Loss_dice: 0.2624 Loss_ce: 0.3070\n",
      "Epoch: 2 [197/877] [1951/2631] lr:0.0003035 Loss: 0.5394 Loss_dice: 0.2545 Loss_ce: 0.2849\n",
      "Epoch: 2 [198/877] [1952/2631] lr:0.0003033 Loss: 0.6160 Loss_dice: 0.2859 Loss_ce: 0.3300\n",
      "Epoch: 2 [199/877] [1953/2631] lr:0.0003031 Loss: 0.5377 Loss_dice: 0.2196 Loss_ce: 0.3181\n",
      "Epoch: 2 [200/877] [1954/2631] lr:0.0003030 Loss: 1.3529 Loss_dice: 0.4818 Loss_ce: 0.8711\n",
      "Epoch: 2 [201/877] [1955/2631] lr:0.0003028 Loss: 0.5436 Loss_dice: 0.2588 Loss_ce: 0.2848\n",
      "Epoch: 2 [202/877] [1956/2631] lr:0.0003026 Loss: 0.5400 Loss_dice: 0.2471 Loss_ce: 0.2928\n",
      "Epoch: 2 [203/877] [1957/2631] lr:0.0003024 Loss: 0.5726 Loss_dice: 0.2156 Loss_ce: 0.3570\n",
      "Epoch: 2 [204/877] [1958/2631] lr:0.0003023 Loss: 0.5614 Loss_dice: 0.2570 Loss_ce: 0.3043\n",
      "Epoch: 2 [205/877] [1959/2631] lr:0.0003021 Loss: 0.5489 Loss_dice: 0.2364 Loss_ce: 0.3126\n",
      "Epoch: 2 [206/877] [1960/2631] lr:0.0003019 Loss: 0.5616 Loss_dice: 0.2676 Loss_ce: 0.2940\n",
      "Epoch: 2 [207/877] [1961/2631] lr:0.0003017 Loss: 0.6620 Loss_dice: 0.2924 Loss_ce: 0.3696\n",
      "Epoch: 2 [208/877] [1962/2631] lr:0.0003016 Loss: 0.5532 Loss_dice: 0.2532 Loss_ce: 0.3000\n",
      "Epoch: 2 [209/877] [1963/2631] lr:0.0003014 Loss: 0.5526 Loss_dice: 0.2636 Loss_ce: 0.2890\n",
      "Epoch: 2 [210/877] [1964/2631] lr:0.0003012 Loss: 0.5765 Loss_dice: 0.2504 Loss_ce: 0.3261\n",
      "Epoch: 2 [211/877] [1965/2631] lr:0.0003010 Loss: 0.5926 Loss_dice: 0.2722 Loss_ce: 0.3204\n",
      "Epoch: 2 [212/877] [1966/2631] lr:0.0003009 Loss: 0.5423 Loss_dice: 0.2466 Loss_ce: 0.2957\n",
      "Epoch: 2 [213/877] [1967/2631] lr:0.0003007 Loss: 0.5480 Loss_dice: 0.2603 Loss_ce: 0.2877\n",
      "Epoch: 2 [214/877] [1968/2631] lr:0.0003005 Loss: 0.5417 Loss_dice: 0.2389 Loss_ce: 0.3028\n",
      "Epoch: 2 [215/877] [1969/2631] lr:0.0003003 Loss: 0.5489 Loss_dice: 0.2703 Loss_ce: 0.2785\n",
      "Epoch: 2 [216/877] [1970/2631] lr:0.0003002 Loss: 0.5454 Loss_dice: 0.2596 Loss_ce: 0.2858\n",
      "Epoch: 2 [217/877] [1971/2631] lr:0.0003000 Loss: 0.5489 Loss_dice: 0.2744 Loss_ce: 0.2745\n",
      "Epoch: 2 [218/877] [1972/2631] lr:0.0002998 Loss: 0.5609 Loss_dice: 0.2715 Loss_ce: 0.2894\n",
      "Epoch: 2 [219/877] [1973/2631] lr:0.0002996 Loss: 0.5577 Loss_dice: 0.2816 Loss_ce: 0.2762\n",
      "Epoch: 2 [220/877] [1974/2631] lr:0.0002995 Loss: 0.5412 Loss_dice: 0.2766 Loss_ce: 0.2646\n",
      "Epoch: 2 [221/877] [1975/2631] lr:0.0002993 Loss: 0.5336 Loss_dice: 0.2718 Loss_ce: 0.2619\n",
      "Epoch: 2 [222/877] [1976/2631] lr:0.0002991 Loss: 0.5478 Loss_dice: 0.2254 Loss_ce: 0.3224\n",
      "Epoch: 2 [223/877] [1977/2631] lr:0.0002989 Loss: 0.5333 Loss_dice: 0.2716 Loss_ce: 0.2618\n",
      "Epoch: 2 [224/877] [1978/2631] lr:0.0002988 Loss: 0.5400 Loss_dice: 0.2670 Loss_ce: 0.2730\n",
      "Epoch: 2 [225/877] [1979/2631] lr:0.0002986 Loss: 0.5398 Loss_dice: 0.2803 Loss_ce: 0.2595\n",
      "Epoch: 2 [226/877] [1980/2631] lr:0.0002984 Loss: 0.5418 Loss_dice: 0.2786 Loss_ce: 0.2633\n",
      "Epoch: 2 [227/877] [1981/2631] lr:0.0002982 Loss: 0.5553 Loss_dice: 0.2776 Loss_ce: 0.2777\n",
      "Epoch: 2 [228/877] [1982/2631] lr:0.0002981 Loss: 0.5597 Loss_dice: 0.2861 Loss_ce: 0.2735\n",
      "Epoch: 2 [229/877] [1983/2631] lr:0.0002979 Loss: 0.5977 Loss_dice: 0.2754 Loss_ce: 0.3222\n",
      "Epoch: 2 [230/877] [1984/2631] lr:0.0002977 Loss: 0.5354 Loss_dice: 0.2644 Loss_ce: 0.2710\n",
      "Epoch: 2 [231/877] [1985/2631] lr:0.0002975 Loss: 0.5383 Loss_dice: 0.2723 Loss_ce: 0.2660\n",
      "Epoch: 2 [232/877] [1986/2631] lr:0.0002974 Loss: 0.5449 Loss_dice: 0.2824 Loss_ce: 0.2624\n",
      "Epoch: 2 [233/877] [1987/2631] lr:0.0002972 Loss: 0.5276 Loss_dice: 0.2688 Loss_ce: 0.2588\n",
      "Epoch: 2 [234/877] [1988/2631] lr:0.0002970 Loss: 0.5416 Loss_dice: 0.2771 Loss_ce: 0.2645\n",
      "Epoch: 2 [235/877] [1989/2631] lr:0.0002968 Loss: 0.5390 Loss_dice: 0.2826 Loss_ce: 0.2564\n",
      "Epoch: 2 [236/877] [1990/2631] lr:0.0002967 Loss: 0.5197 Loss_dice: 0.2679 Loss_ce: 0.2518\n",
      "Epoch: 2 [237/877] [1991/2631] lr:0.0002965 Loss: 0.5438 Loss_dice: 0.2438 Loss_ce: 0.2999\n",
      "Epoch: 2 [238/877] [1992/2631] lr:0.0002963 Loss: 0.5367 Loss_dice: 0.2725 Loss_ce: 0.2642\n",
      "Epoch: 2 [239/877] [1993/2631] lr:0.0002961 Loss: 0.5725 Loss_dice: 0.2927 Loss_ce: 0.2799\n",
      "Epoch: 2 [240/877] [1994/2631] lr:0.0002959 Loss: 0.5467 Loss_dice: 0.2557 Loss_ce: 0.2910\n",
      "Epoch: 2 [241/877] [1995/2631] lr:0.0002958 Loss: 0.5414 Loss_dice: 0.2846 Loss_ce: 0.2568\n",
      "Epoch: 2 [242/877] [1996/2631] lr:0.0002956 Loss: 0.5665 Loss_dice: 0.2936 Loss_ce: 0.2729\n",
      "Epoch: 2 [243/877] [1997/2631] lr:0.0002954 Loss: 0.5356 Loss_dice: 0.2833 Loss_ce: 0.2522\n",
      "Epoch: 2 [244/877] [1998/2631] lr:0.0002952 Loss: 0.5441 Loss_dice: 0.2885 Loss_ce: 0.2557\n",
      "Epoch: 2 [245/877] [1999/2631] lr:0.0002951 Loss: 0.5342 Loss_dice: 0.2798 Loss_ce: 0.2544\n",
      "Epoch: 2 [246/877] [2000/2631] lr:0.0002949 Loss: 0.5396 Loss_dice: 0.2878 Loss_ce: 0.2518\n",
      "Epoch: 2 [247/877] [2001/2631] lr:0.0002947 Loss: 0.5326 Loss_dice: 0.2645 Loss_ce: 0.2681\n",
      "Epoch: 2 [248/877] [2002/2631] lr:0.0002945 Loss: 0.5835 Loss_dice: 0.2983 Loss_ce: 0.2852\n",
      "Epoch: 2 [249/877] [2003/2631] lr:0.0002944 Loss: 0.5400 Loss_dice: 0.2536 Loss_ce: 0.2865\n",
      "Epoch: 2 [250/877] [2004/2631] lr:0.0002942 Loss: 0.5404 Loss_dice: 0.2651 Loss_ce: 0.2754\n",
      "Epoch: 2 [251/877] [2005/2631] lr:0.0002940 Loss: 0.5294 Loss_dice: 0.2859 Loss_ce: 0.2435\n",
      "Epoch: 2 [252/877] [2006/2631] lr:0.0002938 Loss: 0.5443 Loss_dice: 0.2903 Loss_ce: 0.2540\n",
      "Epoch: 2 [253/877] [2007/2631] lr:0.0002937 Loss: 0.5330 Loss_dice: 0.2865 Loss_ce: 0.2465\n",
      "Epoch: 2 [254/877] [2008/2631] lr:0.0002935 Loss: 0.5400 Loss_dice: 0.2699 Loss_ce: 0.2701\n",
      "Epoch: 2 [255/877] [2009/2631] lr:0.0002933 Loss: 0.5439 Loss_dice: 0.2849 Loss_ce: 0.2590\n",
      "Epoch: 2 [256/877] [2010/2631] lr:0.0002931 Loss: 0.6045 Loss_dice: 0.2946 Loss_ce: 0.3100\n",
      "Epoch: 2 [257/877] [2011/2631] lr:0.0002930 Loss: 0.5420 Loss_dice: 0.2881 Loss_ce: 0.2539\n",
      "Epoch: 2 [258/877] [2012/2631] lr:0.0002928 Loss: 0.5416 Loss_dice: 0.2901 Loss_ce: 0.2515\n",
      "Epoch: 2 [259/877] [2013/2631] lr:0.0002926 Loss: 0.5395 Loss_dice: 0.2904 Loss_ce: 0.2491\n",
      "Epoch: 2 [260/877] [2014/2631] lr:0.0002924 Loss: 0.5340 Loss_dice: 0.2892 Loss_ce: 0.2449\n",
      "Epoch: 2 [261/877] [2015/2631] lr:0.0002922 Loss: 0.5387 Loss_dice: 0.2792 Loss_ce: 0.2596\n",
      "Epoch: 2 [262/877] [2016/2631] lr:0.0002921 Loss: 0.5369 Loss_dice: 0.2679 Loss_ce: 0.2689\n",
      "Epoch: 2 [263/877] [2017/2631] lr:0.0002919 Loss: 0.5400 Loss_dice: 0.2861 Loss_ce: 0.2538\n",
      "Epoch: 2 [264/877] [2018/2631] lr:0.0002917 Loss: 0.5415 Loss_dice: 0.2920 Loss_ce: 0.2494\n",
      "Epoch: 2 [265/877] [2019/2631] lr:0.0002915 Loss: 0.6114 Loss_dice: 0.2916 Loss_ce: 0.3198\n",
      "Epoch: 2 [266/877] [2020/2631] lr:0.0002914 Loss: 0.5321 Loss_dice: 0.2803 Loss_ce: 0.2518\n",
      "Epoch: 2 [267/877] [2021/2631] lr:0.0002912 Loss: 0.5547 Loss_dice: 0.2917 Loss_ce: 0.2630\n",
      "Epoch: 2 [268/877] [2022/2631] lr:0.0002910 Loss: 0.6010 Loss_dice: 0.2985 Loss_ce: 0.3025\n",
      "Epoch: 2 [269/877] [2023/2631] lr:0.0002908 Loss: 0.5369 Loss_dice: 0.2406 Loss_ce: 0.2963\n",
      "Epoch: 2 [270/877] [2024/2631] lr:0.0002907 Loss: 0.5453 Loss_dice: 0.2823 Loss_ce: 0.2630\n",
      "Epoch: 2 [271/877] [2025/2631] lr:0.0002905 Loss: 0.5438 Loss_dice: 0.2914 Loss_ce: 0.2524\n",
      "Epoch: 2 [272/877] [2026/2631] lr:0.0002903 Loss: 0.5291 Loss_dice: 0.2899 Loss_ce: 0.2392\n",
      "Epoch: 2 [273/877] [2027/2631] lr:0.0002901 Loss: 1.1139 Loss_dice: 0.4123 Loss_ce: 0.7016\n",
      "Epoch: 2 [274/877] [2028/2631] lr:0.0002899 Loss: 0.5375 Loss_dice: 0.2862 Loss_ce: 0.2513\n",
      "Epoch: 2 [275/877] [2029/2631] lr:0.0002898 Loss: 0.5594 Loss_dice: 0.2868 Loss_ce: 0.2725\n",
      "Epoch: 2 [276/877] [2030/2631] lr:0.0002896 Loss: 0.6355 Loss_dice: 0.3053 Loss_ce: 0.3302\n",
      "Epoch: 2 [277/877] [2031/2631] lr:0.0002894 Loss: 0.5446 Loss_dice: 0.2904 Loss_ce: 0.2542\n",
      "Epoch: 2 [278/877] [2032/2631] lr:0.0002892 Loss: 0.5790 Loss_dice: 0.2814 Loss_ce: 0.2976\n",
      "Epoch: 2 [279/877] [2033/2631] lr:0.0002891 Loss: 0.5311 Loss_dice: 0.2622 Loss_ce: 0.2689\n",
      "Epoch: 2 [280/877] [2034/2631] lr:0.0002889 Loss: 0.6674 Loss_dice: 0.2919 Loss_ce: 0.3755\n",
      "Epoch: 2 [281/877] [2035/2631] lr:0.0002887 Loss: 0.5361 Loss_dice: 0.2625 Loss_ce: 0.2737\n",
      "Epoch: 2 [282/877] [2036/2631] lr:0.0002885 Loss: 0.5610 Loss_dice: 0.2825 Loss_ce: 0.2785\n",
      "Epoch: 2 [283/877] [2037/2631] lr:0.0002884 Loss: 0.5425 Loss_dice: 0.2614 Loss_ce: 0.2811\n",
      "Epoch: 2 [284/877] [2038/2631] lr:0.0002882 Loss: 0.5370 Loss_dice: 0.2441 Loss_ce: 0.2929\n",
      "Epoch: 2 [285/877] [2039/2631] lr:0.0002880 Loss: 0.5301 Loss_dice: 0.2784 Loss_ce: 0.2517\n",
      "Epoch: 2 [286/877] [2040/2631] lr:0.0002878 Loss: 0.5420 Loss_dice: 0.2773 Loss_ce: 0.2647\n",
      "Epoch: 2 [287/877] [2041/2631] lr:0.0002876 Loss: 0.5622 Loss_dice: 0.2669 Loss_ce: 0.2953\n",
      "Epoch: 2 [288/877] [2042/2631] lr:0.0002875 Loss: 0.5449 Loss_dice: 0.2860 Loss_ce: 0.2590\n",
      "Epoch: 2 [289/877] [2043/2631] lr:0.0002873 Loss: 0.5265 Loss_dice: 0.2567 Loss_ce: 0.2698\n",
      "Epoch: 2 [290/877] [2044/2631] lr:0.0002871 Loss: 0.5388 Loss_dice: 0.2567 Loss_ce: 0.2821\n",
      "Epoch: 2 [291/877] [2045/2631] lr:0.0002869 Loss: 0.5321 Loss_dice: 0.2831 Loss_ce: 0.2490\n",
      "Epoch: 2 [292/877] [2046/2631] lr:0.0002868 Loss: 0.5702 Loss_dice: 0.2815 Loss_ce: 0.2888\n",
      "Epoch: 2 [293/877] [2047/2631] lr:0.0002866 Loss: 0.5366 Loss_dice: 0.2663 Loss_ce: 0.2703\n",
      "Epoch: 2 [294/877] [2048/2631] lr:0.0002864 Loss: 0.5415 Loss_dice: 0.2788 Loss_ce: 0.2627\n",
      "Epoch: 2 [295/877] [2049/2631] lr:0.0002862 Loss: 0.5648 Loss_dice: 0.2962 Loss_ce: 0.2687\n",
      "Epoch: 2 [296/877] [2050/2631] lr:0.0002861 Loss: 0.5432 Loss_dice: 0.2842 Loss_ce: 0.2590\n",
      "Epoch: 2 [297/877] [2051/2631] lr:0.0002859 Loss: 0.5369 Loss_dice: 0.2786 Loss_ce: 0.2584\n",
      "Epoch: 2 [298/877] [2052/2631] lr:0.0002857 Loss: 0.5393 Loss_dice: 0.2534 Loss_ce: 0.2859\n",
      "Epoch: 2 [299/877] [2053/2631] lr:0.0002855 Loss: 0.5683 Loss_dice: 0.2856 Loss_ce: 0.2827\n",
      "Epoch: 2 [300/877] [2054/2631] lr:0.0002853 Loss: 0.5264 Loss_dice: 0.2637 Loss_ce: 0.2627\n",
      "Epoch: 2 [301/877] [2055/2631] lr:0.0002852 Loss: 0.5341 Loss_dice: 0.2818 Loss_ce: 0.2523\n",
      "Epoch: 2 [302/877] [2056/2631] lr:0.0002850 Loss: 0.5382 Loss_dice: 0.2289 Loss_ce: 0.3092\n",
      "Epoch: 2 [303/877] [2057/2631] lr:0.0002848 Loss: 0.5343 Loss_dice: 0.2644 Loss_ce: 0.2699\n",
      "Epoch: 2 [304/877] [2058/2631] lr:0.0002846 Loss: 0.5333 Loss_dice: 0.2839 Loss_ce: 0.2494\n",
      "Epoch: 2 [305/877] [2059/2631] lr:0.0002845 Loss: 0.5475 Loss_dice: 0.2821 Loss_ce: 0.2654\n",
      "Epoch: 2 [306/877] [2060/2631] lr:0.0002843 Loss: 0.5408 Loss_dice: 0.2866 Loss_ce: 0.2542\n",
      "Epoch: 2 [307/877] [2061/2631] lr:0.0002841 Loss: 0.5253 Loss_dice: 0.2668 Loss_ce: 0.2585\n",
      "Epoch: 2 [308/877] [2062/2631] lr:0.0002839 Loss: 0.5438 Loss_dice: 0.2810 Loss_ce: 0.2629\n",
      "Epoch: 2 [309/877] [2063/2631] lr:0.0002837 Loss: 0.5683 Loss_dice: 0.2967 Loss_ce: 0.2716\n",
      "Epoch: 2 [310/877] [2064/2631] lr:0.0002836 Loss: 0.5440 Loss_dice: 0.2878 Loss_ce: 0.2562\n",
      "Epoch: 2 [311/877] [2065/2631] lr:0.0002834 Loss: 0.5366 Loss_dice: 0.2898 Loss_ce: 0.2469\n",
      "Epoch: 2 [312/877] [2066/2631] lr:0.0002832 Loss: 0.5309 Loss_dice: 0.2699 Loss_ce: 0.2609\n",
      "Epoch: 2 [313/877] [2067/2631] lr:0.0002830 Loss: 0.5568 Loss_dice: 0.2990 Loss_ce: 0.2577\n",
      "Epoch: 2 [314/877] [2068/2631] lr:0.0002829 Loss: 0.5784 Loss_dice: 0.2923 Loss_ce: 0.2860\n",
      "Epoch: 2 [315/877] [2069/2631] lr:0.0002827 Loss: 0.5272 Loss_dice: 0.2626 Loss_ce: 0.2646\n",
      "Epoch: 2 [316/877] [2070/2631] lr:0.0002825 Loss: 0.5673 Loss_dice: 0.2983 Loss_ce: 0.2690\n",
      "Epoch: 2 [317/877] [2071/2631] lr:0.0002823 Loss: 0.5278 Loss_dice: 0.2878 Loss_ce: 0.2400\n",
      "Epoch: 2 [318/877] [2072/2631] lr:0.0002822 Loss: 0.5407 Loss_dice: 0.2832 Loss_ce: 0.2575\n",
      "Epoch: 2 [319/877] [2073/2631] lr:0.0002820 Loss: 0.5565 Loss_dice: 0.2959 Loss_ce: 0.2606\n",
      "Epoch: 2 [320/877] [2074/2631] lr:0.0002818 Loss: 0.5615 Loss_dice: 0.2943 Loss_ce: 0.2672\n",
      "Epoch: 2 [321/877] [2075/2631] lr:0.0002816 Loss: 0.6034 Loss_dice: 0.2979 Loss_ce: 0.3055\n",
      "Epoch: 2 [322/877] [2076/2631] lr:0.0002814 Loss: 0.5355 Loss_dice: 0.2940 Loss_ce: 0.2415\n",
      "Epoch: 2 [323/877] [2077/2631] lr:0.0002813 Loss: 0.5558 Loss_dice: 0.2096 Loss_ce: 0.3462\n",
      "Epoch: 2 [324/877] [2078/2631] lr:0.0002811 Loss: 0.5540 Loss_dice: 0.2976 Loss_ce: 0.2564\n",
      "Epoch: 2 [325/877] [2079/2631] lr:0.0002809 Loss: 0.5290 Loss_dice: 0.2941 Loss_ce: 0.2349\n",
      "Epoch: 2 [326/877] [2080/2631] lr:0.0002807 Loss: 0.5276 Loss_dice: 0.2852 Loss_ce: 0.2424\n",
      "Epoch: 2 [327/877] [2081/2631] lr:0.0002806 Loss: 0.5333 Loss_dice: 0.2923 Loss_ce: 0.2410\n",
      "Epoch: 2 [328/877] [2082/2631] lr:0.0002804 Loss: 0.5291 Loss_dice: 0.2953 Loss_ce: 0.2337\n",
      "Epoch: 2 [329/877] [2083/2631] lr:0.0002802 Loss: 0.5305 Loss_dice: 0.2558 Loss_ce: 0.2746\n",
      "Epoch: 2 [330/877] [2084/2631] lr:0.0002800 Loss: 0.5289 Loss_dice: 0.2853 Loss_ce: 0.2435\n",
      "Epoch: 2 [331/877] [2085/2631] lr:0.0002798 Loss: 0.5238 Loss_dice: 0.2895 Loss_ce: 0.2343\n",
      "Epoch: 2 [332/877] [2086/2631] lr:0.0002797 Loss: 0.5650 Loss_dice: 0.2940 Loss_ce: 0.2710\n",
      "Epoch: 2 [333/877] [2087/2631] lr:0.0002795 Loss: 0.5282 Loss_dice: 0.2576 Loss_ce: 0.2706\n",
      "Epoch: 2 [334/877] [2088/2631] lr:0.0002793 Loss: 0.5457 Loss_dice: 0.2929 Loss_ce: 0.2528\n",
      "Epoch: 2 [335/877] [2089/2631] lr:0.0002791 Loss: 0.5790 Loss_dice: 0.2981 Loss_ce: 0.2809\n",
      "Epoch: 2 [336/877] [2090/2631] lr:0.0002790 Loss: 0.5549 Loss_dice: 0.2970 Loss_ce: 0.2580\n",
      "Epoch: 2 [337/877] [2091/2631] lr:0.0002788 Loss: 0.5379 Loss_dice: 0.2946 Loss_ce: 0.2433\n",
      "Epoch: 2 [338/877] [2092/2631] lr:0.0002786 Loss: 0.5235 Loss_dice: 0.2879 Loss_ce: 0.2357\n",
      "Epoch: 2 [339/877] [2093/2631] lr:0.0002784 Loss: 0.5830 Loss_dice: 0.3028 Loss_ce: 0.2803\n",
      "Epoch: 2 [340/877] [2094/2631] lr:0.0002782 Loss: 0.5297 Loss_dice: 0.2631 Loss_ce: 0.2666\n",
      "Epoch: 2 [341/877] [2095/2631] lr:0.0002781 Loss: 0.5374 Loss_dice: 0.2976 Loss_ce: 0.2398\n",
      "Epoch: 2 [342/877] [2096/2631] lr:0.0002779 Loss: 0.5632 Loss_dice: 0.2844 Loss_ce: 0.2788\n",
      "Epoch: 2 [343/877] [2097/2631] lr:0.0002777 Loss: 0.5299 Loss_dice: 0.2895 Loss_ce: 0.2404\n",
      "Epoch: 2 [344/877] [2098/2631] lr:0.0002775 Loss: 0.5565 Loss_dice: 0.2943 Loss_ce: 0.2622\n",
      "Epoch: 2 [345/877] [2099/2631] lr:0.0002773 Loss: 0.5285 Loss_dice: 0.2688 Loss_ce: 0.2597\n",
      "Epoch: 2 [346/877] [2100/2631] lr:0.0002772 Loss: 0.5394 Loss_dice: 0.2705 Loss_ce: 0.2689\n",
      "Epoch: 2 [347/877] [2101/2631] lr:0.0002770 Loss: 0.5307 Loss_dice: 0.2718 Loss_ce: 0.2589\n",
      "Epoch: 2 [348/877] [2102/2631] lr:0.0002768 Loss: 0.5683 Loss_dice: 0.2987 Loss_ce: 0.2696\n",
      "Epoch: 2 [349/877] [2103/2631] lr:0.0002766 Loss: 0.5315 Loss_dice: 0.2876 Loss_ce: 0.2438\n",
      "Epoch: 2 [350/877] [2104/2631] lr:0.0002765 Loss: 0.5411 Loss_dice: 0.2935 Loss_ce: 0.2476\n",
      "Epoch: 2 [351/877] [2105/2631] lr:0.0002763 Loss: 0.5273 Loss_dice: 0.2704 Loss_ce: 0.2569\n",
      "Epoch: 2 [352/877] [2106/2631] lr:0.0002761 Loss: 0.5424 Loss_dice: 0.2960 Loss_ce: 0.2464\n",
      "Epoch: 2 [353/877] [2107/2631] lr:0.0002759 Loss: 0.6267 Loss_dice: 0.2861 Loss_ce: 0.3406\n",
      "Epoch: 2 [354/877] [2108/2631] lr:0.0002757 Loss: 0.5177 Loss_dice: 0.2834 Loss_ce: 0.2343\n",
      "Epoch: 2 [355/877] [2109/2631] lr:0.0002756 Loss: 0.6567 Loss_dice: 0.2975 Loss_ce: 0.3592\n",
      "Epoch: 2 [356/877] [2110/2631] lr:0.0002754 Loss: 0.5429 Loss_dice: 0.2920 Loss_ce: 0.2508\n",
      "Epoch: 2 [357/877] [2111/2631] lr:0.0002752 Loss: 0.5244 Loss_dice: 0.2728 Loss_ce: 0.2516\n",
      "Epoch: 2 [358/877] [2112/2631] lr:0.0002750 Loss: 0.5336 Loss_dice: 0.2918 Loss_ce: 0.2418\n",
      "Epoch: 2 [359/877] [2113/2631] lr:0.0002749 Loss: 0.5396 Loss_dice: 0.2922 Loss_ce: 0.2474\n",
      "Epoch: 2 [360/877] [2114/2631] lr:0.0002747 Loss: 0.5232 Loss_dice: 0.2712 Loss_ce: 0.2520\n",
      "Epoch: 2 [361/877] [2115/2631] lr:0.0002745 Loss: 0.5661 Loss_dice: 0.2971 Loss_ce: 0.2690\n",
      "Epoch: 2 [362/877] [2116/2631] lr:0.0002743 Loss: 0.5414 Loss_dice: 0.2792 Loss_ce: 0.2622\n",
      "Epoch: 2 [363/877] [2117/2631] lr:0.0002741 Loss: 0.5746 Loss_dice: 0.2002 Loss_ce: 0.3744\n",
      "Epoch: 2 [364/877] [2118/2631] lr:0.0002740 Loss: 0.5363 Loss_dice: 0.2465 Loss_ce: 0.2898\n",
      "Epoch: 2 [365/877] [2119/2631] lr:0.0002738 Loss: 0.5222 Loss_dice: 0.2609 Loss_ce: 0.2613\n",
      "Epoch: 2 [366/877] [2120/2631] lr:0.0002736 Loss: 0.5615 Loss_dice: 0.2985 Loss_ce: 0.2631\n",
      "Epoch: 2 [367/877] [2121/2631] lr:0.0002734 Loss: 0.5333 Loss_dice: 0.2602 Loss_ce: 0.2731\n",
      "Epoch: 2 [368/877] [2122/2631] lr:0.0002733 Loss: 0.5263 Loss_dice: 0.2858 Loss_ce: 0.2405\n",
      "Epoch: 2 [369/877] [2123/2631] lr:0.0002731 Loss: 0.5368 Loss_dice: 0.2587 Loss_ce: 0.2781\n",
      "Epoch: 2 [370/877] [2124/2631] lr:0.0002729 Loss: 0.5429 Loss_dice: 0.2913 Loss_ce: 0.2517\n",
      "Epoch: 2 [371/877] [2125/2631] lr:0.0002727 Loss: 0.5374 Loss_dice: 0.2715 Loss_ce: 0.2658\n",
      "Epoch: 2 [372/877] [2126/2631] lr:0.0002725 Loss: 0.5475 Loss_dice: 0.2835 Loss_ce: 0.2640\n",
      "Epoch: 2 [373/877] [2127/2631] lr:0.0002724 Loss: 0.5401 Loss_dice: 0.2921 Loss_ce: 0.2480\n",
      "Epoch: 2 [374/877] [2128/2631] lr:0.0002722 Loss: 0.5358 Loss_dice: 0.2929 Loss_ce: 0.2428\n",
      "Epoch: 2 [375/877] [2129/2631] lr:0.0002720 Loss: 0.5289 Loss_dice: 0.2481 Loss_ce: 0.2807\n",
      "Epoch: 2 [376/877] [2130/2631] lr:0.0002718 Loss: 0.5315 Loss_dice: 0.2558 Loss_ce: 0.2758\n",
      "Epoch: 2 [377/877] [2131/2631] lr:0.0002716 Loss: 0.6166 Loss_dice: 0.2936 Loss_ce: 0.3230\n",
      "Epoch: 2 [378/877] [2132/2631] lr:0.0002715 Loss: 0.5465 Loss_dice: 0.2229 Loss_ce: 0.3237\n",
      "Epoch: 2 [379/877] [2133/2631] lr:0.0002713 Loss: 0.5537 Loss_dice: 0.2863 Loss_ce: 0.2674\n",
      "Epoch: 2 [380/877] [2134/2631] lr:0.0002711 Loss: 0.5258 Loss_dice: 0.2615 Loss_ce: 0.2643\n",
      "Epoch: 2 [381/877] [2135/2631] lr:0.0002709 Loss: 0.5351 Loss_dice: 0.2554 Loss_ce: 0.2797\n",
      "Epoch: 2 [382/877] [2136/2631] lr:0.0002708 Loss: 0.5275 Loss_dice: 0.2586 Loss_ce: 0.2689\n",
      "Epoch: 2 [383/877] [2137/2631] lr:0.0002706 Loss: 0.5263 Loss_dice: 0.2886 Loss_ce: 0.2378\n",
      "Epoch: 2 [384/877] [2138/2631] lr:0.0002704 Loss: 0.5634 Loss_dice: 0.2938 Loss_ce: 0.2697\n",
      "Epoch: 2 [385/877] [2139/2631] lr:0.0002702 Loss: 0.5647 Loss_dice: 0.2991 Loss_ce: 0.2655\n",
      "Epoch: 2 [386/877] [2140/2631] lr:0.0002700 Loss: 1.1778 Loss_dice: 0.4476 Loss_ce: 0.7303\n",
      "Epoch: 2 [387/877] [2141/2631] lr:0.0002699 Loss: 0.5455 Loss_dice: 0.2383 Loss_ce: 0.3072\n",
      "Epoch: 2 [388/877] [2142/2631] lr:0.0002697 Loss: 0.5681 Loss_dice: 0.3004 Loss_ce: 0.2677\n",
      "Epoch: 2 [389/877] [2143/2631] lr:0.0002695 Loss: 0.5463 Loss_dice: 0.2507 Loss_ce: 0.2956\n",
      "Epoch: 2 [390/877] [2144/2631] lr:0.0002693 Loss: 0.5319 Loss_dice: 0.2346 Loss_ce: 0.2973\n",
      "Epoch: 2 [391/877] [2145/2631] lr:0.0002691 Loss: 0.5408 Loss_dice: 0.2700 Loss_ce: 0.2709\n",
      "Epoch: 2 [392/877] [2146/2631] lr:0.0002690 Loss: 0.5492 Loss_dice: 0.2780 Loss_ce: 0.2712\n",
      "Epoch: 2 [393/877] [2147/2631] lr:0.0002688 Loss: 0.5398 Loss_dice: 0.2320 Loss_ce: 0.3079\n",
      "Epoch: 2 [394/877] [2148/2631] lr:0.0002686 Loss: 0.5850 Loss_dice: 0.3112 Loss_ce: 0.2738\n",
      "Epoch: 2 [395/877] [2149/2631] lr:0.0002684 Loss: 0.5845 Loss_dice: 0.2917 Loss_ce: 0.2927\n",
      "Epoch: 2 [396/877] [2150/2631] lr:0.0002683 Loss: 0.5509 Loss_dice: 0.3002 Loss_ce: 0.2507\n",
      "Epoch: 2 [397/877] [2151/2631] lr:0.0002681 Loss: 0.6591 Loss_dice: 0.2999 Loss_ce: 0.3592\n",
      "Epoch: 2 [398/877] [2152/2631] lr:0.0002679 Loss: 0.5298 Loss_dice: 0.2873 Loss_ce: 0.2425\n",
      "Epoch: 2 [399/877] [2153/2631] lr:0.0002677 Loss: 0.5725 Loss_dice: 0.3008 Loss_ce: 0.2716\n",
      "Epoch: 2 [400/877] [2154/2631] lr:0.0002675 Loss: 0.5375 Loss_dice: 0.2924 Loss_ce: 0.2450\n",
      "Epoch: 2 [401/877] [2155/2631] lr:0.0002674 Loss: 0.5413 Loss_dice: 0.2940 Loss_ce: 0.2473\n",
      "Epoch: 2 [402/877] [2156/2631] lr:0.0002672 Loss: 0.5464 Loss_dice: 0.2919 Loss_ce: 0.2545\n",
      "Epoch: 2 [403/877] [2157/2631] lr:0.0002670 Loss: 0.5726 Loss_dice: 0.2834 Loss_ce: 0.2892\n",
      "Epoch: 2 [404/877] [2158/2631] lr:0.0002668 Loss: 0.5445 Loss_dice: 0.2587 Loss_ce: 0.2857\n",
      "Epoch: 2 [405/877] [2159/2631] lr:0.0002666 Loss: 0.5799 Loss_dice: 0.2972 Loss_ce: 0.2827\n",
      "Epoch: 2 [406/877] [2160/2631] lr:0.0002665 Loss: 0.5750 Loss_dice: 0.2898 Loss_ce: 0.2851\n",
      "Epoch: 2 [407/877] [2161/2631] lr:0.0002663 Loss: 0.5622 Loss_dice: 0.2874 Loss_ce: 0.2748\n",
      "Epoch: 2 [408/877] [2162/2631] lr:0.0002661 Loss: 0.5305 Loss_dice: 0.2923 Loss_ce: 0.2382\n",
      "Epoch: 2 [409/877] [2163/2631] lr:0.0002659 Loss: 0.5351 Loss_dice: 0.2695 Loss_ce: 0.2656\n",
      "Epoch: 2 [410/877] [2164/2631] lr:0.0002658 Loss: 0.5294 Loss_dice: 0.2726 Loss_ce: 0.2568\n",
      "Epoch: 2 [411/877] [2165/2631] lr:0.0002656 Loss: 0.5373 Loss_dice: 0.2864 Loss_ce: 0.2509\n",
      "Epoch: 2 [412/877] [2166/2631] lr:0.0002654 Loss: 0.5653 Loss_dice: 0.2954 Loss_ce: 0.2699\n",
      "Epoch: 2 [413/877] [2167/2631] lr:0.0002652 Loss: 0.6137 Loss_dice: 0.3063 Loss_ce: 0.3074\n",
      "Epoch: 2 [414/877] [2168/2631] lr:0.0002650 Loss: 0.5428 Loss_dice: 0.2911 Loss_ce: 0.2517\n",
      "Epoch: 2 [415/877] [2169/2631] lr:0.0002649 Loss: 0.5330 Loss_dice: 0.2293 Loss_ce: 0.3037\n",
      "Epoch: 2 [416/877] [2170/2631] lr:0.0002647 Loss: 0.5330 Loss_dice: 0.2723 Loss_ce: 0.2607\n",
      "Epoch: 2 [417/877] [2171/2631] lr:0.0002645 Loss: 0.5328 Loss_dice: 0.2870 Loss_ce: 0.2458\n",
      "Epoch: 2 [418/877] [2172/2631] lr:0.0002643 Loss: 0.5279 Loss_dice: 0.2519 Loss_ce: 0.2760\n",
      "Epoch: 2 [419/877] [2173/2631] lr:0.0002641 Loss: 0.5274 Loss_dice: 0.2549 Loss_ce: 0.2726\n",
      "Epoch: 2 [420/877] [2174/2631] lr:0.0002640 Loss: 0.5296 Loss_dice: 0.2780 Loss_ce: 0.2516\n",
      "Epoch: 2 [421/877] [2175/2631] lr:0.0002638 Loss: 0.5536 Loss_dice: 0.2874 Loss_ce: 0.2663\n",
      "Epoch: 2 [422/877] [2176/2631] lr:0.0002636 Loss: 0.5664 Loss_dice: 0.2921 Loss_ce: 0.2743\n",
      "Epoch: 2 [423/877] [2177/2631] lr:0.0002634 Loss: 0.5415 Loss_dice: 0.2870 Loss_ce: 0.2545\n",
      "Epoch: 2 [424/877] [2178/2631] lr:0.0002632 Loss: 0.5324 Loss_dice: 0.2758 Loss_ce: 0.2566\n",
      "Epoch: 2 [425/877] [2179/2631] lr:0.0002631 Loss: 0.5479 Loss_dice: 0.2917 Loss_ce: 0.2562\n",
      "Epoch: 2 [426/877] [2180/2631] lr:0.0002629 Loss: 0.5265 Loss_dice: 0.2690 Loss_ce: 0.2574\n",
      "Epoch: 2 [427/877] [2181/2631] lr:0.0002627 Loss: 0.5363 Loss_dice: 0.2954 Loss_ce: 0.2409\n",
      "Epoch: 2 [428/877] [2182/2631] lr:0.0002625 Loss: 0.5354 Loss_dice: 0.2881 Loss_ce: 0.2473\n",
      "Epoch: 2 [429/877] [2183/2631] lr:0.0002624 Loss: 0.5283 Loss_dice: 0.2809 Loss_ce: 0.2474\n",
      "Epoch: 2 [430/877] [2184/2631] lr:0.0002622 Loss: 0.5250 Loss_dice: 0.2691 Loss_ce: 0.2558\n",
      "Epoch: 2 [431/877] [2185/2631] lr:0.0002620 Loss: 0.5317 Loss_dice: 0.2942 Loss_ce: 0.2376\n",
      "Epoch: 2 [432/877] [2186/2631] lr:0.0002618 Loss: 0.5379 Loss_dice: 0.2938 Loss_ce: 0.2442\n",
      "Epoch: 2 [433/877] [2187/2631] lr:0.0002616 Loss: 0.5978 Loss_dice: 0.2540 Loss_ce: 0.3438\n",
      "Epoch: 2 [434/877] [2188/2631] lr:0.0002615 Loss: 0.5333 Loss_dice: 0.2927 Loss_ce: 0.2405\n",
      "Epoch: 2 [435/877] [2189/2631] lr:0.0002613 Loss: 0.5951 Loss_dice: 0.2606 Loss_ce: 0.3345\n",
      "Epoch: 2 [436/877] [2190/2631] lr:0.0002611 Loss: 0.5264 Loss_dice: 0.2889 Loss_ce: 0.2374\n",
      "Epoch: 2 [437/877] [2191/2631] lr:0.0002609 Loss: 0.5436 Loss_dice: 0.2960 Loss_ce: 0.2476\n",
      "Epoch: 2 [438/877] [2192/2631] lr:0.0002607 Loss: 0.5316 Loss_dice: 0.2654 Loss_ce: 0.2661\n",
      "Epoch: 2 [439/877] [2193/2631] lr:0.0002606 Loss: 0.5552 Loss_dice: 0.2872 Loss_ce: 0.2679\n",
      "Epoch: 2 [440/877] [2194/2631] lr:0.0002604 Loss: 0.5561 Loss_dice: 0.2510 Loss_ce: 0.3050\n",
      "Epoch: 2 [441/877] [2195/2631] lr:0.0002602 Loss: 0.6255 Loss_dice: 0.3069 Loss_ce: 0.3186\n",
      "Epoch: 2 [442/877] [2196/2631] lr:0.0002600 Loss: 0.5450 Loss_dice: 0.2682 Loss_ce: 0.2767\n",
      "Epoch: 2 [443/877] [2197/2631] lr:0.0002598 Loss: 0.5331 Loss_dice: 0.2880 Loss_ce: 0.2450\n",
      "Epoch: 2 [444/877] [2198/2631] lr:0.0002597 Loss: 0.5927 Loss_dice: 0.2992 Loss_ce: 0.2935\n",
      "Epoch: 2 [445/877] [2199/2631] lr:0.0002595 Loss: 0.5985 Loss_dice: 0.2819 Loss_ce: 0.3167\n",
      "Epoch: 2 [446/877] [2200/2631] lr:0.0002593 Loss: 0.6393 Loss_dice: 0.2965 Loss_ce: 0.3428\n",
      "Epoch: 2 [447/877] [2201/2631] lr:0.0002591 Loss: 0.5300 Loss_dice: 0.2875 Loss_ce: 0.2425\n",
      "Epoch: 2 [448/877] [2202/2631] lr:0.0002590 Loss: 0.5272 Loss_dice: 0.2621 Loss_ce: 0.2652\n",
      "Epoch: 2 [449/877] [2203/2631] lr:0.0002588 Loss: 0.5331 Loss_dice: 0.2873 Loss_ce: 0.2458\n",
      "Epoch: 2 [450/877] [2204/2631] lr:0.0002586 Loss: 0.5342 Loss_dice: 0.2823 Loss_ce: 0.2519\n",
      "Epoch: 2 [451/877] [2205/2631] lr:0.0002584 Loss: 0.5376 Loss_dice: 0.2826 Loss_ce: 0.2550\n",
      "Epoch: 2 [452/877] [2206/2631] lr:0.0002582 Loss: 0.5203 Loss_dice: 0.2610 Loss_ce: 0.2593\n",
      "Epoch: 2 [453/877] [2207/2631] lr:0.0002581 Loss: 0.5266 Loss_dice: 0.2344 Loss_ce: 0.2921\n",
      "Epoch: 2 [454/877] [2208/2631] lr:0.0002579 Loss: 0.5741 Loss_dice: 0.3007 Loss_ce: 0.2734\n",
      "Epoch: 2 [455/877] [2209/2631] lr:0.0002577 Loss: 0.7017 Loss_dice: 0.3266 Loss_ce: 0.3751\n",
      "Epoch: 2 [456/877] [2210/2631] lr:0.0002575 Loss: 0.5380 Loss_dice: 0.2873 Loss_ce: 0.2507\n",
      "Epoch: 2 [457/877] [2211/2631] lr:0.0002573 Loss: 0.5441 Loss_dice: 0.2889 Loss_ce: 0.2552\n",
      "Epoch: 2 [458/877] [2212/2631] lr:0.0002572 Loss: 0.5341 Loss_dice: 0.2899 Loss_ce: 0.2441\n",
      "Epoch: 2 [459/877] [2213/2631] lr:0.0002570 Loss: 0.5557 Loss_dice: 0.2977 Loss_ce: 0.2579\n",
      "Epoch: 2 [460/877] [2214/2631] lr:0.0002568 Loss: 0.5520 Loss_dice: 0.3022 Loss_ce: 0.2498\n",
      "Epoch: 2 [461/877] [2215/2631] lr:0.0002566 Loss: 0.5420 Loss_dice: 0.2987 Loss_ce: 0.2433\n",
      "Epoch: 2 [462/877] [2216/2631] lr:0.0002564 Loss: 0.5334 Loss_dice: 0.2767 Loss_ce: 0.2567\n",
      "Epoch: 2 [463/877] [2217/2631] lr:0.0002563 Loss: 0.5572 Loss_dice: 0.2868 Loss_ce: 0.2705\n",
      "Epoch: 2 [464/877] [2218/2631] lr:0.0002561 Loss: 0.5464 Loss_dice: 0.2906 Loss_ce: 0.2558\n",
      "Epoch: 2 [465/877] [2219/2631] lr:0.0002559 Loss: 0.5392 Loss_dice: 0.2685 Loss_ce: 0.2706\n",
      "Epoch: 2 [466/877] [2220/2631] lr:0.0002557 Loss: 0.5464 Loss_dice: 0.2845 Loss_ce: 0.2619\n",
      "Epoch: 2 [467/877] [2221/2631] lr:0.0002556 Loss: 0.5880 Loss_dice: 0.2865 Loss_ce: 0.3016\n",
      "Epoch: 2 [468/877] [2222/2631] lr:0.0002554 Loss: 0.5515 Loss_dice: 0.2903 Loss_ce: 0.2611\n",
      "Epoch: 2 [469/877] [2223/2631] lr:0.0002552 Loss: 0.5305 Loss_dice: 0.2483 Loss_ce: 0.2822\n",
      "Epoch: 2 [470/877] [2224/2631] lr:0.0002550 Loss: 0.5682 Loss_dice: 0.2976 Loss_ce: 0.2707\n",
      "Epoch: 2 [471/877] [2225/2631] lr:0.0002548 Loss: 0.5328 Loss_dice: 0.2758 Loss_ce: 0.2570\n",
      "Epoch: 2 [472/877] [2226/2631] lr:0.0002547 Loss: 0.5418 Loss_dice: 0.2913 Loss_ce: 0.2504\n",
      "Epoch: 2 [473/877] [2227/2631] lr:0.0002545 Loss: 0.5323 Loss_dice: 0.2909 Loss_ce: 0.2414\n",
      "Epoch: 2 [474/877] [2228/2631] lr:0.0002543 Loss: 0.5410 Loss_dice: 0.2917 Loss_ce: 0.2493\n",
      "Epoch: 2 [475/877] [2229/2631] lr:0.0002541 Loss: 0.5325 Loss_dice: 0.2781 Loss_ce: 0.2543\n",
      "Epoch: 2 [476/877] [2230/2631] lr:0.0002539 Loss: 0.5982 Loss_dice: 0.2884 Loss_ce: 0.3098\n",
      "Epoch: 2 [477/877] [2231/2631] lr:0.0002538 Loss: 0.5223 Loss_dice: 0.2670 Loss_ce: 0.2553\n",
      "Epoch: 2 [478/877] [2232/2631] lr:0.0002536 Loss: 0.5386 Loss_dice: 0.2899 Loss_ce: 0.2487\n",
      "Epoch: 2 [479/877] [2233/2631] lr:0.0002534 Loss: 0.5681 Loss_dice: 0.3011 Loss_ce: 0.2669\n",
      "Epoch: 2 [480/877] [2234/2631] lr:0.0002532 Loss: 0.5508 Loss_dice: 0.2862 Loss_ce: 0.2647\n",
      "Epoch: 2 [481/877] [2235/2631] lr:0.0002530 Loss: 0.5443 Loss_dice: 0.2916 Loss_ce: 0.2526\n",
      "Epoch: 2 [482/877] [2236/2631] lr:0.0002529 Loss: 0.5589 Loss_dice: 0.2935 Loss_ce: 0.2654\n",
      "Epoch: 2 [483/877] [2237/2631] lr:0.0002527 Loss: 0.5729 Loss_dice: 0.2559 Loss_ce: 0.3170\n",
      "Epoch: 2 [484/877] [2238/2631] lr:0.0002525 Loss: 0.5452 Loss_dice: 0.2273 Loss_ce: 0.3180\n",
      "Epoch: 2 [485/877] [2239/2631] lr:0.0002523 Loss: 0.5276 Loss_dice: 0.2782 Loss_ce: 0.2494\n",
      "Epoch: 2 [486/877] [2240/2631] lr:0.0002521 Loss: 0.5759 Loss_dice: 0.2998 Loss_ce: 0.2762\n",
      "Epoch: 2 [487/877] [2241/2631] lr:0.0002520 Loss: 0.5248 Loss_dice: 0.2814 Loss_ce: 0.2434\n",
      "Epoch: 2 [488/877] [2242/2631] lr:0.0002518 Loss: 0.5322 Loss_dice: 0.2815 Loss_ce: 0.2506\n",
      "Epoch: 2 [489/877] [2243/2631] lr:0.0002516 Loss: 0.5412 Loss_dice: 0.2741 Loss_ce: 0.2671\n",
      "Epoch: 2 [490/877] [2244/2631] lr:0.0002514 Loss: 0.5330 Loss_dice: 0.2957 Loss_ce: 0.2373\n",
      "Epoch: 2 [491/877] [2245/2631] lr:0.0002513 Loss: 0.5476 Loss_dice: 0.2929 Loss_ce: 0.2547\n",
      "Epoch: 2 [492/877] [2246/2631] lr:0.0002511 Loss: 0.5398 Loss_dice: 0.2298 Loss_ce: 0.3100\n",
      "Epoch: 2 [493/877] [2247/2631] lr:0.0002509 Loss: 0.5327 Loss_dice: 0.2929 Loss_ce: 0.2399\n",
      "Epoch: 2 [494/877] [2248/2631] lr:0.0002507 Loss: 0.5274 Loss_dice: 0.2909 Loss_ce: 0.2365\n",
      "Epoch: 2 [495/877] [2249/2631] lr:0.0002505 Loss: 0.6918 Loss_dice: 0.2888 Loss_ce: 0.4030\n",
      "Epoch: 2 [496/877] [2250/2631] lr:0.0002504 Loss: 0.5339 Loss_dice: 0.2941 Loss_ce: 0.2398\n",
      "Epoch: 2 [497/877] [2251/2631] lr:0.0002502 Loss: 0.5399 Loss_dice: 0.2945 Loss_ce: 0.2454\n",
      "Epoch: 2 [498/877] [2252/2631] lr:0.0002500 Loss: 0.5335 Loss_dice: 0.2840 Loss_ce: 0.2495\n",
      "Epoch: 2 [499/877] [2253/2631] lr:0.0002498 Loss: 0.5344 Loss_dice: 0.2881 Loss_ce: 0.2463\n",
      "Epoch: 2 [500/877] [2254/2631] lr:0.0002496 Loss: 0.5470 Loss_dice: 0.2998 Loss_ce: 0.2472\n",
      "Epoch: 2 [501/877] [2255/2631] lr:0.0002495 Loss: 0.5511 Loss_dice: 0.3015 Loss_ce: 0.2496\n",
      "Epoch: 2 [502/877] [2256/2631] lr:0.0002493 Loss: 0.6395 Loss_dice: 0.3132 Loss_ce: 0.3263\n",
      "Epoch: 2 [503/877] [2257/2631] lr:0.0002491 Loss: 0.5292 Loss_dice: 0.2778 Loss_ce: 0.2515\n",
      "Epoch: 2 [504/877] [2258/2631] lr:0.0002489 Loss: 0.5396 Loss_dice: 0.2908 Loss_ce: 0.2488\n",
      "Epoch: 2 [505/877] [2259/2631] lr:0.0002487 Loss: 0.5430 Loss_dice: 0.2997 Loss_ce: 0.2433\n",
      "Epoch: 2 [506/877] [2260/2631] lr:0.0002486 Loss: 0.5576 Loss_dice: 0.2766 Loss_ce: 0.2810\n",
      "Epoch: 2 [507/877] [2261/2631] lr:0.0002484 Loss: 0.5308 Loss_dice: 0.2892 Loss_ce: 0.2416\n",
      "Epoch: 2 [508/877] [2262/2631] lr:0.0002482 Loss: 0.5221 Loss_dice: 0.2569 Loss_ce: 0.2652\n",
      "Epoch: 2 [509/877] [2263/2631] lr:0.0002480 Loss: 0.5363 Loss_dice: 0.2968 Loss_ce: 0.2396\n",
      "Epoch: 2 [510/877] [2264/2631] lr:0.0002479 Loss: 0.5463 Loss_dice: 0.2954 Loss_ce: 0.2509\n",
      "Epoch: 2 [511/877] [2265/2631] lr:0.0002477 Loss: 0.5375 Loss_dice: 0.2938 Loss_ce: 0.2437\n",
      "Epoch: 2 [512/877] [2266/2631] lr:0.0002475 Loss: 0.5525 Loss_dice: 0.2985 Loss_ce: 0.2541\n",
      "Epoch: 2 [513/877] [2267/2631] lr:0.0002473 Loss: 0.5331 Loss_dice: 0.2565 Loss_ce: 0.2767\n",
      "Epoch: 2 [514/877] [2268/2631] lr:0.0002471 Loss: 0.5281 Loss_dice: 0.2954 Loss_ce: 0.2327\n",
      "Epoch: 2 [515/877] [2269/2631] lr:0.0002470 Loss: 0.5457 Loss_dice: 0.2999 Loss_ce: 0.2458\n",
      "Epoch: 2 [516/877] [2270/2631] lr:0.0002468 Loss: 0.5281 Loss_dice: 0.2765 Loss_ce: 0.2516\n",
      "Epoch: 2 [517/877] [2271/2631] lr:0.0002466 Loss: 0.5457 Loss_dice: 0.2989 Loss_ce: 0.2468\n",
      "Epoch: 2 [518/877] [2272/2631] lr:0.0002464 Loss: 0.5546 Loss_dice: 0.3090 Loss_ce: 0.2456\n",
      "Epoch: 2 [519/877] [2273/2631] lr:0.0002462 Loss: 0.5206 Loss_dice: 0.2838 Loss_ce: 0.2368\n",
      "Epoch: 2 [520/877] [2274/2631] lr:0.0002461 Loss: 0.5483 Loss_dice: 0.3019 Loss_ce: 0.2464\n",
      "Epoch: 2 [521/877] [2275/2631] lr:0.0002459 Loss: 0.5584 Loss_dice: 0.3044 Loss_ce: 0.2540\n",
      "Epoch: 2 [522/877] [2276/2631] lr:0.0002457 Loss: 0.5345 Loss_dice: 0.3005 Loss_ce: 0.2340\n",
      "Epoch: 2 [523/877] [2277/2631] lr:0.0002455 Loss: 0.5508 Loss_dice: 0.2893 Loss_ce: 0.2615\n",
      "Epoch: 2 [524/877] [2278/2631] lr:0.0002453 Loss: 1.5345 Loss_dice: 0.5367 Loss_ce: 0.9978\n",
      "Epoch: 2 [525/877] [2279/2631] lr:0.0002452 Loss: 0.5287 Loss_dice: 0.2510 Loss_ce: 0.2777\n",
      "Epoch: 2 [526/877] [2280/2631] lr:0.0002450 Loss: 0.5469 Loss_dice: 0.2442 Loss_ce: 0.3027\n",
      "Epoch: 2 [527/877] [2281/2631] lr:0.0002448 Loss: 0.5686 Loss_dice: 0.2901 Loss_ce: 0.2785\n",
      "Epoch: 2 [528/877] [2282/2631] lr:0.0002446 Loss: 0.5493 Loss_dice: 0.2830 Loss_ce: 0.2663\n",
      "Epoch: 2 [529/877] [2283/2631] lr:0.0002444 Loss: 0.5375 Loss_dice: 0.2715 Loss_ce: 0.2659\n",
      "Epoch: 2 [530/877] [2284/2631] lr:0.0002443 Loss: 0.5376 Loss_dice: 0.2629 Loss_ce: 0.2747\n",
      "Epoch: 2 [531/877] [2285/2631] lr:0.0002441 Loss: 0.5375 Loss_dice: 0.2886 Loss_ce: 0.2489\n",
      "Epoch: 2 [532/877] [2286/2631] lr:0.0002439 Loss: 0.5280 Loss_dice: 0.2513 Loss_ce: 0.2767\n",
      "Epoch: 2 [533/877] [2287/2631] lr:0.0002437 Loss: 0.5381 Loss_dice: 0.2901 Loss_ce: 0.2480\n",
      "Epoch: 2 [534/877] [2288/2631] lr:0.0002436 Loss: 0.5277 Loss_dice: 0.2811 Loss_ce: 0.2466\n",
      "Epoch: 2 [535/877] [2289/2631] lr:0.0002434 Loss: 0.5334 Loss_dice: 0.2875 Loss_ce: 0.2459\n",
      "Epoch: 2 [536/877] [2290/2631] lr:0.0002432 Loss: 0.5362 Loss_dice: 0.2847 Loss_ce: 0.2515\n",
      "Epoch: 2 [537/877] [2291/2631] lr:0.0002430 Loss: 0.5308 Loss_dice: 0.2879 Loss_ce: 0.2429\n",
      "Epoch: 2 [538/877] [2292/2631] lr:0.0002428 Loss: 0.5336 Loss_dice: 0.2485 Loss_ce: 0.2850\n",
      "Epoch: 2 [539/877] [2293/2631] lr:0.0002427 Loss: 0.5442 Loss_dice: 0.2763 Loss_ce: 0.2679\n",
      "Epoch: 2 [540/877] [2294/2631] lr:0.0002425 Loss: 0.5362 Loss_dice: 0.2853 Loss_ce: 0.2509\n",
      "Epoch: 2 [541/877] [2295/2631] lr:0.0002423 Loss: 0.7461 Loss_dice: 0.3091 Loss_ce: 0.4370\n",
      "Epoch: 2 [542/877] [2296/2631] lr:0.0002421 Loss: 0.5554 Loss_dice: 0.2867 Loss_ce: 0.2688\n",
      "Epoch: 2 [543/877] [2297/2631] lr:0.0002419 Loss: 0.5384 Loss_dice: 0.2623 Loss_ce: 0.2760\n",
      "Epoch: 2 [544/877] [2298/2631] lr:0.0002418 Loss: 0.5418 Loss_dice: 0.2425 Loss_ce: 0.2993\n",
      "Epoch: 2 [545/877] [2299/2631] lr:0.0002416 Loss: 0.5775 Loss_dice: 0.2463 Loss_ce: 0.3311\n",
      "Epoch: 2 [546/877] [2300/2631] lr:0.0002414 Loss: 0.5479 Loss_dice: 0.2117 Loss_ce: 0.3362\n",
      "Epoch: 2 [547/877] [2301/2631] lr:0.0002412 Loss: 0.5333 Loss_dice: 0.2344 Loss_ce: 0.2990\n",
      "Epoch: 2 [548/877] [2302/2631] lr:0.0002410 Loss: 0.5616 Loss_dice: 0.2717 Loss_ce: 0.2899\n",
      "Epoch: 2 [549/877] [2303/2631] lr:0.0002409 Loss: 0.5425 Loss_dice: 0.2688 Loss_ce: 0.2738\n",
      "Epoch: 2 [550/877] [2304/2631] lr:0.0002407 Loss: 0.6142 Loss_dice: 0.2898 Loss_ce: 0.3244\n",
      "Epoch: 2 [551/877] [2305/2631] lr:0.0002405 Loss: 0.6415 Loss_dice: 0.2902 Loss_ce: 0.3513\n",
      "Epoch: 2 [552/877] [2306/2631] lr:0.0002403 Loss: 0.5926 Loss_dice: 0.2825 Loss_ce: 0.3102\n",
      "Epoch: 2 [553/877] [2307/2631] lr:0.0002402 Loss: 0.5640 Loss_dice: 0.2837 Loss_ce: 0.2803\n",
      "Epoch: 2 [554/877] [2308/2631] lr:0.0002400 Loss: 0.5542 Loss_dice: 0.2640 Loss_ce: 0.2902\n",
      "Epoch: 2 [555/877] [2309/2631] lr:0.0002398 Loss: 0.5470 Loss_dice: 0.2832 Loss_ce: 0.2638\n",
      "Epoch: 2 [556/877] [2310/2631] lr:0.0002396 Loss: 0.6053 Loss_dice: 0.2913 Loss_ce: 0.3140\n",
      "Epoch: 2 [557/877] [2311/2631] lr:0.0002394 Loss: 0.6590 Loss_dice: 0.3011 Loss_ce: 0.3578\n",
      "Epoch: 2 [558/877] [2312/2631] lr:0.0002393 Loss: 0.5432 Loss_dice: 0.2688 Loss_ce: 0.2744\n",
      "Epoch: 2 [559/877] [2313/2631] lr:0.0002391 Loss: 0.5456 Loss_dice: 0.2836 Loss_ce: 0.2620\n",
      "Epoch: 2 [560/877] [2314/2631] lr:0.0002389 Loss: 0.5486 Loss_dice: 0.2659 Loss_ce: 0.2826\n",
      "Epoch: 2 [561/877] [2315/2631] lr:0.0002387 Loss: 0.5395 Loss_dice: 0.2718 Loss_ce: 0.2677\n",
      "Epoch: 2 [562/877] [2316/2631] lr:0.0002385 Loss: 0.5534 Loss_dice: 0.3036 Loss_ce: 0.2498\n",
      "Epoch: 2 [563/877] [2317/2631] lr:0.0002384 Loss: 0.5531 Loss_dice: 0.2926 Loss_ce: 0.2605\n",
      "Epoch: 2 [564/877] [2318/2631] lr:0.0002382 Loss: 0.6147 Loss_dice: 0.3029 Loss_ce: 0.3118\n",
      "Epoch: 2 [565/877] [2319/2631] lr:0.0002380 Loss: 0.5378 Loss_dice: 0.2810 Loss_ce: 0.2568\n",
      "Epoch: 2 [566/877] [2320/2631] lr:0.0002378 Loss: 0.5313 Loss_dice: 0.2896 Loss_ce: 0.2417\n",
      "Epoch: 2 [567/877] [2321/2631] lr:0.0002376 Loss: 0.5876 Loss_dice: 0.2401 Loss_ce: 0.3475\n",
      "Epoch: 2 [568/877] [2322/2631] lr:0.0002375 Loss: 0.5438 Loss_dice: 0.2856 Loss_ce: 0.2582\n",
      "Epoch: 2 [569/877] [2323/2631] lr:0.0002373 Loss: 0.5302 Loss_dice: 0.2507 Loss_ce: 0.2794\n",
      "Epoch: 2 [570/877] [2324/2631] lr:0.0002371 Loss: 0.5443 Loss_dice: 0.2914 Loss_ce: 0.2530\n",
      "Epoch: 2 [571/877] [2325/2631] lr:0.0002369 Loss: 0.5689 Loss_dice: 0.2893 Loss_ce: 0.2796\n",
      "Epoch: 2 [572/877] [2326/2631] lr:0.0002368 Loss: 0.5383 Loss_dice: 0.2640 Loss_ce: 0.2742\n",
      "Epoch: 2 [573/877] [2327/2631] lr:0.0002366 Loss: 0.5424 Loss_dice: 0.2885 Loss_ce: 0.2539\n",
      "Epoch: 2 [574/877] [2328/2631] lr:0.0002364 Loss: 0.5335 Loss_dice: 0.2826 Loss_ce: 0.2509\n",
      "Epoch: 2 [575/877] [2329/2631] lr:0.0002362 Loss: 0.5436 Loss_dice: 0.2647 Loss_ce: 0.2789\n",
      "Epoch: 2 [576/877] [2330/2631] lr:0.0002360 Loss: 0.5245 Loss_dice: 0.2759 Loss_ce: 0.2486\n",
      "Epoch: 2 [577/877] [2331/2631] lr:0.0002359 Loss: 0.5311 Loss_dice: 0.2889 Loss_ce: 0.2421\n",
      "Epoch: 2 [578/877] [2332/2631] lr:0.0002357 Loss: 0.5449 Loss_dice: 0.2878 Loss_ce: 0.2571\n",
      "Epoch: 2 [579/877] [2333/2631] lr:0.0002355 Loss: 0.5547 Loss_dice: 0.2174 Loss_ce: 0.3372\n",
      "Epoch: 2 [580/877] [2334/2631] lr:0.0002353 Loss: 0.5837 Loss_dice: 0.2887 Loss_ce: 0.2950\n",
      "Epoch: 2 [581/877] [2335/2631] lr:0.0002351 Loss: 0.5310 Loss_dice: 0.2911 Loss_ce: 0.2399\n",
      "Epoch: 2 [582/877] [2336/2631] lr:0.0002350 Loss: 0.5308 Loss_dice: 0.2836 Loss_ce: 0.2472\n",
      "Epoch: 2 [583/877] [2337/2631] lr:0.0002348 Loss: 0.5244 Loss_dice: 0.2752 Loss_ce: 0.2492\n",
      "Epoch: 2 [584/877] [2338/2631] lr:0.0002346 Loss: 0.5284 Loss_dice: 0.2913 Loss_ce: 0.2371\n",
      "Epoch: 2 [585/877] [2339/2631] lr:0.0002344 Loss: 0.5615 Loss_dice: 0.2704 Loss_ce: 0.2911\n",
      "Epoch: 2 [586/877] [2340/2631] lr:0.0002342 Loss: 0.5365 Loss_dice: 0.2565 Loss_ce: 0.2800\n",
      "Epoch: 2 [587/877] [2341/2631] lr:0.0002341 Loss: 0.5176 Loss_dice: 0.2535 Loss_ce: 0.2641\n",
      "Epoch: 2 [588/877] [2342/2631] lr:0.0002339 Loss: 0.5518 Loss_dice: 0.2978 Loss_ce: 0.2541\n",
      "Epoch: 2 [589/877] [2343/2631] lr:0.0002337 Loss: 0.5242 Loss_dice: 0.2947 Loss_ce: 0.2295\n",
      "Epoch: 2 [590/877] [2344/2631] lr:0.0002335 Loss: 0.5425 Loss_dice: 0.2836 Loss_ce: 0.2589\n",
      "Epoch: 2 [591/877] [2345/2631] lr:0.0002334 Loss: 0.5632 Loss_dice: 0.2947 Loss_ce: 0.2685\n",
      "Epoch: 2 [592/877] [2346/2631] lr:0.0002332 Loss: 0.5324 Loss_dice: 0.2480 Loss_ce: 0.2844\n",
      "Epoch: 2 [593/877] [2347/2631] lr:0.0002330 Loss: 0.5722 Loss_dice: 0.2961 Loss_ce: 0.2761\n",
      "Epoch: 2 [594/877] [2348/2631] lr:0.0002328 Loss: 0.6262 Loss_dice: 0.2940 Loss_ce: 0.3322\n",
      "Epoch: 2 [595/877] [2349/2631] lr:0.0002326 Loss: 0.5457 Loss_dice: 0.2978 Loss_ce: 0.2479\n",
      "Epoch: 2 [596/877] [2350/2631] lr:0.0002325 Loss: 0.5402 Loss_dice: 0.2955 Loss_ce: 0.2447\n",
      "Epoch: 2 [597/877] [2351/2631] lr:0.0002323 Loss: 0.5303 Loss_dice: 0.2961 Loss_ce: 0.2342\n",
      "Epoch: 2 [598/877] [2352/2631] lr:0.0002321 Loss: 0.5414 Loss_dice: 0.3036 Loss_ce: 0.2378\n",
      "Epoch: 2 [599/877] [2353/2631] lr:0.0002319 Loss: 0.5368 Loss_dice: 0.2940 Loss_ce: 0.2429\n",
      "Epoch: 2 [600/877] [2354/2631] lr:0.0002317 Loss: 0.5659 Loss_dice: 0.2490 Loss_ce: 0.3169\n",
      "Epoch: 2 [601/877] [2355/2631] lr:0.0002316 Loss: 0.5414 Loss_dice: 0.3067 Loss_ce: 0.2346\n",
      "Epoch: 2 [602/877] [2356/2631] lr:0.0002314 Loss: 0.6476 Loss_dice: 0.3131 Loss_ce: 0.3345\n",
      "Epoch: 2 [603/877] [2357/2631] lr:0.0002312 Loss: 0.5584 Loss_dice: 0.3033 Loss_ce: 0.2551\n",
      "Epoch: 2 [604/877] [2358/2631] lr:0.0002310 Loss: 0.5248 Loss_dice: 0.2952 Loss_ce: 0.2296\n",
      "Epoch: 2 [605/877] [2359/2631] lr:0.0002309 Loss: 0.5307 Loss_dice: 0.2912 Loss_ce: 0.2395\n",
      "Epoch: 2 [606/877] [2360/2631] lr:0.0002307 Loss: 0.5400 Loss_dice: 0.2902 Loss_ce: 0.2498\n",
      "Epoch: 2 [607/877] [2361/2631] lr:0.0002305 Loss: 0.5441 Loss_dice: 0.3029 Loss_ce: 0.2412\n",
      "Epoch: 2 [608/877] [2362/2631] lr:0.0002303 Loss: 0.5257 Loss_dice: 0.2564 Loss_ce: 0.2693\n",
      "Epoch: 2 [609/877] [2363/2631] lr:0.0002301 Loss: 0.5334 Loss_dice: 0.2963 Loss_ce: 0.2370\n",
      "Epoch: 2 [610/877] [2364/2631] lr:0.0002300 Loss: 0.5238 Loss_dice: 0.2791 Loss_ce: 0.2447\n",
      "Epoch: 2 [611/877] [2365/2631] lr:0.0002298 Loss: 0.5272 Loss_dice: 0.2798 Loss_ce: 0.2474\n",
      "Epoch: 2 [612/877] [2366/2631] lr:0.0002296 Loss: 0.5625 Loss_dice: 0.2940 Loss_ce: 0.2685\n",
      "Epoch: 2 [613/877] [2367/2631] lr:0.0002294 Loss: 0.5366 Loss_dice: 0.2949 Loss_ce: 0.2416\n",
      "Epoch: 2 [614/877] [2368/2631] lr:0.0002292 Loss: 0.5556 Loss_dice: 0.2325 Loss_ce: 0.3231\n",
      "Epoch: 2 [615/877] [2369/2631] lr:0.0002291 Loss: 0.5322 Loss_dice: 0.2885 Loss_ce: 0.2436\n",
      "Epoch: 2 [616/877] [2370/2631] lr:0.0002289 Loss: 0.5262 Loss_dice: 0.2812 Loss_ce: 0.2450\n",
      "Epoch: 2 [617/877] [2371/2631] lr:0.0002287 Loss: 0.5356 Loss_dice: 0.2804 Loss_ce: 0.2552\n",
      "Epoch: 2 [618/877] [2372/2631] lr:0.0002285 Loss: 0.7625 Loss_dice: 0.3255 Loss_ce: 0.4371\n",
      "Epoch: 2 [619/877] [2373/2631] lr:0.0002284 Loss: 0.5275 Loss_dice: 0.2680 Loss_ce: 0.2595\n",
      "Epoch: 2 [620/877] [2374/2631] lr:0.0002282 Loss: 0.5440 Loss_dice: 0.2832 Loss_ce: 0.2608\n",
      "Epoch: 2 [621/877] [2375/2631] lr:0.0002280 Loss: 0.5307 Loss_dice: 0.2636 Loss_ce: 0.2671\n",
      "Epoch: 2 [622/877] [2376/2631] lr:0.0002278 Loss: 1.1236 Loss_dice: 0.4282 Loss_ce: 0.6954\n",
      "Epoch: 2 [623/877] [2377/2631] lr:0.0002276 Loss: 0.6829 Loss_dice: 0.3142 Loss_ce: 0.3688\n",
      "Epoch: 2 [624/877] [2378/2631] lr:0.0002275 Loss: 0.5768 Loss_dice: 0.2818 Loss_ce: 0.2949\n",
      "Epoch: 2 [625/877] [2379/2631] lr:0.0002273 Loss: 0.5437 Loss_dice: 0.2598 Loss_ce: 0.2839\n",
      "Epoch: 2 [626/877] [2380/2631] lr:0.0002271 Loss: 0.5625 Loss_dice: 0.2515 Loss_ce: 0.3110\n",
      "Epoch: 2 [627/877] [2381/2631] lr:0.0002269 Loss: 0.5801 Loss_dice: 0.2722 Loss_ce: 0.3079\n",
      "Epoch: 2 [628/877] [2382/2631] lr:0.0002267 Loss: 0.5443 Loss_dice: 0.2839 Loss_ce: 0.2603\n",
      "Epoch: 2 [629/877] [2383/2631] lr:0.0002266 Loss: 0.5719 Loss_dice: 0.2616 Loss_ce: 0.3103\n",
      "Epoch: 2 [630/877] [2384/2631] lr:0.0002264 Loss: 0.5519 Loss_dice: 0.2850 Loss_ce: 0.2669\n",
      "Epoch: 2 [631/877] [2385/2631] lr:0.0002262 Loss: 0.6610 Loss_dice: 0.3107 Loss_ce: 0.3503\n",
      "Epoch: 2 [632/877] [2386/2631] lr:0.0002260 Loss: 0.5687 Loss_dice: 0.2922 Loss_ce: 0.2765\n",
      "Epoch: 2 [633/877] [2387/2631] lr:0.0002259 Loss: 0.5375 Loss_dice: 0.2547 Loss_ce: 0.2828\n",
      "Epoch: 2 [634/877] [2388/2631] lr:0.0002257 Loss: 0.5968 Loss_dice: 0.2670 Loss_ce: 0.3297\n",
      "Epoch: 2 [635/877] [2389/2631] lr:0.0002255 Loss: 0.5792 Loss_dice: 0.2881 Loss_ce: 0.2911\n",
      "Epoch: 2 [636/877] [2390/2631] lr:0.0002253 Loss: 0.5377 Loss_dice: 0.2832 Loss_ce: 0.2545\n",
      "Epoch: 2 [637/877] [2391/2631] lr:0.0002251 Loss: 0.6135 Loss_dice: 0.2820 Loss_ce: 0.3316\n",
      "Epoch: 2 [638/877] [2392/2631] lr:0.0002250 Loss: 0.5392 Loss_dice: 0.2649 Loss_ce: 0.2743\n",
      "Epoch: 2 [639/877] [2393/2631] lr:0.0002248 Loss: 0.5349 Loss_dice: 0.2842 Loss_ce: 0.2508\n",
      "Epoch: 2 [640/877] [2394/2631] lr:0.0002246 Loss: 0.5375 Loss_dice: 0.2757 Loss_ce: 0.2617\n",
      "Epoch: 2 [641/877] [2395/2631] lr:0.0002244 Loss: 0.6823 Loss_dice: 0.2997 Loss_ce: 0.3826\n",
      "Epoch: 2 [642/877] [2396/2631] lr:0.0002243 Loss: 0.5319 Loss_dice: 0.2642 Loss_ce: 0.2677\n",
      "Epoch: 2 [643/877] [2397/2631] lr:0.0002241 Loss: 0.5238 Loss_dice: 0.2724 Loss_ce: 0.2514\n",
      "Epoch: 2 [644/877] [2398/2631] lr:0.0002239 Loss: 0.5438 Loss_dice: 0.2795 Loss_ce: 0.2643\n",
      "Epoch: 2 [645/877] [2399/2631] lr:0.0002237 Loss: 0.5607 Loss_dice: 0.2667 Loss_ce: 0.2940\n",
      "Epoch: 2 [646/877] [2400/2631] lr:0.0002235 Loss: 0.5323 Loss_dice: 0.2467 Loss_ce: 0.2857\n",
      "Epoch: 2 [647/877] [2401/2631] lr:0.0002234 Loss: 0.8596 Loss_dice: 0.3518 Loss_ce: 0.5078\n",
      "Epoch: 2 [648/877] [2402/2631] lr:0.0002232 Loss: 0.5665 Loss_dice: 0.2597 Loss_ce: 0.3068\n",
      "Epoch: 2 [649/877] [2403/2631] lr:0.0002230 Loss: 0.5260 Loss_dice: 0.2740 Loss_ce: 0.2520\n",
      "Epoch: 2 [650/877] [2404/2631] lr:0.0002228 Loss: 0.5572 Loss_dice: 0.2851 Loss_ce: 0.2720\n",
      "Epoch: 2 [651/877] [2405/2631] lr:0.0002227 Loss: 0.5427 Loss_dice: 0.2834 Loss_ce: 0.2593\n",
      "Epoch: 2 [652/877] [2406/2631] lr:0.0002225 Loss: 0.8626 Loss_dice: 0.3337 Loss_ce: 0.5289\n",
      "Epoch: 2 [653/877] [2407/2631] lr:0.0002223 Loss: 0.5856 Loss_dice: 0.2972 Loss_ce: 0.2885\n",
      "Epoch: 2 [654/877] [2408/2631] lr:0.0002221 Loss: 0.5394 Loss_dice: 0.2597 Loss_ce: 0.2797\n",
      "Epoch: 2 [655/877] [2409/2631] lr:0.0002219 Loss: 0.5283 Loss_dice: 0.2824 Loss_ce: 0.2459\n",
      "Epoch: 2 [656/877] [2410/2631] lr:0.0002218 Loss: 0.5689 Loss_dice: 0.2905 Loss_ce: 0.2783\n",
      "Epoch: 2 [657/877] [2411/2631] lr:0.0002216 Loss: 0.5597 Loss_dice: 0.2865 Loss_ce: 0.2732\n",
      "Epoch: 2 [658/877] [2412/2631] lr:0.0002214 Loss: 0.5840 Loss_dice: 0.2995 Loss_ce: 0.2845\n",
      "Epoch: 2 [659/877] [2413/2631] lr:0.0002212 Loss: 0.5476 Loss_dice: 0.2885 Loss_ce: 0.2591\n",
      "Epoch: 2 [660/877] [2414/2631] lr:0.0002210 Loss: 0.5445 Loss_dice: 0.2574 Loss_ce: 0.2871\n",
      "Epoch: 2 [661/877] [2415/2631] lr:0.0002209 Loss: 0.5476 Loss_dice: 0.2549 Loss_ce: 0.2928\n",
      "Epoch: 2 [662/877] [2416/2631] lr:0.0002207 Loss: 0.5994 Loss_dice: 0.2451 Loss_ce: 0.3543\n",
      "Epoch: 2 [663/877] [2417/2631] lr:0.0002205 Loss: 0.5363 Loss_dice: 0.2792 Loss_ce: 0.2572\n",
      "Epoch: 2 [664/877] [2418/2631] lr:0.0002203 Loss: 0.5674 Loss_dice: 0.2887 Loss_ce: 0.2787\n",
      "Epoch: 2 [665/877] [2419/2631] lr:0.0002202 Loss: 0.6078 Loss_dice: 0.3003 Loss_ce: 0.3074\n",
      "Epoch: 2 [666/877] [2420/2631] lr:0.0002200 Loss: 0.5430 Loss_dice: 0.2169 Loss_ce: 0.3261\n",
      "Epoch: 2 [667/877] [2421/2631] lr:0.0002198 Loss: 0.5883 Loss_dice: 0.2693 Loss_ce: 0.3191\n",
      "Epoch: 2 [668/877] [2422/2631] lr:0.0002196 Loss: 0.5440 Loss_dice: 0.2816 Loss_ce: 0.2624\n",
      "Epoch: 2 [669/877] [2423/2631] lr:0.0002194 Loss: 0.5539 Loss_dice: 0.2779 Loss_ce: 0.2760\n",
      "Epoch: 2 [670/877] [2424/2631] lr:0.0002193 Loss: 0.5213 Loss_dice: 0.2721 Loss_ce: 0.2492\n",
      "Epoch: 2 [671/877] [2425/2631] lr:0.0002191 Loss: 0.7577 Loss_dice: 0.3177 Loss_ce: 0.4400\n",
      "Epoch: 2 [672/877] [2426/2631] lr:0.0002189 Loss: 0.5258 Loss_dice: 0.2684 Loss_ce: 0.2574\n",
      "Epoch: 2 [673/877] [2427/2631] lr:0.0002187 Loss: 0.5324 Loss_dice: 0.2605 Loss_ce: 0.2719\n",
      "Epoch: 2 [674/877] [2428/2631] lr:0.0002186 Loss: 0.5323 Loss_dice: 0.2851 Loss_ce: 0.2471\n",
      "Epoch: 2 [675/877] [2429/2631] lr:0.0002184 Loss: 0.5519 Loss_dice: 0.2432 Loss_ce: 0.3087\n",
      "Epoch: 2 [676/877] [2430/2631] lr:0.0002182 Loss: 0.5332 Loss_dice: 0.2757 Loss_ce: 0.2575\n",
      "Epoch: 2 [677/877] [2431/2631] lr:0.0002180 Loss: 0.5398 Loss_dice: 0.2823 Loss_ce: 0.2575\n",
      "Epoch: 2 [678/877] [2432/2631] lr:0.0002178 Loss: 0.6212 Loss_dice: 0.2749 Loss_ce: 0.3462\n",
      "Epoch: 2 [679/877] [2433/2631] lr:0.0002177 Loss: 0.5448 Loss_dice: 0.2645 Loss_ce: 0.2803\n",
      "Epoch: 2 [680/877] [2434/2631] lr:0.0002175 Loss: 0.5471 Loss_dice: 0.2793 Loss_ce: 0.2678\n",
      "Epoch: 2 [681/877] [2435/2631] lr:0.0002173 Loss: 0.6549 Loss_dice: 0.2989 Loss_ce: 0.3560\n",
      "Epoch: 2 [682/877] [2436/2631] lr:0.0002171 Loss: 0.5444 Loss_dice: 0.2866 Loss_ce: 0.2578\n",
      "Epoch: 2 [683/877] [2437/2631] lr:0.0002170 Loss: 0.5335 Loss_dice: 0.2734 Loss_ce: 0.2601\n",
      "Epoch: 2 [684/877] [2438/2631] lr:0.0002168 Loss: 0.5405 Loss_dice: 0.2628 Loss_ce: 0.2776\n",
      "Epoch: 2 [685/877] [2439/2631] lr:0.0002166 Loss: 0.5532 Loss_dice: 0.2804 Loss_ce: 0.2728\n",
      "Epoch: 2 [686/877] [2440/2631] lr:0.0002164 Loss: 0.5756 Loss_dice: 0.2870 Loss_ce: 0.2886\n",
      "Epoch: 2 [687/877] [2441/2631] lr:0.0002163 Loss: 0.5663 Loss_dice: 0.2719 Loss_ce: 0.2944\n",
      "Epoch: 2 [688/877] [2442/2631] lr:0.0002161 Loss: 0.5211 Loss_dice: 0.2713 Loss_ce: 0.2499\n",
      "Epoch: 2 [689/877] [2443/2631] lr:0.0002159 Loss: 0.5347 Loss_dice: 0.2771 Loss_ce: 0.2577\n",
      "Epoch: 2 [690/877] [2444/2631] lr:0.0002157 Loss: 0.5752 Loss_dice: 0.2950 Loss_ce: 0.2803\n",
      "Epoch: 2 [691/877] [2445/2631] lr:0.0002155 Loss: 0.5338 Loss_dice: 0.2704 Loss_ce: 0.2634\n",
      "Epoch: 2 [692/877] [2446/2631] lr:0.0002154 Loss: 0.5506 Loss_dice: 0.2892 Loss_ce: 0.2614\n",
      "Epoch: 2 [693/877] [2447/2631] lr:0.0002152 Loss: 0.5719 Loss_dice: 0.2973 Loss_ce: 0.2746\n",
      "Epoch: 2 [694/877] [2448/2631] lr:0.0002150 Loss: 0.5319 Loss_dice: 0.2864 Loss_ce: 0.2455\n",
      "Epoch: 2 [695/877] [2449/2631] lr:0.0002148 Loss: 0.5526 Loss_dice: 0.2944 Loss_ce: 0.2582\n",
      "Epoch: 2 [696/877] [2450/2631] lr:0.0002147 Loss: 0.5569 Loss_dice: 0.2884 Loss_ce: 0.2685\n",
      "Epoch: 2 [697/877] [2451/2631] lr:0.0002145 Loss: 0.5947 Loss_dice: 0.2829 Loss_ce: 0.3118\n",
      "Epoch: 2 [698/877] [2452/2631] lr:0.0002143 Loss: 0.5415 Loss_dice: 0.2934 Loss_ce: 0.2481\n",
      "Epoch: 2 [699/877] [2453/2631] lr:0.0002141 Loss: 0.5330 Loss_dice: 0.2737 Loss_ce: 0.2592\n",
      "Epoch: 2 [700/877] [2454/2631] lr:0.0002139 Loss: 0.5624 Loss_dice: 0.2919 Loss_ce: 0.2705\n",
      "Epoch: 2 [701/877] [2455/2631] lr:0.0002138 Loss: 0.5343 Loss_dice: 0.2882 Loss_ce: 0.2461\n",
      "Epoch: 2 [702/877] [2456/2631] lr:0.0002136 Loss: 0.6182 Loss_dice: 0.2749 Loss_ce: 0.3433\n",
      "Epoch: 2 [703/877] [2457/2631] lr:0.0002134 Loss: 0.5380 Loss_dice: 0.2883 Loss_ce: 0.2497\n",
      "Epoch: 2 [704/877] [2458/2631] lr:0.0002132 Loss: 0.5653 Loss_dice: 0.2967 Loss_ce: 0.2686\n",
      "Epoch: 2 [705/877] [2459/2631] lr:0.0002131 Loss: 0.5653 Loss_dice: 0.2888 Loss_ce: 0.2765\n",
      "Epoch: 2 [706/877] [2460/2631] lr:0.0002129 Loss: 0.5316 Loss_dice: 0.2585 Loss_ce: 0.2731\n",
      "Epoch: 2 [707/877] [2461/2631] lr:0.0002127 Loss: 0.5415 Loss_dice: 0.2829 Loss_ce: 0.2585\n",
      "Epoch: 2 [708/877] [2462/2631] lr:0.0002125 Loss: 0.5319 Loss_dice: 0.2672 Loss_ce: 0.2647\n",
      "Epoch: 2 [709/877] [2463/2631] lr:0.0002124 Loss: 0.5675 Loss_dice: 0.2917 Loss_ce: 0.2758\n",
      "Epoch: 2 [710/877] [2464/2631] lr:0.0002122 Loss: 0.5607 Loss_dice: 0.2826 Loss_ce: 0.2782\n",
      "Epoch: 2 [711/877] [2465/2631] lr:0.0002120 Loss: 0.5433 Loss_dice: 0.2833 Loss_ce: 0.2600\n",
      "Epoch: 2 [712/877] [2466/2631] lr:0.0002118 Loss: 0.5329 Loss_dice: 0.2663 Loss_ce: 0.2666\n",
      "Epoch: 2 [713/877] [2467/2631] lr:0.0002116 Loss: 0.5664 Loss_dice: 0.2810 Loss_ce: 0.2854\n",
      "Epoch: 2 [714/877] [2468/2631] lr:0.0002115 Loss: 0.5770 Loss_dice: 0.2837 Loss_ce: 0.2933\n",
      "Epoch: 2 [715/877] [2469/2631] lr:0.0002113 Loss: 0.5481 Loss_dice: 0.2830 Loss_ce: 0.2652\n",
      "Epoch: 2 [716/877] [2470/2631] lr:0.0002111 Loss: 0.5324 Loss_dice: 0.2273 Loss_ce: 0.3051\n",
      "Epoch: 2 [717/877] [2471/2631] lr:0.0002109 Loss: 0.5683 Loss_dice: 0.2814 Loss_ce: 0.2869\n",
      "Epoch: 2 [718/877] [2472/2631] lr:0.0002108 Loss: 0.5263 Loss_dice: 0.2877 Loss_ce: 0.2386\n",
      "Epoch: 2 [719/877] [2473/2631] lr:0.0002106 Loss: 0.5257 Loss_dice: 0.2932 Loss_ce: 0.2325\n",
      "Epoch: 2 [720/877] [2474/2631] lr:0.0002104 Loss: 0.5695 Loss_dice: 0.2520 Loss_ce: 0.3174\n",
      "Epoch: 2 [721/877] [2475/2631] lr:0.0002102 Loss: 0.5546 Loss_dice: 0.2966 Loss_ce: 0.2580\n",
      "Epoch: 2 [722/877] [2476/2631] lr:0.0002101 Loss: 0.5334 Loss_dice: 0.2878 Loss_ce: 0.2456\n",
      "Epoch: 2 [723/877] [2477/2631] lr:0.0002099 Loss: 0.5426 Loss_dice: 0.2880 Loss_ce: 0.2546\n",
      "Epoch: 2 [724/877] [2478/2631] lr:0.0002097 Loss: 0.5324 Loss_dice: 0.2655 Loss_ce: 0.2669\n",
      "Epoch: 2 [725/877] [2479/2631] lr:0.0002095 Loss: 0.5883 Loss_dice: 0.3039 Loss_ce: 0.2844\n",
      "Epoch: 2 [726/877] [2480/2631] lr:0.0002093 Loss: 0.5388 Loss_dice: 0.2870 Loss_ce: 0.2518\n",
      "Epoch: 2 [727/877] [2481/2631] lr:0.0002092 Loss: 0.5947 Loss_dice: 0.2865 Loss_ce: 0.3081\n",
      "Epoch: 2 [728/877] [2482/2631] lr:0.0002090 Loss: 0.5422 Loss_dice: 0.2589 Loss_ce: 0.2834\n",
      "Epoch: 2 [729/877] [2483/2631] lr:0.0002088 Loss: 0.5310 Loss_dice: 0.2709 Loss_ce: 0.2601\n",
      "Epoch: 2 [730/877] [2484/2631] lr:0.0002086 Loss: 0.5576 Loss_dice: 0.2878 Loss_ce: 0.2698\n",
      "Epoch: 2 [731/877] [2485/2631] lr:0.0002085 Loss: 0.5430 Loss_dice: 0.2948 Loss_ce: 0.2482\n",
      "Epoch: 2 [732/877] [2486/2631] lr:0.0002083 Loss: 0.5641 Loss_dice: 0.2897 Loss_ce: 0.2744\n",
      "Epoch: 2 [733/877] [2487/2631] lr:0.0002081 Loss: 0.5587 Loss_dice: 0.2929 Loss_ce: 0.2658\n",
      "Epoch: 2 [734/877] [2488/2631] lr:0.0002079 Loss: 0.5356 Loss_dice: 0.2405 Loss_ce: 0.2951\n",
      "Epoch: 2 [735/877] [2489/2631] lr:0.0002078 Loss: 0.5657 Loss_dice: 0.2772 Loss_ce: 0.2885\n",
      "Epoch: 2 [736/877] [2490/2631] lr:0.0002076 Loss: 0.5343 Loss_dice: 0.2624 Loss_ce: 0.2718\n",
      "Epoch: 2 [737/877] [2491/2631] lr:0.0002074 Loss: 0.5302 Loss_dice: 0.2772 Loss_ce: 0.2531\n",
      "Epoch: 2 [738/877] [2492/2631] lr:0.0002072 Loss: 0.5200 Loss_dice: 0.2450 Loss_ce: 0.2751\n",
      "Epoch: 2 [739/877] [2493/2631] lr:0.0002070 Loss: 0.9118 Loss_dice: 0.3833 Loss_ce: 0.5285\n",
      "Epoch: 2 [740/877] [2494/2631] lr:0.0002069 Loss: 0.5468 Loss_dice: 0.2974 Loss_ce: 0.2495\n",
      "Epoch: 2 [741/877] [2495/2631] lr:0.0002067 Loss: 0.5500 Loss_dice: 0.2980 Loss_ce: 0.2521\n",
      "Epoch: 2 [742/877] [2496/2631] lr:0.0002065 Loss: 0.5312 Loss_dice: 0.2819 Loss_ce: 0.2492\n",
      "Epoch: 2 [743/877] [2497/2631] lr:0.0002063 Loss: 0.5417 Loss_dice: 0.2974 Loss_ce: 0.2443\n",
      "Epoch: 2 [744/877] [2498/2631] lr:0.0002062 Loss: 0.6453 Loss_dice: 0.3041 Loss_ce: 0.3412\n",
      "Epoch: 2 [745/877] [2499/2631] lr:0.0002060 Loss: 0.5228 Loss_dice: 0.2886 Loss_ce: 0.2342\n",
      "Epoch: 2 [746/877] [2500/2631] lr:0.0002058 Loss: 0.5247 Loss_dice: 0.2739 Loss_ce: 0.2508\n",
      "Epoch: 2 [747/877] [2501/2631] lr:0.0002056 Loss: 0.5401 Loss_dice: 0.2750 Loss_ce: 0.2651\n",
      "Epoch: 2 [748/877] [2502/2631] lr:0.0002055 Loss: 0.5327 Loss_dice: 0.2755 Loss_ce: 0.2572\n",
      "Epoch: 2 [749/877] [2503/2631] lr:0.0002053 Loss: 0.5263 Loss_dice: 0.2649 Loss_ce: 0.2614\n",
      "Epoch: 2 [750/877] [2504/2631] lr:0.0002051 Loss: 0.5214 Loss_dice: 0.2694 Loss_ce: 0.2519\n",
      "Epoch: 2 [751/877] [2505/2631] lr:0.0002049 Loss: 0.5303 Loss_dice: 0.2491 Loss_ce: 0.2812\n",
      "Epoch: 2 [752/877] [2506/2631] lr:0.0002048 Loss: 0.5289 Loss_dice: 0.2788 Loss_ce: 0.2501\n",
      "Epoch: 2 [753/877] [2507/2631] lr:0.0002046 Loss: 0.5348 Loss_dice: 0.2647 Loss_ce: 0.2701\n",
      "Epoch: 2 [754/877] [2508/2631] lr:0.0002044 Loss: 0.5370 Loss_dice: 0.2874 Loss_ce: 0.2496\n",
      "Epoch: 2 [755/877] [2509/2631] lr:0.0002042 Loss: 0.5279 Loss_dice: 0.2705 Loss_ce: 0.2575\n",
      "Epoch: 2 [756/877] [2510/2631] lr:0.0002041 Loss: 0.5637 Loss_dice: 0.2808 Loss_ce: 0.2829\n",
      "Epoch: 2 [757/877] [2511/2631] lr:0.0002039 Loss: 0.5340 Loss_dice: 0.2875 Loss_ce: 0.2464\n",
      "Epoch: 2 [758/877] [2512/2631] lr:0.0002037 Loss: 0.5421 Loss_dice: 0.2905 Loss_ce: 0.2516\n",
      "Epoch: 2 [759/877] [2513/2631] lr:0.0002035 Loss: 0.5190 Loss_dice: 0.2586 Loss_ce: 0.2604\n",
      "Epoch: 2 [760/877] [2514/2631] lr:0.0002033 Loss: 0.5279 Loss_dice: 0.2734 Loss_ce: 0.2545\n",
      "Epoch: 2 [761/877] [2515/2631] lr:0.0002032 Loss: 0.5444 Loss_dice: 0.2884 Loss_ce: 0.2560\n",
      "Epoch: 2 [762/877] [2516/2631] lr:0.0002030 Loss: 0.5915 Loss_dice: 0.3013 Loss_ce: 0.2903\n",
      "Epoch: 2 [763/877] [2517/2631] lr:0.0002028 Loss: 0.5309 Loss_dice: 0.2682 Loss_ce: 0.2627\n",
      "Epoch: 2 [764/877] [2518/2631] lr:0.0002026 Loss: 0.5504 Loss_dice: 0.2831 Loss_ce: 0.2673\n",
      "Epoch: 2 [765/877] [2519/2631] lr:0.0002025 Loss: 0.5464 Loss_dice: 0.2899 Loss_ce: 0.2564\n",
      "Epoch: 2 [766/877] [2520/2631] lr:0.0002023 Loss: 0.6997 Loss_dice: 0.3238 Loss_ce: 0.3759\n",
      "Epoch: 2 [767/877] [2521/2631] lr:0.0002021 Loss: 0.5307 Loss_dice: 0.2763 Loss_ce: 0.2544\n",
      "Epoch: 2 [768/877] [2522/2631] lr:0.0002019 Loss: 0.5537 Loss_dice: 0.2935 Loss_ce: 0.2602\n",
      "Epoch: 2 [769/877] [2523/2631] lr:0.0002018 Loss: 0.5612 Loss_dice: 0.2895 Loss_ce: 0.2717\n",
      "Epoch: 2 [770/877] [2524/2631] lr:0.0002016 Loss: 0.5747 Loss_dice: 0.2952 Loss_ce: 0.2795\n",
      "Epoch: 2 [771/877] [2525/2631] lr:0.0002014 Loss: 0.5493 Loss_dice: 0.2898 Loss_ce: 0.2595\n",
      "Epoch: 2 [772/877] [2526/2631] lr:0.0002012 Loss: 0.5522 Loss_dice: 0.2469 Loss_ce: 0.3053\n",
      "Epoch: 2 [773/877] [2527/2631] lr:0.0002011 Loss: 0.6135 Loss_dice: 0.2942 Loss_ce: 0.3193\n",
      "Epoch: 2 [774/877] [2528/2631] lr:0.0002009 Loss: 0.5643 Loss_dice: 0.2812 Loss_ce: 0.2831\n",
      "Epoch: 2 [775/877] [2529/2631] lr:0.0002007 Loss: 0.5893 Loss_dice: 0.2945 Loss_ce: 0.2948\n",
      "Epoch: 2 [776/877] [2530/2631] lr:0.0002005 Loss: 0.5278 Loss_dice: 0.2796 Loss_ce: 0.2482\n",
      "Epoch: 2 [777/877] [2531/2631] lr:0.0002004 Loss: 0.5946 Loss_dice: 0.2872 Loss_ce: 0.3074\n",
      "Epoch: 2 [778/877] [2532/2631] lr:0.0002002 Loss: 0.5397 Loss_dice: 0.2857 Loss_ce: 0.2540\n",
      "Epoch: 2 [779/877] [2533/2631] lr:0.0002000 Loss: 0.5493 Loss_dice: 0.2418 Loss_ce: 0.3075\n",
      "Epoch: 2 [780/877] [2534/2631] lr:0.0001998 Loss: 0.5670 Loss_dice: 0.2622 Loss_ce: 0.3048\n",
      "Epoch: 2 [781/877] [2535/2631] lr:0.0001997 Loss: 0.5377 Loss_dice: 0.2625 Loss_ce: 0.2752\n",
      "Epoch: 2 [782/877] [2536/2631] lr:0.0001995 Loss: 0.5466 Loss_dice: 0.2929 Loss_ce: 0.2537\n",
      "Epoch: 2 [783/877] [2537/2631] lr:0.0001993 Loss: 0.5464 Loss_dice: 0.2921 Loss_ce: 0.2543\n",
      "Epoch: 2 [784/877] [2538/2631] lr:0.0001991 Loss: 0.5958 Loss_dice: 0.2975 Loss_ce: 0.2983\n",
      "Epoch: 2 [785/877] [2539/2631] lr:0.0001990 Loss: 0.5390 Loss_dice: 0.2893 Loss_ce: 0.2496\n",
      "Epoch: 2 [786/877] [2540/2631] lr:0.0001988 Loss: 0.5280 Loss_dice: 0.2765 Loss_ce: 0.2515\n",
      "Epoch: 2 [787/877] [2541/2631] lr:0.0001986 Loss: 0.5724 Loss_dice: 0.2917 Loss_ce: 0.2806\n",
      "Epoch: 2 [788/877] [2542/2631] lr:0.0001984 Loss: 0.5386 Loss_dice: 0.2912 Loss_ce: 0.2475\n",
      "Epoch: 2 [789/877] [2543/2631] lr:0.0001983 Loss: 0.5951 Loss_dice: 0.3065 Loss_ce: 0.2886\n",
      "Epoch: 2 [790/877] [2544/2631] lr:0.0001981 Loss: 0.5288 Loss_dice: 0.2835 Loss_ce: 0.2453\n",
      "Epoch: 2 [791/877] [2545/2631] lr:0.0001979 Loss: 0.5390 Loss_dice: 0.2901 Loss_ce: 0.2488\n",
      "Epoch: 2 [792/877] [2546/2631] lr:0.0001977 Loss: 0.5397 Loss_dice: 0.2834 Loss_ce: 0.2563\n",
      "Epoch: 2 [793/877] [2547/2631] lr:0.0001976 Loss: 0.5357 Loss_dice: 0.2844 Loss_ce: 0.2513\n",
      "Epoch: 2 [794/877] [2548/2631] lr:0.0001974 Loss: 0.5446 Loss_dice: 0.2952 Loss_ce: 0.2494\n",
      "Epoch: 2 [795/877] [2549/2631] lr:0.0001972 Loss: 0.5346 Loss_dice: 0.2781 Loss_ce: 0.2565\n",
      "Epoch: 2 [796/877] [2550/2631] lr:0.0001970 Loss: 0.5403 Loss_dice: 0.2354 Loss_ce: 0.3049\n",
      "Epoch: 2 [797/877] [2551/2631] lr:0.0001969 Loss: 0.5454 Loss_dice: 0.2921 Loss_ce: 0.2533\n",
      "Epoch: 2 [798/877] [2552/2631] lr:0.0001967 Loss: 0.5325 Loss_dice: 0.2830 Loss_ce: 0.2495\n",
      "Epoch: 2 [799/877] [2553/2631] lr:0.0001965 Loss: 0.5539 Loss_dice: 0.2950 Loss_ce: 0.2590\n",
      "Epoch: 2 [800/877] [2554/2631] lr:0.0001963 Loss: 0.5401 Loss_dice: 0.2858 Loss_ce: 0.2543\n",
      "Epoch: 2 [801/877] [2555/2631] lr:0.0001962 Loss: 0.5455 Loss_dice: 0.2932 Loss_ce: 0.2523\n",
      "Epoch: 2 [802/877] [2556/2631] lr:0.0001960 Loss: 0.5257 Loss_dice: 0.2856 Loss_ce: 0.2402\n",
      "Epoch: 2 [803/877] [2557/2631] lr:0.0001958 Loss: 0.5270 Loss_dice: 0.2557 Loss_ce: 0.2712\n",
      "Epoch: 2 [804/877] [2558/2631] lr:0.0001956 Loss: 0.5420 Loss_dice: 0.2903 Loss_ce: 0.2517\n",
      "Epoch: 2 [805/877] [2559/2631] lr:0.0001955 Loss: 0.5341 Loss_dice: 0.2703 Loss_ce: 0.2638\n",
      "Epoch: 2 [806/877] [2560/2631] lr:0.0001953 Loss: 0.5359 Loss_dice: 0.2869 Loss_ce: 0.2490\n",
      "Epoch: 2 [807/877] [2561/2631] lr:0.0001951 Loss: 0.5333 Loss_dice: 0.2881 Loss_ce: 0.2452\n",
      "Epoch: 2 [808/877] [2562/2631] lr:0.0001949 Loss: 0.5245 Loss_dice: 0.2860 Loss_ce: 0.2385\n",
      "Epoch: 2 [809/877] [2563/2631] lr:0.0001948 Loss: 0.5248 Loss_dice: 0.2736 Loss_ce: 0.2511\n",
      "Epoch: 2 [810/877] [2564/2631] lr:0.0001946 Loss: 0.5458 Loss_dice: 0.2726 Loss_ce: 0.2732\n",
      "Epoch: 2 [811/877] [2565/2631] lr:0.0001944 Loss: 0.6020 Loss_dice: 0.3024 Loss_ce: 0.2996\n",
      "Epoch: 2 [812/877] [2566/2631] lr:0.0001942 Loss: 0.5288 Loss_dice: 0.2877 Loss_ce: 0.2411\n",
      "Epoch: 2 [813/877] [2567/2631] lr:0.0001941 Loss: 0.5238 Loss_dice: 0.2553 Loss_ce: 0.2686\n",
      "Epoch: 2 [814/877] [2568/2631] lr:0.0001939 Loss: 0.5449 Loss_dice: 0.2878 Loss_ce: 0.2571\n",
      "Epoch: 2 [815/877] [2569/2631] lr:0.0001937 Loss: 0.5439 Loss_dice: 0.2691 Loss_ce: 0.2749\n",
      "Epoch: 2 [816/877] [2570/2631] lr:0.0001935 Loss: 0.5465 Loss_dice: 0.2760 Loss_ce: 0.2705\n",
      "Epoch: 2 [817/877] [2571/2631] lr:0.0001934 Loss: 0.5300 Loss_dice: 0.2856 Loss_ce: 0.2445\n",
      "Epoch: 2 [818/877] [2572/2631] lr:0.0001932 Loss: 0.5296 Loss_dice: 0.2917 Loss_ce: 0.2379\n",
      "Epoch: 2 [819/877] [2573/2631] lr:0.0001930 Loss: 0.5286 Loss_dice: 0.2786 Loss_ce: 0.2500\n",
      "Epoch: 2 [820/877] [2574/2631] lr:0.0001928 Loss: 0.5628 Loss_dice: 0.2825 Loss_ce: 0.2804\n",
      "Epoch: 2 [821/877] [2575/2631] lr:0.0001927 Loss: 0.5607 Loss_dice: 0.2841 Loss_ce: 0.2766\n",
      "Epoch: 2 [822/877] [2576/2631] lr:0.0001925 Loss: 0.5611 Loss_dice: 0.2934 Loss_ce: 0.2677\n",
      "Epoch: 2 [823/877] [2577/2631] lr:0.0001923 Loss: 0.5512 Loss_dice: 0.2494 Loss_ce: 0.3018\n",
      "Epoch: 2 [824/877] [2578/2631] lr:0.0001921 Loss: 0.5331 Loss_dice: 0.2720 Loss_ce: 0.2611\n",
      "Epoch: 2 [825/877] [2579/2631] lr:0.0001920 Loss: 0.5351 Loss_dice: 0.2873 Loss_ce: 0.2478\n",
      "Epoch: 2 [826/877] [2580/2631] lr:0.0001918 Loss: 0.5302 Loss_dice: 0.2640 Loss_ce: 0.2662\n",
      "Epoch: 2 [827/877] [2581/2631] lr:0.0001916 Loss: 0.5448 Loss_dice: 0.2886 Loss_ce: 0.2562\n",
      "Epoch: 2 [828/877] [2582/2631] lr:0.0001914 Loss: 0.5758 Loss_dice: 0.2965 Loss_ce: 0.2794\n",
      "Epoch: 2 [829/877] [2583/2631] lr:0.0001913 Loss: 0.5979 Loss_dice: 0.2949 Loss_ce: 0.3030\n",
      "Epoch: 2 [830/877] [2584/2631] lr:0.0001911 Loss: 0.5275 Loss_dice: 0.2292 Loss_ce: 0.2983\n",
      "Epoch: 2 [831/877] [2585/2631] lr:0.0001909 Loss: 0.5231 Loss_dice: 0.2606 Loss_ce: 0.2625\n",
      "Epoch: 2 [832/877] [2586/2631] lr:0.0001907 Loss: 0.5276 Loss_dice: 0.2728 Loss_ce: 0.2548\n",
      "Epoch: 2 [833/877] [2587/2631] lr:0.0001906 Loss: 0.5873 Loss_dice: 0.2797 Loss_ce: 0.3075\n",
      "Epoch: 2 [834/877] [2588/2631] lr:0.0001904 Loss: 1.0312 Loss_dice: 0.3824 Loss_ce: 0.6488\n",
      "Epoch: 2 [835/877] [2589/2631] lr:0.0001902 Loss: 0.5275 Loss_dice: 0.2880 Loss_ce: 0.2394\n",
      "Epoch: 2 [836/877] [2590/2631] lr:0.0001901 Loss: 0.5252 Loss_dice: 0.2723 Loss_ce: 0.2529\n",
      "Epoch: 2 [837/877] [2591/2631] lr:0.0001899 Loss: 0.5345 Loss_dice: 0.2771 Loss_ce: 0.2574\n",
      "Epoch: 2 [838/877] [2592/2631] lr:0.0001897 Loss: 0.5428 Loss_dice: 0.2714 Loss_ce: 0.2714\n",
      "Epoch: 2 [839/877] [2593/2631] lr:0.0001895 Loss: 0.5338 Loss_dice: 0.2876 Loss_ce: 0.2462\n",
      "Epoch: 2 [840/877] [2594/2631] lr:0.0001894 Loss: 0.5428 Loss_dice: 0.2983 Loss_ce: 0.2444\n",
      "Epoch: 2 [841/877] [2595/2631] lr:0.0001892 Loss: 0.5436 Loss_dice: 0.3004 Loss_ce: 0.2432\n",
      "Epoch: 2 [842/877] [2596/2631] lr:0.0001890 Loss: 0.5371 Loss_dice: 0.2993 Loss_ce: 0.2378\n",
      "Epoch: 2 [843/877] [2597/2631] lr:0.0001888 Loss: 0.5294 Loss_dice: 0.2676 Loss_ce: 0.2618\n",
      "Epoch: 2 [844/877] [2598/2631] lr:0.0001887 Loss: 0.5415 Loss_dice: 0.2989 Loss_ce: 0.2426\n",
      "Epoch: 2 [845/877] [2599/2631] lr:0.0001885 Loss: 0.5667 Loss_dice: 0.2894 Loss_ce: 0.2773\n",
      "Epoch: 2 [846/877] [2600/2631] lr:0.0001883 Loss: 0.5381 Loss_dice: 0.2990 Loss_ce: 0.2390\n",
      "Epoch: 2 [847/877] [2601/2631] lr:0.0001881 Loss: 0.5306 Loss_dice: 0.2949 Loss_ce: 0.2357\n",
      "Epoch: 2 [848/877] [2602/2631] lr:0.0001880 Loss: 0.5265 Loss_dice: 0.2812 Loss_ce: 0.2454\n",
      "Epoch: 2 [849/877] [2603/2631] lr:0.0001878 Loss: 0.5340 Loss_dice: 0.2156 Loss_ce: 0.3184\n",
      "Epoch: 2 [850/877] [2604/2631] lr:0.0001876 Loss: 0.5434 Loss_dice: 0.2937 Loss_ce: 0.2497\n",
      "Epoch: 2 [851/877] [2605/2631] lr:0.0001874 Loss: 0.5777 Loss_dice: 0.3027 Loss_ce: 0.2750\n",
      "Epoch: 2 [852/877] [2606/2631] lr:0.0001873 Loss: 0.5187 Loss_dice: 0.2892 Loss_ce: 0.2295\n",
      "Epoch: 2 [853/877] [2607/2631] lr:0.0001871 Loss: 0.5492 Loss_dice: 0.2414 Loss_ce: 0.3078\n",
      "Epoch: 2 [854/877] [2608/2631] lr:0.0001869 Loss: 0.6034 Loss_dice: 0.3041 Loss_ce: 0.2992\n",
      "Epoch: 2 [855/877] [2609/2631] lr:0.0001868 Loss: 0.5238 Loss_dice: 0.2570 Loss_ce: 0.2668\n",
      "Epoch: 2 [856/877] [2610/2631] lr:0.0001866 Loss: 0.5215 Loss_dice: 0.2829 Loss_ce: 0.2385\n",
      "Epoch: 2 [857/877] [2611/2631] lr:0.0001864 Loss: 0.5621 Loss_dice: 0.2990 Loss_ce: 0.2631\n",
      "Epoch: 2 [858/877] [2612/2631] lr:0.0001862 Loss: 0.5276 Loss_dice: 0.2930 Loss_ce: 0.2345\n",
      "Epoch: 2 [859/877] [2613/2631] lr:0.0001861 Loss: 0.5343 Loss_dice: 0.2843 Loss_ce: 0.2500\n",
      "Epoch: 2 [860/877] [2614/2631] lr:0.0001859 Loss: 0.5302 Loss_dice: 0.2756 Loss_ce: 0.2545\n",
      "Epoch: 2 [861/877] [2615/2631] lr:0.0001857 Loss: 0.5629 Loss_dice: 0.2154 Loss_ce: 0.3475\n",
      "Epoch: 2 [862/877] [2616/2631] lr:0.0001855 Loss: 0.5429 Loss_dice: 0.2848 Loss_ce: 0.2581\n",
      "Epoch: 2 [863/877] [2617/2631] lr:0.0001854 Loss: 0.5276 Loss_dice: 0.2692 Loss_ce: 0.2583\n",
      "Epoch: 2 [864/877] [2618/2631] lr:0.0001852 Loss: 0.5482 Loss_dice: 0.2883 Loss_ce: 0.2599\n",
      "Epoch: 2 [865/877] [2619/2631] lr:0.0001850 Loss: 0.5526 Loss_dice: 0.2875 Loss_ce: 0.2651\n",
      "Epoch: 2 [866/877] [2620/2631] lr:0.0001848 Loss: 0.5701 Loss_dice: 0.2340 Loss_ce: 0.3361\n",
      "Epoch: 2 [867/877] [2621/2631] lr:0.0001847 Loss: 0.5305 Loss_dice: 0.2895 Loss_ce: 0.2409\n",
      "Epoch: 2 [868/877] [2622/2631] lr:0.0001845 Loss: 0.5260 Loss_dice: 0.2374 Loss_ce: 0.2886\n",
      "Epoch: 2 [869/877] [2623/2631] lr:0.0001843 Loss: 0.5403 Loss_dice: 0.2898 Loss_ce: 0.2505\n",
      "Epoch: 2 [870/877] [2624/2631] lr:0.0001842 Loss: 0.5269 Loss_dice: 0.2330 Loss_ce: 0.2939\n",
      "Epoch: 2 [871/877] [2625/2631] lr:0.0001840 Loss: 0.5905 Loss_dice: 0.3027 Loss_ce: 0.2878\n",
      "Epoch: 2 [872/877] [2626/2631] lr:0.0001838 Loss: 0.6968 Loss_dice: 0.3136 Loss_ce: 0.3832\n",
      "Epoch: 2 [873/877] [2627/2631] lr:0.0001836 Loss: 0.5291 Loss_dice: 0.2848 Loss_ce: 0.2443\n",
      "Epoch: 2 [874/877] [2628/2631] lr:0.0001835 Loss: 0.5276 Loss_dice: 0.2939 Loss_ce: 0.2338\n",
      "Epoch: 2 [875/877] [2629/2631] lr:0.0001833 Loss: 0.5257 Loss_dice: 0.2644 Loss_ce: 0.2613\n",
      "Epoch: 2 [876/877] [2630/2631] lr:0.0001831 Loss: 0.5580 Loss_dice: 0.2924 Loss_ce: 0.2656\n",
      "Saved Model to output/./checkpoint//Epoch_2_model.ckpt\n",
      "============== End Training ==============\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "from mindspore import ops\n",
    "from mindspore import SummaryRecord\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore import Tensor, Model, context\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.communication.management import init, get_rank, get_group_size\n",
    "from mindspore.train.loss_scale_manager import FixedLossScaleManager\n",
    "\n",
    "# config.device_target == 'Ascend'\n",
    "if config.device_target == 'Ascend':\n",
    "    device_id = int(os.getenv('DEVICE_ID'))\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=config.device_target, save_graphs=False)\n",
    "else:\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=config.device_target, save_graphs=False)\n",
    "\n",
    "mindspore.set_seed(1)\n",
    "\n",
    "@moxing_wrapper()\n",
    "def train_net(run_distribute=False):\n",
    "    if run_distribute:\n",
    "        init()\n",
    "        if config.device_target == 'Ascend':\n",
    "            rank_id = get_device_id()\n",
    "            rank_size = get_device_num()\n",
    "        else:\n",
    "            rank_id = get_rank()\n",
    "            rank_size = get_group_size()\n",
    "        parallel_mode = ParallelMode.DATA_PARALLEL\n",
    "        context.set_auto_parallel_context(parallel_mode=parallel_mode,\n",
    "                                          device_num=rank_size,\n",
    "                                          gradients_mean=True)\n",
    "    else:\n",
    "        rank_id = 0\n",
    "        rank_size = 1\n",
    "    # (1) create dataloader\n",
    "    train_dataset = create_dataset(data_path=config.data_path + \"/image/\",\n",
    "                                   seg_path=config.data_path + \"/seg/\",\n",
    "                                   rank_size=rank_size,\n",
    "                                   rank_id=rank_id, is_training=True)\n",
    "    train_data_size = train_dataset.get_dataset_size()\n",
    "    print(\"train dataset length is:\", train_data_size)\n",
    "    # (2) construct network\n",
    "    if config.device_target == 'Ascend':\n",
    "        network = UNet3d()\n",
    "    else:\n",
    "        network = UNet3d_()\n",
    "\n",
    "    # (3) define loss funtion\n",
    "    loss_ce_fn = nn.CrossEntropyLoss()\n",
    "    loss_dice_fn = nn.DiceLoss(smooth=1e-5)\n",
    "    # (4) lr shedule and optimizor\n",
    "    lr = Tensor(dynamic_lr(config, train_data_size), mstype.float32)\n",
    "    optimizer = nn.Adam(params=network.trainable_params(), learning_rate=lr)\n",
    "    # (5) set training mode\n",
    "    network.set_train()\n",
    "    # (6) Start training\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    def forward_fn(data, label):\n",
    "        logits = network(data)\n",
    "        loss_ce = loss_ce_fn(logits, label)\n",
    "        loss_dice = loss_dice_fn(logits, label)\n",
    "        loss = loss_dice + loss_ce\n",
    "        return loss, loss_dice, loss_ce, logits\n",
    "    # Get gradient function\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "    # Define function of one-step training\n",
    "    def train_step(data, label):\n",
    "        (loss, loss_dice, loss_ce, _), grads = grad_fn(data, label)\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss, loss_dice, loss_ce\n",
    "\n",
    "    with SummaryRecord('./summary_dir/summary_01') as summary_record:\n",
    "        for epoch in range(config.max_epoch):\n",
    "            for step, (data, label) in enumerate(train_dataset.create_tuple_iterator()):\n",
    "                loss, loss_dice, loss_ce = train_step(data, label)\n",
    "\n",
    "                current_step = epoch * train_data_size + step\n",
    "                current_lr = optimizer.get_lr()\n",
    "                summary_record.record(current_step)\n",
    "                summary_record.add_value('scalar', 'lr', current_lr)\n",
    "                summary_record.add_value('scalar', 'loss_total', loss)\n",
    "                summary_record.add_value('scalar', 'loss_dice', loss_dice)\n",
    "                summary_record.add_value('scalar', 'loss_ce', loss_ce)\n",
    "\n",
    "                loss, loss_dice, loss_ce = loss.asnumpy(), loss_dice.asnumpy(), loss_ce.asnumpy()\n",
    "                print(\"Epoch: %d [%d/%d] [%d/%d] lr:%.7f Loss: %.4f Loss_dice: %.4f Loss_ce: %.4f\" %\n",
    "                      (epoch, step, train_data_size, current_step, train_data_size*config.max_epoch,\n",
    "                       current_lr, loss, loss_dice, loss_ce))\n",
    "            # Save checkpoint\n",
    "            ckpt_save_dir = os.path.join(config.output_path, config.checkpoint_path)\n",
    "            if not os.path.exists(ckpt_save_dir):\n",
    "                os.makedirs(ckpt_save_dir)\n",
    "            mindspore.save_checkpoint(network, os.path.join(ckpt_save_dir, \"Epoch_\"+str(epoch)+\"_model.ckpt\"))\n",
    "            print(\"Saved Model to {}/Epoch_{}_model.ckpt\".format(ckpt_save_dir, epoch))\n",
    "\n",
    "    print(\"============== End Training ==============\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def first(iterable, default=None):\n",
    "    \"\"\"\n",
    "    Returns the first item in the given iterable or `default` if empty, meaningful mostly with 'for' expressions.\n",
    "    \"\"\"\n",
    "    for i in iterable:\n",
    "        return i\n",
    "    return default\n",
    "\n",
    "def _get_scan_interval(image_size, roi_size, num_image_dims, overlap):\n",
    "    \"\"\"\n",
    "    Compute scan interval according to the image size, roi size and overlap.\n",
    "    Scan interval will be `int((1 - overlap) * roi_size)`, if interval is 0,\n",
    "    use 1 instead to make sure sliding window works.\n",
    "    \"\"\"\n",
    "    if len(image_size) != num_image_dims:\n",
    "        raise ValueError(\"image different from spatial dims.\")\n",
    "    if len(roi_size) != num_image_dims:\n",
    "        raise ValueError(\"roi size different from spatial dims.\")\n",
    "\n",
    "    scan_interval = []\n",
    "    for i in range(num_image_dims):\n",
    "        if roi_size[i] == image_size[i]:\n",
    "            scan_interval.append(int(roi_size[i]))\n",
    "        else:\n",
    "            interval = int(roi_size[i] * (1 - overlap))\n",
    "            scan_interval.append(interval if interval > 0 else 1)\n",
    "    return tuple(scan_interval)\n",
    "\n",
    "def dense_patch_slices(image_size, patch_size, scan_interval):\n",
    "    \"\"\"\n",
    "    Enumerate all slices defining ND patches of size `patch_size` from an `image_size` input image.\n",
    "\n",
    "    Args:\n",
    "        image_size: dimensions of image to iterate over\n",
    "        patch_size: size of patches to generate slices\n",
    "        scan_interval: dense patch sampling interval\n",
    "\n",
    "    Returns:\n",
    "        a list of slice objects defining each patch\n",
    "    \"\"\"\n",
    "    num_spatial_dims = len(image_size)\n",
    "    patch_size = patch_size\n",
    "    scan_num = []\n",
    "    for i in range(num_spatial_dims):\n",
    "        if scan_interval[i] == 0:\n",
    "            scan_num.append(1)\n",
    "        else:\n",
    "            num = int(math.ceil(float(image_size[i]) / scan_interval[i]))\n",
    "            scan_dim = first(d for d in range(num) if d * scan_interval[i] + patch_size[i] >= image_size[i])\n",
    "            scan_num.append(scan_dim + 1 if scan_dim is not None else 1)\n",
    "    starts = []\n",
    "    for dim in range(num_spatial_dims):\n",
    "        dim_starts = []\n",
    "        for idx in range(scan_num[dim]):\n",
    "            start_idx = idx * scan_interval[dim]\n",
    "            start_idx -= max(start_idx + patch_size[dim] - image_size[dim], 0)\n",
    "            dim_starts.append(start_idx)\n",
    "        starts.append(dim_starts)\n",
    "    out = np.asarray([x.flatten() for x in np.meshgrid(*starts, indexing=\"ij\")]).T\n",
    "    return [(slice(None),)*2 + tuple(slice(s, s + patch_size[d]) for d, s in enumerate(x)) for x in out]\n",
    "\n",
    "def create_sliding_window(image, roi_size, overlap):\n",
    "    num_image_dims = len(image.shape) - 2\n",
    "    if overlap < 0 or overlap >= 1:\n",
    "        raise AssertionError(\"overlap must be >= 0 and < 1.\")\n",
    "    image_size_temp = list(image.shape[2:])\n",
    "    image_size = tuple(max(image_size_temp[i], roi_size[i]) for i in range(num_image_dims))\n",
    "\n",
    "    scan_interval = _get_scan_interval(image_size, roi_size, num_image_dims, overlap)\n",
    "    slices = dense_patch_slices(image_size, roi_size, scan_interval)\n",
    "    windows_sliding = [image[slice] for slice in slices]\n",
    "    return windows_sliding, slices\n",
    "\n",
    "def CalculateDice(y_pred, label):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y_pred: predictions. As for classification tasks,\n",
    "            `y_pred` should has the shape [BN] where N is larger than 1. As for segmentation tasks,\n",
    "            the shape should be [BNHW] or [BNHWD].\n",
    "        label: ground truth, the first dim is batch.\n",
    "    \"\"\"\n",
    "    metric = metrics()\n",
    "    y_pred_output = np.expand_dims(np.argmax(y_pred, axis=1), axis=1)\n",
    "    y_pred = one_hot(y_pred_output)\n",
    "    y = one_hot(label)\n",
    "    y_pred, y = ignore_background(y_pred, y)\n",
    "\n",
    "    dice = metric.dice_metric(y_pred, y)\n",
    "    hd95 = metric.hd95_metric(y_pred, y)\n",
    "    jc = metric.jc_metric(y_pred, y)\n",
    "    asd = metric.asd_metric(y_pred, y)\n",
    "    sens = metric.sensitivity_metric(y_pred, y)\n",
    "    return dice, hd95, jc, asd, sens\n",
    "\n",
    "def ignore_background(y_pred, label):\n",
    "    \"\"\"\n",
    "    This function is used to remove background (the first channel) for `y_pred` and `y`.\n",
    "    Args:\n",
    "        y_pred: predictions. As for classification tasks,\n",
    "            `y_pred` should has the shape [BN] where N is larger than 1. As for segmentation tasks,\n",
    "            the shape should be [BNHW] or [BNHWD].\n",
    "        label: ground truth, the first dim is batch.\n",
    "    \"\"\"\n",
    "    label = label[:, 1:] if label.shape[1] > 1 else label\n",
    "    y_pred = y_pred[:, 1:] if y_pred.shape[1] > 1 else y_pred\n",
    "    return y_pred, label\n",
    "\n",
    "def one_hot(labels):\n",
    "    N, _, D, H, W = labels.shape\n",
    "    labels = np.reshape(labels, (N, -1))\n",
    "    labels = labels.astype(np.int32)\n",
    "    N, K = labels.shape\n",
    "    one_hot_encoding = np.zeros((N, config.num_classes, K), dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        for j in range(K):\n",
    "            one_hot_encoding[i, labels[i][j], j] = 1\n",
    "    labels = np.reshape(one_hot_encoding, (N, config.num_classes, D, H, W))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "九、模型预测\n",
    "设置好测试数据集路径和加载模型路径，就可以开始测试啦！"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(8379:140121727778944,MainProcess):2022-11-09-08:50:36.145.188 [mindspore/dataset/engine/datasets_user_defined.py:766] GeneratorDataset's num_parallel_workers: 4 is too large which may cause a lot of memory occupation (>85%) or out of memory(OOM) during multiprocessing. Therefore, it is recommended to reduce num_parallel_workers to 2 or smaller.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset length is: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(8379:140121727778944,MainProcess):2022-11-09-08:50:40.164.53 [mindspore/dataset/engine/datasets_user_defined.py:766] GeneratorDataset's num_parallel_workers: 4 is too large which may cause a lot of memory occupation (>85%) or out of memory(OOM) during multiprocessing. Therefore, it is recommended to reduce num_parallel_workers to 2 or smaller.\n",
      "[WARNING] ME(8379:140121727778944,MainProcess):2022-11-09-08:50:41.777.156 [mindspore/dataset/engine/datasets_user_defined.py:766] GeneratorDataset's num_parallel_workers: 4 is too large which may cause a lot of memory occupation (>85%) or out of memory(OOM) during multiprocessing. Therefore, it is recommended to reduce num_parallel_workers to 2 or smaller.\n",
      "[WARNING] ME(16640:140121727778944,_MPWorker-145):2022-11-09-08:50:49.734.710 [mindspore/dataset/engine/queue.py:120] Using shared memory queue, but rowsize is larger than allocated memory max_rowsize 16777216 current rowsize 119537664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current image shape is (1, 1, 512, 512, 140)\n",
      "The 0 batch Dice: 0.9764, HD95:0.0000, JC: 0.9539, ASD: 0.2789, Sens: 0.9799\n",
      "current image shape is (1, 1, 512, 512, 158)\n",
      "The 1 batch Dice: 0.9766, HD95:0.0000, JC: 0.9542, ASD: 0.4121, Sens: 0.9796\n",
      "current image shape is (1, 1, 512, 512, 133)\n",
      "The 2 batch Dice: 0.9728, HD95:0.0000, JC: 0.9470, ASD: 0.6999, Sens: 0.9795\n",
      "current image shape is (1, 1, 512, 512, 114)\n",
      "The 3 batch Dice: 0.9201, HD95:27.0185, JC: 0.8521, ASD: 5.8562, Sens: 0.9867\n",
      "current image shape is (1, 1, 512, 512, 350)\n",
      "The 4 batch Dice: 0.9581, HD95:0.0000, JC: 0.9195, ASD: 0.0336, Sens: 0.9396\n",
      "current image shape is (1, 1, 512, 512, 290)\n",
      "The 5 batch Dice: 0.9668, HD95:0.0000, JC: 0.9357, ASD: 1.9029, Sens: 0.9753\n",
      "current image shape is (1, 1, 512, 512, 172)\n",
      "The 6 batch Dice: 0.9849, HD95:0.0000, JC: 0.9703, ASD: 0.0262, Sens: 0.9892\n",
      "current image shape is (1, 1, 512, 512, 125)\n",
      "The 7 batch Dice: 0.9795, HD95:0.0000, JC: 0.9598, ASD: 0.0651, Sens: 0.9805\n",
      "current image shape is (1, 1, 512, 512, 248)\n",
      "The 8 batch Dice: 0.9717, HD95:0.0000, JC: 0.9450, ASD: 1.0691, Sens: 0.9806\n",
      "current image shape is (1, 1, 512, 512, 481)\n",
      "The 9 batch Dice: 0.8112, HD95:10.2470, JC: 0.6824, ASD: 1.3854, Sens: 0.7098\n",
      "**********************End Eval***************************************\n",
      "The average Dice: 0.9518, HD95:3.7265, JC: 0.9120, ASD: 1.1729, Sens: 0.9501\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mindspore import dtype as mstype\n",
    "from mindspore import Model, context, Tensor\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "\n",
    "\n",
    "if config.device_target == 'Ascend':\n",
    "    device_id = int(os.getenv('DEVICE_ID'))\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=config.device_target, save_graphs=False)\n",
    "else:\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=config.device_target, save_graphs=False)\n",
    "\n",
    "# @moxing_wrapper()\n",
    "def test_net(data_path, ckpt_path):\n",
    "    data_dir = data_path + \"/image/\"\n",
    "    seg_dir = data_path + \"/seg/\"\n",
    "    eval_dataset = create_dataset(data_path=data_dir, seg_path=seg_dir, is_training=False)\n",
    "    eval_data_size = eval_dataset.get_dataset_size()\n",
    "    print(\"test dataset length is:\", eval_data_size)\n",
    "\n",
    "    metrics_score = {}\n",
    "    metrics_score[\"dice\"] = 0\n",
    "    metrics_score[\"hd95\"] = 0\n",
    "    metrics_score[\"jc\"] = 0\n",
    "\n",
    "    if config.device_target == 'Ascend':\n",
    "        network = UNet3d()\n",
    "    else:\n",
    "        network = UNet3d_()\n",
    "    network.set_train(False)\n",
    "    param_dict = load_checkpoint(ckpt_path)\n",
    "    load_param_into_net(network, param_dict)\n",
    "    # metrics\n",
    "    results = {}\n",
    "    results[\"dice\"] = 0\n",
    "    results[\"hd95\"] = 0\n",
    "    results[\"jc\"] = 0\n",
    "    results[\"asd\"] = 0\n",
    "    results[\"sens\"] = 0\n",
    "    model = Model(network)\n",
    "    index = 0\n",
    "    total_dice = 0\n",
    "    config.batch_size=1\n",
    "    for batch in eval_dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "        image = batch[\"image\"]\n",
    "        seg = batch[\"seg\"]\n",
    "        print(\"current image shape is {}\".format(image.shape), flush=True)\n",
    "        sliding_window_list, slice_list = create_sliding_window(image, config.roi_size, config.overlap)\n",
    "        image_size = (config.batch_size, config.num_classes) + image.shape[2:]\n",
    "        output_image = np.zeros(image_size, np.float32)\n",
    "        count_map = np.zeros(image_size, np.float32)\n",
    "        importance_map = np.ones(config.roi_size, np.float32)\n",
    "        for window, slice_ in zip(sliding_window_list, slice_list):\n",
    "            window_image = Tensor(window, mstype.float32)\n",
    "            pred_probs = model.predict(window_image)\n",
    "            output_image[slice_] += pred_probs.asnumpy()\n",
    "            count_map[slice_] += importance_map\n",
    "        output_image = output_image / count_map\n",
    "        dice, hd95, jc, asd, sens = CalculateDice(output_image, seg)\n",
    "\n",
    "        print(\"The %d batch Dice: %.4f, HD95:%.4f, JC: %.4f, ASD: %.4f, Sens: %.4f\"%(index, dice, hd95, jc, asd, sens))\n",
    "        total_dice += dice\n",
    "        results[\"dice\"] += dice\n",
    "        results[\"hd95\"] += hd95\n",
    "        results[\"jc\"] += jc\n",
    "        results[\"asd\"] += asd\n",
    "        results[\"sens\"] += sens\n",
    "        index = index + 1\n",
    "    avg_dice = results[\"dice\"] / eval_data_size\n",
    "    avg_hd95 = results[\"hd95\"] / eval_data_size\n",
    "    avg_jc = results[\"jc\"] / eval_data_size\n",
    "    avg_asd = results[\"asd\"] / eval_data_size\n",
    "    avg_sens = results[\"sens\"] / eval_data_size\n",
    "    print(\"**********************End Eval***************************************\")\n",
    "    print(\"The average Dice: %.4f, HD95:%.4f, JC: %.4f, ASD: %.4f, Sens: %.4f\" %\n",
    "          (avg_dice, avg_hd95, avg_jc, avg_asd, avg_sens))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_net(data_path=\"data/LUNA16/val/\",\n",
    "             ckpt_path=\"./output/checkpoint/model.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
